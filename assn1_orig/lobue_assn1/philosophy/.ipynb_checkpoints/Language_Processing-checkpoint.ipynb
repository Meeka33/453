{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First break documents apart so they can be more focused\n",
    "\n",
    "page_dirname = 'corpus'\n",
    "if not os.path.exists(page_dirname):\n",
    "    os.makedirs(page_dirname)\n",
    "\n",
    "def process_docs(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        path=directory + '/' + filename\n",
    "        load_doc(path)\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    content=file.read()\n",
    "    file.close()\n",
    "    webtext=BeautifulSoup(content, 'html.parser')\n",
    "    pagetext=webtext.find(id=\"main-text\")\n",
    "    \n",
    "    #Identify sub headers for topic sections\n",
    "    subject=[headers.text for headers in webtext.find_all('h1')]\n",
    "    topics=[]\n",
    "    for headers in pagetext.find_all('h2'):\n",
    "        topics.append(subject[0]+': '+headers.text)\n",
    "        \n",
    "    #Split HTML sections by header classes\n",
    "    fullbody=str(pagetext)\n",
    "    body_split = re.split('(<h2>|</h2>)', fullbody)\n",
    "    collection=[]\n",
    "    for section in body_split:\n",
    "        clean=BeautifulSoup(section, 'html.parser')\n",
    "        paragraphs=clean.select('p')\n",
    "        if paragraphs:\n",
    "            collection.append(paragraphs)\n",
    "   \n",
    "    #split and clean each section of text\n",
    "    counter=0\n",
    "    for section in collection:\n",
    "        text=''\n",
    "        for x in range(len(section)):\n",
    "            sent=str(section[x].text)\n",
    "            sent=sent.strip().replace('\\n', ' ')\n",
    "            sent=sent.strip().replace('<p>', ' ')\n",
    "            sent=sent.strip().replace('</p>', ' ')\n",
    "            sent=sent.strip().replace('<em>', ' ')\n",
    "            sent=sent.strip().replace('</em>', ' ')\n",
    "            text=text +' '+ sent\n",
    "        filename = page_dirname + '/' + topics[counter]+'.txt'\n",
    "        save_list(text, filename)\n",
    "        counter=counter+1\n",
    "\n",
    "     \n",
    "def save_list(lines, filename):\n",
    "    file=open(filename, 'w')\n",
    "    file.write(lines)\n",
    "    file.close()\n",
    "    \n",
    "   \n",
    "process_docs('philanguage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10207\n",
      "4509\n"
     ]
    }
   ],
   "source": [
    "#Start by creating a vocabulary to pull from\n",
    "\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc=load_doc(filename)\n",
    "    tokens=clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def process_docs(directory, vocab):\n",
    "    for filename in os.listdir(directory):\n",
    "        path=directory + '/' + filename\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "        \n",
    "vocab=Counter()\n",
    "process_docs('corpus', vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "min_occurrence=2\n",
    "tokens=[k for k,c in vocab.items() if c>min_occurrence]\n",
    "print(len(tokens))\n",
    "\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of documents:  97\n"
     ]
    }
   ],
   "source": [
    "#Now create bag of words: Import articles, clean, filter words only in vocab defined above, convert to string\n",
    "\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "def doc_to_line(filename, vocab):\n",
    "    doc=load_doc(filename)\n",
    "    tokens=clean_doc(doc)\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_docs(directory, vocab):\n",
    "    lines=list()\n",
    "    titles=list()\n",
    "    for filename in os.listdir(directory):\n",
    "        path=directory + '/' + filename\n",
    "        line=doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "        titles.append(filename)\n",
    "    return lines, titles\n",
    "\n",
    "def load_clean_dataset(vocab):\n",
    "    docs,labels=process_docs('corpus', vocab)\n",
    "    return docs, labels\n",
    "\n",
    "vocab_filename='vocab.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=set(vocab.split())\n",
    "docs, labels=load_clean_dataset(vocab)\n",
    "\n",
    "print('total number of documents: ', len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philosophy/\n",
      "    items.jl\n",
      "    scrapy_log_output.txt\n",
      "    run_articles_spider.py\n",
      "    scrapy.cfg\n",
      "    vocab.txt\n",
      "    Language_Processing.ipynb\n",
      "    corpusLudwig Wittgenstein.txt\n",
      "    doc_query.py\n",
      "    __pycache__/\n",
      "        run_articles_spider.cpython-37.pyc\n",
      "    philosophy/\n",
      "        settings.py\n",
      "        items.py\n",
      "        middlewares.py\n",
      "        __init__.py\n",
      "        pipelines.py\n",
      "        spiders/\n",
      "            __init__.py\n",
      "            articles-spider.py\n",
      "            __pycache__/\n",
      "                __init__.cpython-38.pyc\n",
      "                __init__.cpython-37.pyc\n",
      "                articles-spider.cpython-37.pyc\n",
      "        __pycache__/\n",
      "            __init__.cpython-38.pyc\n",
      "            settings.cpython-37.pyc\n",
      "            __init__.cpython-37.pyc\n",
      "            items.cpython-37.pyc\n",
      "            items.cpython-38.pyc\n",
      "            articleItems.cpython-38.pyc\n",
      "            settings.cpython-38.pyc\n",
      "    corpus/\n",
      "        Mental Representation: 3. Conceptual and Non-Conceptual Representation.txt\n",
      "        Private Language: 4. Kripke’s Sceptical Wittgenstein.txt\n",
      "        Logical Form:   2. Propositions and Traditional Grammar.txt\n",
      "        Vienna Circle: 2. The Basics: People, Activities and Overview of Doctrines.txt\n",
      "        Russell’s Logical Atomism: 2. Origins and Development of Russell’s Logical Atomism.txt\n",
      "        Mental Representation: 2. Propositional Attitudes.txt\n",
      "        Wittgenstein’s Aesthetics: 4. Conclusion.txt\n",
      "        Logical Form: 3. Motivations for Revision.txt\n",
      "        Russell’s Logical Atomism: 4. Ontological Aspects of Russell’s Logical Atomism.txt\n",
      "        Mental Representation: 1. The Representational Theory of Mind.txt\n",
      "        Mental Representation: 6. Content Determination.txt\n",
      "        George Edward Moore: 4. Philosophical Analysis.txt\n",
      "        Religious Language: 2. The Content of Religious Utterances.txt\n",
      "        Russell’s Logical Atomism: 5. Influence and Reception.txt\n",
      "        Mental Representation: 5. Imagery.txt\n",
      "        Gottlob Frege: 1. Frege’s Life and Influences.txt\n",
      "        Wittgenstein’s Logical Atomism: 3. Metaphysical Atomism.txt\n",
      "        Bertrand Russell: 7. Russell’s Social and Political Philosophy.txt\n",
      "        Facts: 1. Philosophies of Facts.txt\n",
      "        Bertrand Russell: 2. Russell’s Work in Logic.txt\n",
      "        Logical Form: 8. Transformational Grammar.txt\n",
      "        Religious Language: 3. The Use of Religious Language.txt\n",
      "        Logical Form: 1. Patterns of Reason.txt\n",
      "        George Edward Moore: 3. Principia Ethica.txt\n",
      "        States of Affairs: 1. Introducing States of Affairs.txt\n",
      "        Bertrand Russell: 8. Contemporary Russell Scholarship.txt\n",
      "        States of Affairs: 2. Thoughts and States of Affairs.txt\n",
      "        Gertrude Elizabeth Margaret Anscombe: 6. Conclusion.txt\n",
      "        Analysis: 5. Modern Conceptions of Analysis, outside Analytic Philosophy.txt\n",
      "        Gertrude Elizabeth Margaret Anscombe: 1. Life.txt\n",
      "        Gertrude Elizabeth Margaret Anscombe: 4. Action Theory.txt\n",
      "        Vienna Circle: 4. Concluding Remarks.txt\n",
      "        George Edward Moore: 1. Life and Career.txt\n",
      "        Analysis: 6. Conceptions of Analysis in Analytic Philosophy and the Introduction of the Logical (Transformative) Conception.txt\n",
      "        Gottlob Frege: 3. Frege’s Philosophy of Language.txt\n",
      "        Wittgenstein’s Philosophy of Mathematics: 3. The Later Wittgenstein on Mathematics: Some Preliminaries.txt\n",
      "        Logical Form: 6. Regimentation and Communicative Slack.txt\n",
      "        Wittgenstein’s Aesthetics: 2. The Critique of Scientism.txt\n",
      "        Religious Language: 1. Preliminaries: The Face Value Theory.txt\n",
      "        Wittgenstein’s Logical Atomism: 1. Names and Objects.txt\n",
      "        Analysis: 7. Conclusion.txt\n",
      "        Certainty: 2. Conceptions of certainty.txt\n",
      "        Wittgenstein’s Logical Atomism: 4. The Epistemology of Logical Atomism.txt\n",
      "        States of Affairs: 3. Facts and States of Affairs.txt\n",
      "        Bertrand Russell: 1. Russell’s Chronology.txt\n",
      "        Analysis: 3. Medieval and Renaissance Conceptions of Analysis.txt\n",
      "        Logical Form: 5. Descriptions and Analysis.txt\n",
      "        Logical Form: 4. Frege and Formal Language.txt\n",
      "        Gertrude Elizabeth Margaret Anscombe: 5. Moral Philosophy.txt\n",
      "        Analysis: 4. Early Modern Conceptions of Analysis and the Development of the Decompositional Conception.txt\n",
      "        Wittgenstein’s Logical Atomism: 5. The Dismantling of Logical Atomism.txt\n",
      "        George Edward Moore: 5. Perception and Sense-data.txt\n",
      "        Bertrand Russell: 5. Russell’s Theory of Neutral Monism.txt\n",
      "        George Edward Moore: 2. The Refutation of Idealism.txt\n",
      "        States of Affairs: 5. The Unity of a State of Affairs.txt\n",
      "        George Edward Moore: 6. Common Sense and Certainty.txt\n",
      "        Wittgenstein’s Aesthetics: 1. The Critique of Traditional Aesthetics.txt\n",
      "        Vienna Circle: 3. Selected Doctrines and their Criticisms.txt\n",
      "        Analysis: 2. Ancient Conceptions of Analysis and the Emergence of the Regressive Conception.txt\n",
      "        Logical Truth: 2. The Mathematical Characterization of Logical Truth.txt\n",
      "        Analysis: 1. General Introduction.txt\n",
      "        Private Language: 2. The Significance of the Issue.txt\n",
      "        Logical Truth: 1. The Nature of Logical Truth.txt\n",
      "        Wittgenstein’s Aesthetics: 3. The Comparative Approach.txt\n",
      "        Logical Form: 9. Semantic Structure and Events.txt\n",
      "        Vienna Circle: 1. Introductory Remarks.txt\n",
      "        Certainty: 1. Kinds of certainty.txt\n",
      "        Wittgenstein’s Philosophy of Mathematics: 2. The Middle Wittgenstein’s Finitistic Constructivism.txt\n",
      "        Mental Representation: 9. Thought and Language.txt\n",
      "        Russell’s Logical Atomism: 1. Introduction.txt\n",
      "        Mental Representation: 4. Representationalism and Phenomenalism.txt\n",
      "        Mental Representation: 7. Internalism and Externalism.txt\n",
      "        States of Affairs: 4. States of Affairs as Complexes.txt\n",
      "        States of Affairs: 6. Conclusion.txt\n",
      "        Facts: 2. Formal Theories of Facts.txt\n",
      "        Wittgenstein’s Philosophy of Mathematics: 1. Wittgenstein on Mathematics in the Tractatus.txt\n",
      "        Logical Form: 7. Notation and Restricted Quantification.txt\n",
      "        Private Language: 1. Overview: Wittgenstein’s Argument and its Interpretations.txt\n",
      "        Mental Representation: 8. The Computational Theory of Mind.txt\n",
      "        Ludwig Wittgenstein: 3. The Later Wittgenstein.txt\n",
      "        Wittgenstein’s Logical Atomism: 2. Linguistic Atomism.txt\n",
      "        Russell’s Logical Atomism: 3. Russell’s Philosophical Method and the Notion of Analysis.txt\n",
      "        Bertrand Russell: 4. Russell’s Theory of Definite Descriptions.txt\n",
      "        Private Language: 3. The Private Language Argument Expounded.txt\n",
      "        Religious Language: 5. Reference and Logic.txt\n",
      "        Bertrand Russell: 3. Russell’s Work in Analytic Philosophy.txt\n",
      "        Bertrand Russell: 6. Russell’s Atheism.txt\n",
      "        Wittgenstein’s Philosophy of Mathematics: 4. The Impact of Philosophy of Mathematics on Mathematics.txt\n",
      "        Certainty: 3. Two dimensions of certainty.txt\n",
      "        Gertrude Elizabeth Margaret Anscombe: 3. Metaphysics.txt\n",
      "        Ludwig Wittgenstein: 2. The Early Wittgenstein.txt\n",
      "        George Edward Moore: 7. Moore's Legacy.txt\n",
      "        Logical Form: 10. Further Questions.txt\n",
      "        Gertrude Elizabeth Margaret Anscombe: 2. Wittgenstein’s Influence.txt\n",
      "        Gottlob Frege: 2. Frege’s Logic and Philosophy of Mathematics.txt\n",
      "        Ludwig Wittgenstein: 1. Biographical Sketch.txt\n",
      "        Religious Language: 4. Religious Minimalism.txt\n",
      "    .ipynb_checkpoints/\n",
      "        Language_Processing-checkpoint.ipynb\n",
      "    philanguage/\n",
      "        vienna-circle.html\n",
      "        states-of-affairs.html\n",
      "        certainty.html\n",
      "        anscombe.html\n",
      "        frege.html\n",
      "        russell.html\n",
      "        mental-representation.html\n",
      "        private-language.html\n",
      "        moore.html\n",
      "        logical-truth.html\n",
      "        wittgenstein-atomism.html\n",
      "        wittgenstein.html\n",
      "        logical-form.html\n",
      "        wittgenstein-aesthetics.html\n",
      "        facts.html\n",
      "        religious-language.html\n",
      "        wittgenstein-mathematics.html\n",
      "        logical-atomism.html\n",
      "        analysis.html\n",
      "\n",
      "Scrapy spider names:\n",
      "\n",
      "\n",
      "JSON lines written to items.jl\n",
      "\n",
      "length of vocabulary: 10207\n",
      "number of tokens:  4509\n",
      "total number of documents:  97\n",
      "Would you like to search the philosophy of languge corpus? (y/n)n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to search the philosophy of languge corpus? (y/n)n\n"
     ]
    }
   ],
   "source": [
    "from run_articles_spider import Doc_query\n",
    "\n",
    "begin=input(\"Would you like to search the philosophy of languge corpus? (y/n)\")\n",
    "if begin=='y':\n",
    "    query=input(\"What would you like to look up today?\")\n",
    "    Doc_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def get_tfidf_cosim(documents, query):\n",
    "#     docTFIDF = TfidfVectorizer().fit_transform(docs)\n",
    "#     queryTFIDF = TfidfVectorizer().fit(docs)\n",
    "#     queryTFIDF = queryTFIDF.transform([query])\n",
    "\n",
    "#     cosineSimilarities = cosine_similarity(queryTFIDF, docTFIDF).flatten()\n",
    "#     related_docs_indices = cosineSimilarities.argsort()[:-5:-1]\n",
    "#     return related_docs_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def Doc_query(query):\n",
    "    docTFIDF = TfidfVectorizer().fit_transform(docs)\n",
    "    queryTFIDF = TfidfVectorizer().fit(docs)\n",
    "    queryTFIDF = queryTFIDF.transform([query])\n",
    "\n",
    "    cosineSimilarities = cosine_similarity(queryTFIDF, docTFIDF).flatten()\n",
    "    related_docs_indices = cosineSimilarities.argsort()[:-5:-1]\n",
    "    \n",
    "    print(\"The top document hit is: \\n\", (labels[related_docs_indices[0]]).rstrip('.txt'))\n",
    "    print(\"\\nThe relevant search identifiers are:\")\n",
    "    print(Counter(docs[related_docs_indices[0]].split()).most_common(5))\n",
    "    answer=input(\"Would you like to see more detail about this article? (y/n) \\n\")\n",
    "    if answer=='y':\n",
    "        print(\"Full text excerpt is as follows: \\n\")\n",
    "        path= 'corpus/'+ labels[related_docs_indices[0]]\n",
    "        f=open(path, 'r')\n",
    "        for line in f:\n",
    "            print(line)\n",
    "        f.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    answer=input(\"Would you like to see another document? (y/n) \\n\")\n",
    "    if answer=='y':\n",
    "        print(\"\\nThe next document hit is: \\n\", (labels[related_docs_indices[1]]).rstrip('.txt'))\n",
    "        print(\"\\nThe relevant search identifiers are:\")\n",
    "        print(Counter(docs[related_docs_indices[1]].split()).most_common(5))\n",
    "        answer=input(\"Would you like to see more detail about this article? (y/n) \\n\")\n",
    "        if answer=='y':\n",
    "            print(\"Full text excerpt is as follows: \\n\")\n",
    "            path= 'corpus/'+ labels[related_docs_indices[1]]\n",
    "            f=open(path, 'r')\n",
    "            for line in f:\n",
    "                print(line)\n",
    "            f.close()\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top document hit is: \n",
      " Logical Truth: 2. The Mathematical Characterization of Logical Truth\n",
      "\n",
      "The relevant search identifiers are:\n",
      "[('logical', 94), ('truth', 55), ('validity', 52), ('modeltheoretic', 41), ('one', 33)]\n",
      "Would you like to see more detail about this article? (y/n) \n",
      "\n",
      "Would you like to see another document? (y/n) \n",
      "y\n",
      "\n",
      "The next document hit is: \n",
      " Wittgenstein’s Philosophy of Mathematics: 3. The Later Wittgenstein on Mathematics: Some Preliminaries\n",
      "\n",
      "The relevant search identifiers are:\n",
      "[('wittgenstein', 91), ('rfm', 80), ('mathematical', 61), ('proposition', 45), ('mathematics', 37)]\n",
      "Would you like to see more detail about this article? (y/n) \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "#query='how do we interpret multi dimensional space in language?'\n",
    "query='Is language closely related to calculus?'\n",
    "Doc_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59 35 36 67]\n",
      "Logical Truth: 2. The Mathematical Characterization of Logical Truth.txt\n",
      "[('logical', 94), ('truth', 55), ('validity', 52), ('modeltheoretic', 41), ('one', 33)]\n",
      "Wittgenstein’s Philosophy of Mathematics: 3. The Later Wittgenstein on Mathematics: Some Preliminaries.txt\n",
      "[('wittgenstein', 91), ('rfm', 80), ('mathematical', 61), ('proposition', 45), ('mathematics', 37)]\n",
      "Logical Form: 6. Regimentation and Communicative Slack.txt\n",
      "[('language', 24), ('sentences', 19), ('logical', 15), ('propositions', 12), ('natural', 12)]\n",
      "Wittgenstein’s Philosophy of Mathematics: 2. The Middle Wittgenstein’s Finitistic Constructivism.txt\n",
      "[('mathematical', 101), ('pr', 82), ('wittgenstein', 76), ('infinite', 62), ('pg', 60)]\n"
     ]
    }
   ],
   "source": [
    "query='Is language closely related to calculus?'\n",
    "calculus=Doc_query(query)\n",
    "print(calculus)\n",
    "\n",
    "for x in calculus:\n",
    "    print(labels[x])\n",
    "    print(Counter(docs[x].split()).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query='How can language represent religious ideas?'\n",
    "religion=Doc_query(query)\n",
    "print(religion)\n",
    "\n",
    "for x in religion:\n",
    "    print(labels[x])\n",
    "    print(Counter(docs[x].split()).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: wittgenstein propositions proposition tractatus analysis mathematical mathematics logical sense form work objects philosophical case means fact later simple terms thought\n",
      "Topic #1: affairs states state facts properties objects socrates wise possible worlds exist fact true parts thoughts particulars obtains obtain ontologically exists\n",
      "Topic #2: religious aesthetic god language work moral religion action causal theory discourse philosophy utterances ethics effect view account intention cause aesthetics\n",
      "Topic #3: mental representations properties content anscombe representation phenomenal states intentional cognitive fodor psychology representational processes contents experience computational relations cambridge mind\n",
      "Topic #4: logical sentence sentences form boy propositions like structure grammatical language dog valid natural frege inference proposition expressions predicate true function\n",
      "Topic #5: analysis analytic frege geometry supplementary greek decompositional section conceptions conception ancient concepts concept philosophy modern century method plato seen understanding\n",
      "Topic #6: russell knowledge physical bertrand logical theory world atomic certain atomism political life classes entities writings facts view relations relation propositions\n",
      "Topic #7: circle vienna philosophy carnap members science war logic scientific remained schlick neurath philosophical work years empirical uebel formal criterion theoretical\n",
      "Topic #8: moore moores cambridge sensedata ethical value position philosopher philosophy thesis experience discussions important intrinsic idealism believe common explicitly sense things\n",
      "Topic #9: private certainty language belief argument knowledge know descartes sensation meaning investigations pi clear philosophical epistemic problem text pw use kripke\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=97 and n_features=1000...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: language given work value logical theory proposition logic form argument case moral example wittgenstein way say effect truth question second\n",
      "Topic #1: phenomenal analysis boy experience properties form russell logical proposition example say argument function concept sentence experiences claim complex plato work\n",
      "Topic #2: affairs logical states facts state fact true truth russell proposition propositions possible properties objects view moore sentence sense analysis argument\n",
      "Topic #3: religious god language discourse theory propositions sentences true structure utterances example like form account action speakers proposition theories sentence claims\n",
      "Topic #4: circle sensedata carnap logical science affairs vienna physical philosophy position terms criterion theoretical moore theory empirical schlick statements objects clear\n",
      "Topic #5: logical language philosophy russell wittgenstein logic analysis frege terms philosophical theory carnap work propositions form circle way science statements meaning\n",
      "Topic #6: analysis logical anscombe philosophy russell conception cambridge frege concept work number predicate supplementary form say view concepts used logic wittgenstein\n",
      "Topic #7: mental belief certainty states content certain true representations properties representation subject beliefs cognitive relations view proposition knowledge sense fodor say\n",
      "Topic #8: wittgenstein proposition mathematical facts propositions set true fact pr mathematics rfm infinite view says numbers pg theory proof calculus say\n",
      "Topic #9: like form logical dog true sentences propositions valid facts inference sentence given following proposition natural predicate romeo logic structure view\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "n_samples = 97\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,max_features=n_features,stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "nmf = NMF(n_components=n_components, random_state=1, beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \" \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "filename='philanguage/frege.html'\n",
    "file=open(filename, 'r')\n",
    "content=file.read()\n",
    "file.close()\n",
    "webtext=BeautifulSoup(content, 'html.parser')\n",
    "pagetext=webtext.find(id=\"main-text\")\n",
    "\n",
    "subject=[headers.text for headers in webtext.find_all('h1')]\n",
    "    \n",
    "topics=[]\n",
    "for headers in pagetext.find_all('h2'):\n",
    "    topics.append(subject[0]+': '+headers.text)\n",
    "    \n",
    "fullbody=str(pagetext)\n",
    "body_split = re.split('(<h2>|</h2>)', fullbody)\n",
    "collection=[]\n",
    "for section in body_split:\n",
    "    clean=BeautifulSoup(section, 'html.parser')\n",
    "    paragraphs=clean.select('p')\n",
    "    if paragraphs:\n",
    "        collection.append(paragraphs)\n",
    "        \n",
    "corpus=list(zip(topics,collection))\n",
    "\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Language Processing 3.7",
   "language": "python",
   "name": "language"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
