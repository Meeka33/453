 The central idea of hypothetico-deductive (HD) confirmation can be roughly described as “deduction-in-reverse”: evidence is said to confirm a hypothesis in case the latter, while not entailed by the former, is able to entail it, with the help of suitable auxiliary hypotheses and assumptions. The basic version (sometimes labelled “naïve”) of the HD notion of confirmation can be spelled out thus: Note that clause (ii) above represents HD-disconfirmation as plain logical inconsistency of the target hypothesis with the data (given the auxiliaries) (see Hempel 1945, 98). HD-confirmation and Hempelian confirmation convey different intuitions (see Huber 2008a for an original analysis). They are, in fact, distinct and strictly incompatible notions. This will be effectively illustrated by the consideration of the following conditions. Entailment condition (EC)  For any \(h,e,k \in \bL\), if \(e\wedge k\) is consistent, \(e\wedge k \vDash h\) and \(k \not\vDash h\), then \(e\) confirms \(h\) relative to \(k\). Confirmation complementarity (CC)  For any \(h, e, k \in \bL\), \(e\) confirms \(h\) relative to \(k\) if and only if \(e\) disconfirms \(\neg h\) relative to \(k\). Special consequence condition (SCC)  For any \(h, e, k \in \bL\), if \(e\) confirms \(h\) relative to \(k\) and \(h\wedge k \vDash h^*\), then \(e\) confirms \(h^*\) relative to \(k\). On the implicit proviso that \(k\) is empty (that is, tautologous: \(k = \top\)), Hempel (1943, 1945) himself had put forward (EC) and (SCC) as compelling adequacy conditions for any theory of confirmation, and devised his own proposal accordingly. As for (CC), he took it as a plain definitional truth (1943, 127). Moreover, Hempelian confirmation (extended) satisfies all conditions above (of course, for arguments \(h\), \(e\) and \(k\) for which it is defined). HD-confirmation, on the contrary, violates all of them. Let us briefly discuss each one in turn. It is rather common for a theory of ampliative (non-deductive) reasoning to retain classical logical entailment as a special case (a feature sometimes called “super-classicality”; see Strasser and Antonelli 2019). That’s essentially what (EC) implies for confirmation. Now given appropriate \(e\), \(h\) and \(k\), if \(e\wedge k\) entails \(h\), we readily get that \(e\) Hempel-confirms \(h\) relative to \(k\) in two simple steps. First, given that \(e\) and \(k\) are both quantifier-free, \(dev_{e}(e\wedge k) = e\wedge k\) according to Hempel’s full definition of \(dev\) (see Hempel 1943, 131). Then it trivially follows that \(e\wedge k \vDash dev_{e}(e\wedge k)\), so \(e\wedge k\) is (directly) Hempel-confirmed and its logical consequence \(h\) is likewise confirmed (indirectly). Logical entailment is thus retained as an instance of Hempelian confirmation in a fairly straightforward way. HD-confirmation, on the contrary, does not fulfil (EC). Here is one odd example (see Sprenger 2011a, 234). With \(k = \top\), just let \(e\) be the observation report that object \(a\) is a black swan, \(swan(a) \wedge black(a)\), and \(h\) be the hypothesis that black swans exist, \(\exists x(swan(x) \wedge black(x))\). Evidence \(e\) verifies \(h\) conclusively, and yet it does not HD-confirm it, simply because \(h \not\vDash e\). So the observation of a black swan turns out to be HD-neutral for the hypothesis that black swans exist! The same example shows how HD-confirmation violates (CC), too. In fact, while HD-neutral for \(h\), \(e\) HD-disconfirms its negation \(\neg h\) that no swan is black, \(\forall x(swan(x) \rightarrow \neg black(x))\), because the latter is obviously inconsistent with (refuted by) \(e\). The violation of (EC) and (CC) by HD-confirmation is arguably a reason for concern, for those conditions seem highly plausible. The special consequence condition (SCC), on the other hand, deserves separate and careful consideration. As we will see later on, (SCC) is a strong constraint, and far from sacrosanct. For now, let us point out one major philosophical motivation in its favor. (SCC) has often been invoked as a means to ensure the fulfilment of the following condition (see, e.g., Hesse 1975, 88; Horwich 1983, 57): Predictive inference condition (PIC)  For any \(e, k \in \bL\), if \(e\) confirms \(\forall x(Fx \rightarrow Gx)\) relative to \(k\), then \(e\) confirms \(F(a) \rightarrow G(a)\) relative to \(k\). In fact, (PIC) readily follows from (SCC) and so it is satisfied by Hempel’s theory. It says that, if evidence \(e\) confirms “all \(F\)s are \(G\)s”, then it also confirms that a further object will be \(G\), if it is \(F\). Notably, this does not hold for HD-confirmation. Here is why. Given \(k = Fa\) (i.e., the assumption that \(a\) comes from the \(F\) population), we have that \(e = Ga\) HD-confirms \(h = \forall x(Fx \rightarrow Gx)\), because the latter entails the former (given \(k\)). (That’s the HD reconstruction of Nicod’s insight, see below.) We also have, of course, that \(h\) entails \(h^* = Fb \rightarrow Gb\). And yet, contrary to (PIC), since \(h^*\) does not entail \(e\) (given \(k\)), it is not HD-confirmed by it either. The troubling conclusion is that the observation that a swan is white (or that a million of them are, for that matters) does not HD-confirm the prediction that a further swan will be found to be white. One attractive feature of HD-confirmation is that it largely eludes the ravens paradox. As the hypothesis \(h\) that all ravens are black does not entail that some generally sampled object \(a\) will be a black raven, the HD view of confirmation is not committed to the eminently Hempelian implication that \(e = raven(a) \wedge black(a)\) confirms \(h\). Likewise, \(\neg black(a) \wedge \neg raven(a)\) does not HD-confirm that all non-black objects are non-raven. The derivation of the paradox, as presented above, is thus blocked. Indeed, HD-confirmation yields a substantially different reading of Nicod’s insight as compared to Hempel’s theory (Okasha 2011 has an important discussion of this distinction). Here is how it goes. If object \(a\) is assumed to have been taken among ravens—so that, crucially, the auxiliary assumption \(k = raven(a)\) is made—and \(a\) is checked for color and found to be black, then, yes, the latter evidence, \(black(a)\), HD-confirms that all ravens are black \((h)\) relative to \(k\). By the same token, \(\neg black(a)\) HD-disconfirms \(h\) relative to the same assumption \(k = raven(a)\). And, again, this is as it should be, in line with Nicod’s mention of “the absence of \(G\) [here, non-black as evidence] in a case of \(F\) [here, raven as an auxiliary assumption]”. It is also true that an object that is found not to be a raven HD-confirms \(h\), but only relative to \(k = \neg black(a)\), that is, if \(a\) is assumed to have been taken among non-black objects to begin with; and this seems acceptable too (after all, while sampling from non-black objects, one might have found the counterinstance of a raven, but didn’t). Unlike Hempel’s theory, moreover, HD-confirmation does not yield the debatable implication that, by itself (that is, given \(k = \top\)), the observation of a non-raven \(a\), \(\neg raven(a)\), must confirm \(h\). Interestingly, the introduction of auxiliary hypotheses and assumptions shows that the issues surrounding Nicod’s remarks can become surprisingly subtle. Consider the following statements (Maher’s 2006 example): \(\alpha_1\) simply specifies that no object is both white and black, while \(\alpha_2\) says that, if there are swans at all, then there also is some black swan. Also posit, again, \(e = swan(a) \wedge white(a)\). Under \(\alpha_1\) and \(\alpha_2\), the observation of a white swan clearly disconfirms (indeed, refutes) the hypothesis \(h\) that all swans are white. Hempel’s theory (extended) faces difficulties here, because for \(k = dev_{e}(\alpha_1 \wedge \alpha_2)\) it turns out that \(e\wedge k\) is inconsistent. But HD-confirmation gets this case right, thus capturing appropriate boundary conditions for Nicod’s generally sensible claims. For, with \(k = \alpha_1 \wedge \alpha_2\), one has that \(h\wedge k\) is consistent and entails \(\neg e\) (for it entails that no swan exists), so that \(e\) HD-disconfirms (refutes) \(h\) relative to \(k\) (see Good 1967 for another famous counterexample to Nicod’s condition). HD-confirmation, however, is also known to suffer from distinctive “paradoxical” implications. One of the most frustrating is surely the following (see Osherson, Smith, and Shafir 1986, 206, for further specific problems). The irrelevant conjunction paradox. Suppose that \(e\) confirms \(h\) relative to (possibly empty) \(k\). Let statement \(q\) be logically consistent with \(e\wedge h\wedge k\), but otherwise entirely irrelevant for all of those conjuncts (perhaps belonging to a completely separate domain of inquiry). Does \(e\) confirm \(h\wedge q\) (relative to \(k\)) as it does with \(h\)? One would want to say no, and this implication can be suitably reconstructed in Hempel’s theory. HD-confirmation, on the contrary, can not draw this distinction: it is easy to show that, on the conditions specified, if the HD clause for confirmation is satisfied for \(e\) and \(h\) (given \(k\)), so it is for \(e\) and \(h\wedge q\) (given \(k\)). (This is simply because, if \(h\wedge k \vDash e\), then \(h\wedge q\wedge k \vDash e\), too, by the monotonicity of classical logical entailment.) Kuipers (2000, 25) suggested that one can live with the irrelevant conjunction problem because, on the conditions specified, \(e\) would still not HD-confirm \(q\) alone (given \(k\)), so that HD-confirmation can be “localized”: \(h\) is the only bit of the conjunction \(h\wedge q\) that gets any confirmation on its own, as it were. Other authors have been reluctant to bite the bullet and have engaged in technical refinements of the “naïve” HD view. In these proposals, the spread of HD-confirmation upon frivolous conjunctions can be blocked at the expense of some additional logical machinery (see Gemes 1993, 1998; Schurz 1991, 1994). Finally, it should be noted that HD-confirmation offers no substantial relief from the blite paradox. On the one hand, \(e = raven(a) \wedge ex_{t\le T}(a) \wedge black(a)\) does not, as such, HD-confirm either \(h = \forall x(raven(x) \rightarrow black(x))\) or \(h^* = \forall x(raven(x) \rightarrow blite(x))\), that is, for empty \(k\). On the other hand, if object \(a\) is assumed to have been sampled from ravens before \(T\) (that is, given \(k = raven(a) \wedge ex_{t\le T}(a))\), then \(black(a)\) is entailed by both “all ravens are black” and “all ravens are blite” and therefore HD-confirms each of them. So HD-confirmation, too, sanctions the existence of confirmation relations that seem intuitively unsound (indeed, indefinitely many of them: as we know, other variations of \(h^*\) can be conceived at will, like the “blurple” hypothesis). One could insist that HD does handle the blite paradox after all, because \(black(a)\) (given \(k\) as above) does not HD-confirms that a raven will be white if examined after \(T\) (Kuipers 2000, 29 ff.). Unfortunately (as pointed out by Schurz 2005, 148) \(black(a)\) does not HD-confirm that a raven will be black if examined after \(T\) either (again, given \(k\) as above). That’s because, as already pointed out, HD-confirmation fails the predictive inference condition (PIC) in general. So, all in all, HD-confirmation can not tell black from blite any more than Hempel-confirmation can. The issues above look contrived and artificial to some people’s taste—even among philosophers. Many have suggested a closer look at real-world inferential practices in the sciences as a more appropriate benchmark for assessment. For one thing, the very idea of hypothetico-deductivism has often been said to stem from the origins of Western science. As reported by Simplicius of Cilicia (sixth century A.D.) in his commentary on Aristotle’s De Caelo, Plato had challenged his pupils to identify combinations of “ordered” motions by which one could account for (namely, deduce) the planets’ wandering trajectories across the heavens as observed by the Earth. As a matter of historical fact, mathematical astronomy has engaged in just this task for centuries: scholars have been trying to define geometrical models from which the apparent motion of celestial bodies would derive. It is fair to say that, at its roots, the kind of challenges that the HD framework faces with scientific reasoning is not so different from the main puzzles that arise from philosophical considerations of a more formal kind. Still, the two areas turn out to be complementary in important ways. The following statement will serve as a useful starting point to extend the scope of our discussion. Underdetermination Theorem (UT) for “naïve” HD-confirmation For any contingent \(h, e \in \bL\), if \(h\) and \(e\) are logically consistent, there exists some \(k \in \bL\) such that \(e\) HD-confirms \(h\) relative to \(k\). (UT) is an elementary logical fact that has been long recognized (see, e.g., Glymour 1980a, 36). In purely formal terms, just positing \(k = h \rightarrow e\) will do for a proof. To appreciate how (UT) can spark any philosophical interest, one has to combine it with some insightful remarks first put forward by Pierre Duhem (1906) and then famously revived by Quine (1951) in a more radical style. (Indeed, (UT) essentially amounts to the “entailment version” of “Quinean underdetermination” in Laudan 1990, 274.) Duhem (he himself a supporter of the HD view) pointed out that in mature sciences such as physics most hypotheses or theories of real interest can not be contradicted by any statement describing observable states of affairs. Taken in isolation, they simply do not logically imply, nor rule out, any observable fact, essentially because (unlike “all ravens are black”) they involve the mention of unobservable entities and processes. So, in effect, Duhem emphasized that, typically, scientific hypotheses or theories are logically consistent with any piece of checkable evidence. Unless, of course, the logical connection is underpinned by auxiliary hypotheses and assumptions suitably bridging the gap between the observational and non-observational vocabulary, as it were. But then, once auxiliaries are in play, logic alone guarantees that some \(k\) exists such that \(h\wedge k\) is consistent, \(h\wedge k \vDash e\), and \(k \not\vDash e\), so that confirmation holds in naïve HD terms (that’s just the UT result above). Apparently, when Duhem’s point applies, the uncritical supporter of whatever hypothesis \(h\) can legitimately claim (naïve HD) confirmation from any \(e\) by simply shaping \(k\) conveniently. In this sense, hypothesis assessment would be radically “underdetermined” by any amount of evidence practically available. Influential authors such as Thomas Kuhn (1962/1970) (but see Laudan 1990, 268, for a more extensive survey) relied on Duhemian insights to suggest that confirmation by empirical evidence is too weak a force to drive the evaluation of theories in science, often inviting conclusions of a relativistic flavor (see Worrall 1996 for an illuminating reconstruction along these lines). Let us briefly consider a classical case, which Duhem himself thoroughly analyzed: the wave vs. particle theories of light in modern optics. Across the decades, wave theorists were able to deduce an impressive list of important empirical facts from their main hypothesis along with appropriate auxiliaries, diffraction phenomena being only one major example. But many particle theorists’ reaction was to retain their hypothesis nonetheless and to reshape other parts of the “theoretical maze” (i.e., \(k\); the term is Popper’s, 1963, p. 330) to recover those observed facts as consequences of their own proposal. And as we’ve seen, if the bare logic of naïve HD was to be taken strictly, surely they could have claimed their overall hypothesis to be confirmed too, just as much as their opponents. Importantly, they didn’t. In fact, it was quite clear that particle theorists, unlike their wave-theory opponents, were striving to remedy weaknesses rather than scoring successes (see Worrall 1990). But why, then? Because, as Duhem himself clearly realized, the logic of naïve HD “is not the only rule for our judgments” (1906, 217). The lesson of (UT) and the Duhemian insight is not quite, it seems, that naïve HD is the last word and scientific inference is unconstrained by stringent rational principles, but rather that the HD view has to be strengthened in order to capture the real nature of evidential support in rational scientific inference. At least, that’s the position of a good deal of philosophers of science working within the HD framework broadly construed. It has even been maintained that “no serious twentieth-century methodologist” has ever subscribed to the naïve HD view above “without crucial qualifications” (Laudan 1990, 278; also see Laudan and Leplin 1991, 466). So the HD approach to confirmation has yielded a number of more articulated variants to meet the challenge of underdetermination. Following (loosely) Norton (2005), we will now survey an instructive sample of them. Naïve HD can be enriched by a resolute form of predictivism. According to this approach, the naïve HD clause for confirmation is too weak because \(e\) must have been predicted in advance from \(h\wedge k\). Karl Popper’s (1934/1959) account of the “corroboration” of hypotheses famously embedded this view, but squarely predictivist stances can be traced back to early modern thinkers like Christiaan Huygens (1629–1695) and Gottfried Wilhelm Leibniz (1646–1716), and in Duhem’s work itself. The predictivist sets a high bar for confirmation. Her favorite examples typically include stunning episodes in which the existence of previously unknown objects, phenomena, or whole classes of them is anticipated: the phases of Venus for Copernican astronomy or the discovery of Neptune for Newtonian physics, all the way up to the Higgs boson for so-called standard model of subatomic particles. The predictivist solution to the underdetermination problem is fairly radical: many of the relevant factual consequences of \(h\wedge k\) will be already known when this theory is articulated, and so unfit for confirmation. Critics have objected that predictivism is in fact far too restrictive. There seem to be many cases in which already known phenomena clearly do provide support to a new hypothesis or theory. Zahar (1973) first raised this problem of “old evidence”, then made famous by Glymour (1980a, 85 ff.) as a difficulty for Bayesianism (see  Section 3  below). Examples of this kind abound in the history of science as elsewhere, but the textbook illustration has become the precession of Mercury’s perihelion, a lasting anomaly for Newtonian physics: Einstein’s general relativity calculations got this long-known fact right, thereby gaining a remarkable piece of initial support for the new theory. In addition to this problem with old evidence, HD predictivism also seems to lack a principled rationale. After all, the temporal order of the discovery of \(e\) and of the articulation of \(h\) and \(k\) may well be an entirely accidental historical contingency. Why should it bear on the confirmation relationship among them? (See Giere 1983 and Musgrave 1974 for classical discussions of these issues. Douglas and Magnus 2013 and Barnes 2018 offer more recent views and rich lists of further references.) As a possible response to the difficulties above, naïve HD can be enriched by the use-novelty criterion (UN) instead. The UN reaction to the underdetermination problem is more conservative than the temporal predictivist strategy. According to this view, to improve on the weak naïve HD clause for confirmation one only has to rule out one particular class of cases, i.e., those in which the description of a known fact, \(e\), served as a constraint in the construction of \(h\wedge k\). The UN view thus comes equipped with a rationale. If \(h\wedge k\) was shaped on the basis of \(e\), UN advocates point out, then it was bound to get that state of affairs right; the theory never ran any risk of failure, thus did not achieve any particularly significant success either. Precisely in these cases, and just for this reason, the evidence \(e\) must not be double-counted: by using it for the construction of the theory, its confirmational power becomes “dried out”, so to speak. The UN completion of naïve HD originated from Lakatos and some of his collaborators (see Lakatos and Zahar 1975 and Worrall 1978; also see Giere 1979, 161–162, and Gillies 1989 for similar views), although important hints in the same direction can be found at least in the work of William Whewell (1840/1847). Consider the touchstone example of Mercury again. According to Zahar (1973), Einstein did not need to rely on the Mercury data to define theory and auxiliaries as to match observationally correct values for the perihelion precession (also see Norton 2011a; and Earman and Janssen 1993 for a very detailed, and more nuanced, account). Being already known, the fact was not of course predicted in a strictly temporal sense, and yet, on Zahar’s reading, it could have been: it was “use-novel” and thus fresh for use to confirm the theory. For a more mundane illustration, so-called cross-validation techniques represent a routine application of the UN idea in statistical settings (as pointed out by Schurz 2014, 92; also see Forster 2007, 592 ff.). According to some commentators, however, the UN criterion needs further elaboration (see Hitchcock and Sober 2004 and Lipton 2005), while others have criticized it as essentially wrong-headed (see Howson 1990 and Mayo 1991, 2014; also see Votsis 2014). Yet another way to enrich naïve HD is to combine it with eliminativism. According to this view, the naïve HD clause for confirmation is too weak because there must have been a low (enough) objective chance of getting the outcome \(e\) (favorable to \(h\)) if \(h\) was false, so that few possibilities exist that \(e\) may have occurred for some reason other than the truth of \(h\). Briefly put, the occurrence of \(e\) must be such that most alternatives to \(h\) can be safely ruled out. The founding figure of eliminativism is Francis Bacon (1561–1626). John Stuart Mill (1843/1872) is a major representative in later times, and Deborah Mayo’s “error-statistical” approach to hypothesis testing arguably develops this tradition (Mayo 1996 and Mayo and Spanos 2010; see Bird 2010, Kitcher 1993, 219 ff., and Meehl 1990 for other contemporary variations). Eliminativism is most credible when experimentation is at issue (see, e.g., Guala 2012). Indeed, the appeal to Bacon’s idea of crucial experiment (instantia crucis) and related notions (e.g., “severe testing”) is a fairly reliable mark of eliminativist inclinations. Experimentation is, to a large extent, precisely an array of techniques to keep undesired interfering factors at a minimum by active manipulation and deliberate control (think of the blinding procedure in medical trials, with \(h\) the hypothesized effectiveness of a novel treatment and \(e\) a relative improvement in clinical endpoints for a target subsample of patients thus treated). When this kind of control obtains, popular statistical tools are supposed to allow for the calculation of the probability of \(e\) in case \(h\) is false meant as a “relative frequency in a (real or hypothetical) series of test applications” (Mayo 1991, 529), and to secure a sufficiently low value to validate the positive outcome of the test. It is much less clear how firm a grip this approach can retain when inference takes place at higher levels of generality and theoretical commitment, where the hypothesis space is typically much too poorly ordered to fit routine error-statistical analyses. Indeed, Laudan (1997, 315; also see Musgrave 2010) spotted in this approach the risk of a “balkanization” of scientific reasoning, namely, a restricted focus on scattered pieces of experimental inference (but see Mayo 2010 for a defense). Naïve HD can also be enriched by the notion of simplicity. According to this view, the naïve HD clause for confirmation is too weak because \(h\wedge k\) must be a simple (enough), unified way to account for evidence \(e\). A classic reference for the simplicity view is Newton’s first law of philosophizing in the Principia (“admit no more causes of natural things than such as are both true and sufficient to explain their appearances”), echoing very closely Ockham’s razor. This basic idea has never lost its appeal—even up to recent times (see, e.g., Quine and Ullian 1970, 69 ff.; Sober 1975; Zellner, Keuzenkamp, and McAleer 2002; Scorzato 2013). Despite Thomas Kuhn’s (1957, 181) suggestions to the contrary, the success of Copernican astronomy over Ptolemy’s system has remained an influential case study fostering the simplicity view (Martens 2009). Moreover, in ordinary scientific problems such as curve fitting, formal criteria of model selection are applied where the paucity of parameters can be interpreted naturally as a key dimension of simplicity (Forster and Sober 1994). Traditionally, two main problems have proven pressing, and frustrating, for the simplicity approach. First, how to provide a sufficiently coherent and illuminating explication of this multifaceted and elusive notion (see Riesch 2010); and second, how to justify the role of simplicity as a properly epistemic (rather than merely pragmatic) virtue (see Kelly 2007, 2008). Finally, naïve HD can be enriched by the appeal to explanation. Here, the naïve HD clause for confirmation is meant to be too weak because \(h\wedge k\) must be able (not only to entail, but) to explain \(e\). By this move, the HD approach embeds the slogan of the so-called inference to the best explanation view: “observations support the hypothesis precisely because it would explain them” (Lipton 2000, 185; also see Lipton 2004). Historically, the main source for this connection between explanation and support is found in the work of Charles Sanders Peirce (1839–1914). Janssen (2003) offers a particularly neat contemporary exhibit, explicitly aimed at “curing cases of the Duhem-Quine disease” (484; also see Thagard 1978, and Douven 2017 for a relevant survey). Quite unlike eliminativist approaches, explanationist analyses tend to focus on large-scale theories and relatively high-level kinds of evidence. Dealing with Einstein’s general relativity, for instance, Janssen (2003) greatly emphasizes its explanation of the equivalence of inertial and gravitational mass (essentially a brute fact in Newtonian physics) over the resolution of the puzzle of Mercury’s perihelion. Explanationist accounts are also distinctively well-equipped to address inference patterns from non-experimental sciences (Cleland 2011). The problems faced by these approaches are similar to those affecting the simplicity view. Agreement is still lacking on the nature of scientific explanation (see Woodward 2019) and it is not clear how far an explanationist variant of HD can go without a sound analysis of that notion. Moreover, some critics have wondered why the relationship of confirmation should be affected by an explanatory connection with the evidence per se (see Salmon 2001). The above discussion does not display an exhaustive list (nor are the listed options mutually exclusive, for that matter: see, e.g., Baker 2003; also see Worrall 2010 for some overlapping implications in an applied setting of real practical value). And our sketched presentation hardly allows for any conclusive assessment. It does suggest, however, that reports of the death of hypothetico-deductivism (see Earman 1992, 64, and Glymour 1980b) might have been exaggerated. For all its difficulties, HD has proven fairly resilient at least as a basic framework to elucidate some key aspects of how hypotheses can be confirmed by the evidence (see Betz 2013, Gemes 2005, and Sprenger 2011b for consonant points of view).