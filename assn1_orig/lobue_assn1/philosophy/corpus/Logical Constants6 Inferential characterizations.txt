 At the end of the section on topic neutrality, we distinguished two notions of topic neutrality. The first notion—insensitivity to the distinguishing features of individuals—is effectively captured by the permutation invariance criterion. How might we capture the second—universal applicability to all thought or reasoning, regardless of its subject matter? We might start by identifying certain ingredients that must be present in anything that is to count as thought or reasoning, then class as logical any expression that can be understood in terms of these ingredients alone. That would ensure a special connection between the logical constants and thought or reasoning as such, a connection that would explain logic’s universal applicability. Along these lines, it has been proposed that the logical constants are just those expressions that can be characterized by a set of purely inferential introduction and elimination  rules.[20]  To grasp the meaning of the conjunction connective \(\dq{\and}\), for example, it is arguably sufficient to learn that it is governed by the rules:  \begin{equation*} \frac{A, B}{A \and B} \quad \frac{A \and B}{A} \quad \frac{A \and B}{B} \end{equation*}  Thus the meaning of \(\dq{\and}\) can be grasped by anyone who understands the significance of the horizontal line in an inference rule. (Contrast \(\dq{\%}\) from the last section, which cannot be grasped by anyone who does not understand what a male is and what a widow is.) Anyone who is capable of articulate thought or reasoning at all should be able to understand these inference rules, and should therefore be in a position to grasp the meaning of \(\dq{\and}\). Or so the thought  goes.[21] To make such a proposal precise, we would have to make a number of additional decisions: We would have to decide whether to use natural deduction rules or sequent rules.  (See the entry on the development of   proof theory.) If we opted to use sequent rules, we would have to decide whether or not to allow “substructure” (see the entry on  substructural logics)  and whether to allow multiple conclusions in the sequents. We would also have to endorse a particular set of purely structural rules (rules not involving any expression of the language essentially). We would have to specify whether it is introduction or elimination rules, or both, that are to characterize the meanings of logical constants.[22]  (In a sequent formulation, we would have to distinguish between right and left introduction and elimination rules.) We would have to allow for subpropositional structure in our rules, in order to make room for quantifier rules. We would have to say when an introduction or elimination rule counts as “purely inferential,” to exclude rules like these:  \begin{equation*} \frac{a~\text{is red}}{Ra} \quad \frac{A, B, \text{water is}~H_{2}O}{A * B} \end{equation*}  The strictest criterion would allow only rules in which every sign, besides a single instance of the constant being characterized, is either structural (like the comma) or schematic (like \(\dq{A}\)). But although this condition is met by the standard rules for conjunction, it is not met by the natural deduction introduction rule for negation, which must employ either another logical constant (\(\dq{\bot}\)) or another instance of the negation sign than the one being introduced. Thus one must either relax the condition for being “purely inferential” or add more structure (see especially Belnap 1982). Different versions of the inferential characterization approach make different decisions about these matters, and these differences affect which constants get certified as “logical.” For example, if we use single-conclusion sequents with the standard rules for the constants, we get the intuitionistic connectives, while if we use multiple-conclusion sequents, we get the classical connectives (Kneale 1956, 253). If we adopt Došen’s constraints on acceptable rules (Došen 1994, 280), the S4 necessity operator gets counted as a logical constant, while if we adopt Hacking’s constraints, it doesn’t (Hacking 1979, 297). Thus, if we are to have any hope of deciding the hard cases in a principled way, we will have to motivate all of the decisions that distinguish our version of the inferential characterization approach from the others. Here, however, we will avoid getting into these issues of detail and focus instead on the basic idea. The basic idea is that the logical constants are distinguished from other sorts of expressions by being “characterizable” in terms of purely inferential rules. But what does “characterizable” mean here? As Gómez-Torrente (2002, 29) observes, it might be taken to require either the fixation of reference (semantic value) or the fixation of sense: Semantic value determination: A constant \(c\) is characterizable by rules \(R\) iff its being governed by \(R\) suffices to fix its reference or semantic value (for example, the truth function it expresses), given certain semantic background assumptions (Hacking 1979, 299, 313). Sense determination: A constant \(c\) is characterizable by rules \(R\) iff its being governed by \(R\) suffices to fix its sense: that is, one can grasp the sense of \(c\) simply by learning that it is governed by \(R\) (Popper 1946–7, 1947; Kneale 1956, 254–5; Peacocke 1987; Hodes 2004, 135). Let us consider these two versions of the inferential characterization approach in turn. Hacking shows that, given certain background semantic assumptions (bivalence, valid inference preserves truth), any introduction and elimination rules meeting certain proof-theoretic conditions (subformula property, provability of elimination theorems for Cut, Identity, and Weakening) will uniquely determine a semantics for the constant they govern (Hacking 1979, 311–314). It is in this sense that these rules “fix the meaning” of the constant: “they are such that if strong semantic assumptions of a general kind are made, then the specific semantics of the individual logical constants is thereby determined” (313). The notion of determination of semantic value in a well-defined semantic framework is, at least, clear—unlike the general notion of determination of sense. However, as Gómez-Torrente points out, by concentrating on the fixation of reference (or semantic value) rather than sense, Hacking opens himself up to an objection not unlike the objection to permutation-invariance approaches we considered above (see also Sainsbury 2001, 369). Consider the quantifier \(\dq{W}\), which means “not for all not …, if all are not male widows, and for all not …, if not all are not male widows” (Gómez-Torrente 2002, 29). (It is important here that \(\dq{W}\) is a primitive sign of the language, not one introduced by a definition in terms of \(\dq{\forall}\), \(\dq{\neg}\), “male”, and “widow”.) Since there are no male widows, \(\dq{W}\) has the same semantic value as our ordinary quantifier \(\dq{\exists}\). (As above, we can think of the semantic value of a quantifier as a function from sets of assignments to sets of assignments.) Now let \(R\) be the standard introduction and elimination rules for \(\dq{\exists}\), and let \(R'\) be the result of substituting \(\dq{W}\) for \(\dq{\exists}\) in these rules. Clearly, \(R'\) is no less “purely inferential” than \(R\). And if \(R\) fixes a semantic value for \(\dq{\exists}\), then \(R'\) fixes a semantic value—the very same semantic value—for \(\dq{W}\). So if logical constants are expressions whose semantic values can be fixed by means of purely inferential introduction and elimination rules, \(\dq{W}\) counts as a logical constant if and only if \(\dq{\exists}\) does. Yet intuitively there is an important difference between these constants. We might describe it this way: whereas learning the rules \(R\) is sufficient to impart a full grasp of \(\dq{\exists}\), one could learn the rules \(R'\) without fully understanding what is meant by \(\dq{W}\). To understand \(\dq{W}\) one must know about the human institution of marriage, and that accounts for our feeling that \(\dq{W}\) is not “topic-neutral” enough to be a logical constant. However, this difference between \(\dq{W}\) and \(\dq{\exists}\) cannot be discerned if we talk only of reference or semantic value; it is a difference in the senses of the two expressions. The idea that introduction and/or elimination rules fix the sense of a logical constant is often motivated by talk of the rules as defining the constant.  Gentzen remarks that the natural deduction rules “represent, as it were, the ‘definitions’ of the symbols concerned, and the eliminations are no more, in the final analysis, than the consequences of these definitions” (1935, §5.13; 1969, 80). However, a genuine definition would permit the constant to be eliminated from every context in which it occurs (see the entry on Definitions), and introduction and elimination rules for logical constants do not, in general, permit this. For example, in an intuitionistic sequent calculus, there is no sequent (or group of sequents) not containing \(\dq{\rightarrow}\) that is equivalent to the sequent \(\dq{A \rightarrow B \vdash C}\). For this reason, Kneale (1956, 257) says only that we can “treat” the rules as definitions, Hacking (1979) speaks of the rules “not as defining but only as characterizing the logical constants,” and Došen (1994) says that the rules provide only an “analysis,” not a  definition.[23] However, even if the rules are not “definitions,” there may still be something to say for the claim that they “fix the senses” of the constants they introduce. For it may be that a speaker’s grasp of the meaning of the constants consists in her mastery of these rules: her disposition to accept inferences conforming to the rules as “primitively compelling” (Peacocke 1987, Hodes 2004). (A speaker finds an inference form primitively compelling just in case she finds it compelling and does not take its correctness to require external ratification, e.g. by inference.) If the senses of logical constants are individuated in this way by the conditions for their grasp, we can distinguish between truth-functionally equivalent constants with different meanings, like \(\dq{\vee}\), \(\dq{\ddagger}\), and \(\dq{\dagger}\), as defined below: To understand \(\dq{\vee}\) one must find the standard introduction rules primitively compelling: To understand \(\dq{\ddagger}\) one must find the following elimination rule primitively compelling: Finally, to grasp the sense of \(\dq{\dagger}\) one must find these introduction rules primitively compelling: \(\dq{\vee}\) and \(\dq{\ddagger}\) will count as logical constants, because their sense-constitutive rules are purely inferential, while \(\dq{\dagger}\) will not, because its rules are not. (In the same way we can distinguish \(\dq{\exists}\) from \(\dq{W}\).) Note that appropriately rewritten versions of \(\refp{or-intro}\) will hold for \(\dq{\ddagger}\) and \(\dq{\dagger}\); the difference is that one can grasp \(\dq{\ddagger}\) and \(\dq{\dagger}\) (but not \(\dq{\vee}\)) without finding these rules primitively compelling (Peacocke 1987, 156; cp. Sainsbury 2001, 370–1). Some critics have doubted that the introduction and elimination rules for the logical constants exhaust the aspects of the use of these constants that must be mastered if one is to understand them. For example, it has been suggested that in order to grasp the conditional and the universal quantifier, one must be disposed to treat certain kinds of inductive evidence as grounds for the assertion of conditionals and universally quantified claims (Dummett 1991, 275–8; Gómez-Torrente 2002, 26–7; Sainsbury 2001, 370–1). It is not clear that these additional aspects of use can be captured in “purely inferential” rules, or that they can be derived from aspects of use that can be so captured. It is sometimes thought that Prior’s (1960) example of a connective “tonk,” whose rules permit inferring anything from anything, decisively refutes the idea that the senses of logical constants are fixed by their introduction and/or elimination rules. But although Prior’s example (anticipated in Popper 1946–7, 284) certainly shows that not all sets of introduction and elimination rules determine a coherent meaning for a logical constant, it does not show that none do, or that the logical constants are not distinctive in having their meanings determined in this way. For some attempts to articulate conditions under which introduction and elimination rules do fix a meaning, see Belnap (1962), Hacking (1979, 296–8), Kremer (1988, 62–6), and Hodes (2004, 156–7). Prawitz (1985; 2005) argues that  any formally suitable introduction rule can fix the meaning for a logical constant. On Prawitz’s view, the lesson we learn from Prior is that we cannot also stipulate an elimination rule, but must justify any proposed elimination rule by showing that there is a procedure for rearranging any direct proof of the premises of the elimination rule into a direct proof of the conclusion. Thus, we can stipulate the introduction rule for “tonk”, but must then content ourselves with the strongest elimination rule for which such a procedure is available: Other philosophers reject Prawitz's (and Gentzen's) view that the introduction rules have priority in fixing the meanings of constants, but retain the idea that the introduction and elimination rules that fix the meaning of a constant must be in harmony: the elimination rules must not permit us to infer more from a compound sentence than would be justified by the premises of the corresponding introduction rules (Dummett 1981, 396; Tennant 1987, 76-98). (For analyses of various notions of harmony, and their relation to notions like normalizability and conservativeness, see Milne 1994, Read 2010, and Steinberger 2011.)