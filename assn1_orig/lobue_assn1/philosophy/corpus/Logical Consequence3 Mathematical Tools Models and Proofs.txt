 Twentieth Century technical work on the notion of logical consequence has centered on two different mathematical tools, proof theory and model theory. Each of these can be seen as explicating different aspects of the concept of logical consequence, backed by different philosophical perspectives. We have characterized logical consequence as necessary truth preservation in virtue of form. This idea can be explicated formally. One can use mathematical structures to account for the range of possibilities over which truth needs to be preserved. The formality of logical consequence can be explicated formally by giving a special role to the logical vocabulary, taken as constituting the forms of sentences. Let us see how model theory attends to both these tasks. The model-centered approach to logical consequence takes the validity of an argument to be absence of counterexample. A counterexample to an argument is, in general, some way of manifesting the manner in which the premises of the argument fail to lead to a conclusion. One way to do this is to provide an argument of the same form for which the premises are clearly true and the conclusion is clearly false. Another way to do this is to provide a circumstance in which the premises are true and the conclusion is false. In the contemporary literature, the intuitive idea of a counterexample is developed into a theory of models. The exact structure of a model will depend on the kind of language at hand (extensional/intensional, first/higher-order, etc.). A model for an extensional first order language consists of a non-empty set which constitutes the domain, and an interpretation function, which assigns to each nonlogical term an extension over the domain—any extension agreeing with its semantic type (individual constants are assigned elements of the domain, function symbols are assigned functions from the domain to itself, one-place first-order predicates are assigned subsets of the domain, etc.). The contemporary model-theoretic definition of logical consequence traces back to Tarski (1936). It builds on the definition of truth in a model given by Tarski in (1935). Tarski defines a true sentence in a model recursively, by giving truth (or satisfaction) conditions on the logical vocabulary. A conjunction, for example, is true in a model if and only if both conjuncts are true in that model. A universally quantified sentence \(\forall xFx\) is true in a model if and only if each instance is true in the model. (Or, on the Tarskian account of satisfaction, if and only if the open sentence \(Fx\) is satisfied by every object in the domain of the model. For detail on how this is accomplished, see the entry on  Tarski’s truth definitions.)  Now we can define logical consequence as preservation of truth over models: an argument is valid if in any model in which the premises are true (or in any interpretation of the premises according to which they are true), the conclusion is true too. The model-theoretic definition is one of the most successful mathematical explications of a philosophical concept to date. It promises to capture both the necessity of logical consequence—by looking at truth over all models, and the formality of logical consequence—by varying the interpretations of the nonlogical vocabulary across models: an argument is valid no matter what the nonlogical vocabulary means. Yet, models are just sets, which are merely mathematical objects. How do they account for the range of possibilities, or circumstances required? John Etchemendy (1990) offers two perspectives for understanding models. On the representational approach, each model is taken to represent a possible world. If an argument preserves truth over models, we are then guaranteed that it preserves truth over possible worlds, and if we accept the identification of necessity with truth in all possible worlds, we have the necessary truth preservation of logical consequence. The problem with this approach is that it identifies logical consequence with metaphysical consequence, and it gives no account of the formality of logical consequence. On the representational approach, there is no basis for a distinction between the logical and the nonlogical vocabulary, and there is no explanation of why the interpretations of the nonlogical vocabulary are maximally varied. The second perspective on models is afforded by the interpretational approach, by which each model assigns extensions to the nonlogical vocabulary from the actual world: what varies between models is not the world depicted but the meaning of the terms. Here, the worry is that necessity isn’t captured. For instance, on the usual division of the vocabulary into logical and nonlogical, identity is considered a logical term, and can be used to form statements about the cardinality of the domain (e.g., ‘‘there are at least two things’’) which are true under every reinterpretation, but perhaps are not necessarily true. On this approach, there is no basis for considering models with domains other than the universe of what actually exists, and specifically, there is no explanation of model theory’s use of domains of different sizes. Each approach, as described here, is flawed with respect to our analysis of logical consequence as necessary and formal. The interpretational approach, by looking only at the actual world fails to account for necessity, and the representational approach fails to account for formality (for details, see Etchemendy 1990, Sher 1996, and Shapiro 1998, and for refinements see Etchemendy 2008). A possible response to Etchemendy would be to blend the representational and the interpretational perspectives, viewing each model as representing a possible world under a re-interpretation of the nonlogical vocabulary (Shapiro 1998, see also Sher 1996 and Hanson 1997 for alternative responses). One of the main challenges set by the model-theoretic definition of logical consequence is to distinguish between the logical and the nonlogical vocabulary. The logical vocabulary is defined in all models by the recursive clauses (such as those mentioned above for conjunction and the universal quantifier), and in that sense its meaning is fixed. The choice of the logical vocabulary determines the class of models considered when evaluating validity, and thus it determines the class of the logically valid arguments. Now, while each formal language is typically defined with a choice of a logical vocabulary, one can ask for a more principled characterization of logical vocabulary. Tarski left the question of a principled distinction open in his 1936, and only gave the lines of a relativistic stance, by which different choices of the logical vocabulary may be admissible. Others have proposed criteria for logicality, demanding that logical constants be appropriately formal, general or topic neutral (for references and details, see the entry on  logical constants).  Note that a choice of the logical vocabulary is a special case of setting constraints on the class of models to be used. It has been suggested that the focus on criteria for the logical vocabulary misses this point, and that more generally the question is which semantic constraints should be adopted, limiting the admissible models for a language (Sagi 2014a, Zinke 2017). Another challenge faced by the model-theoretic account is due to the limitations of its set-theoretic basis. Recall that models are sets. The worry is that truth-preservation over models might not guarantee necessary truth preservation—moreover, it might not even guarantee material truth preservation (truth preservation in the actual world). The reason is that each model domain is a set, but the actual world presumably contains all sets, and as a collection which includes all sets is too ‘‘large’’ to be a set (it constitutes a proper class), the actual world is not accounted for by any model (see Shapiro 1987). One way of dealing with this worry is to employ external means, such as proof theory, in support of the model-theoretic definition. This is done by Georg Kreisel in his “squeezing argument”, which we present in section 3.3. Kreisel’s argument crucially depends on the language in question having a sound and complete proof system. Another option is to use set-theoretic reflection principles. Generally speaking, reflection principles state that whatever is true of the universe of sets, is already true in an initial segment thereof (which is always a set). If reflection principles are accepted, then at least as concerns the relevant language, one can argue that an argument is valid if and only if there is no counter set-model (see Kreisel 1967, Shapiro 1987, Kennedy & Väänänen 2017). Finally, the explanation of logical consequence in terms of truth in models is typically preferred by “Realists”, who take truth of sentences to be independent of what can be known. Explaining logical consequence in terms of truth in models is rather close to explaining logical consequence in terms of truth, and the analysis of truth-in-a-model is sometimes taken to be an explication of truth in terms of correspondence, a typically Realist notion. Some, however, view logical consequence as having an indispensable epistemic component, having to do with the way we establish the conclusion on the basis of the premises. “Anti-realists”, who eschew taking truth (or at least, correspondence-truth) as an explanatory notion, will typically prefer explaining logical consequence in terms of proof—to which we turn next. On the proof-centered approach to logical consequence, the validity of an argument amounts to there being a proof of the conclusions from the premises. Exactly what proofs are is a big issue but the idea is fairly plain (at least if you have been exposed to some proof system or other). Proofs are made up of small steps, the primitive inference principles of the proof system. The 20th Century has seen very many different kinds of proof systems, from so-called Hilbert proofs, with simple rules and complex axioms, to natural deduction systems, with few (or even no) axioms and very many rules. The proof-centered approach highlights epistemic aspects of logical consequence. A proof does not merely attest to the validity of the argument: it provides the steps by which we can establish this validity. And so, if a reasoner has grounds for the premises of an argument, and they infer the conclusion via a series of applications of valid inference rules, they thereby obtain grounds for the conclusion (see Prawitz 2012). One can go further and subscribe to inferentialism, the view by which the meaning of expressions is determined by their role in inference. The idea is that our use of a linguistic expression is regulated by rules, and mastering the rules suffices for understanding the expression. This gives us a preliminary restriction on what semantic values of expressions can be: they cannot make any distinctions not accounted for by the rules. One can then go even further, and reject any kind of meaning that goes beyond the rules—adopting the later Wittgensteinian slogan “meaning is use”. This view is favored by anti-realists about meaning, since meaning on this view is fully explained by what is knowable. The condition of necessity on logical consequence obtains a new interpretation in the proof-centered approach. The condition can be reformulated thus: in a valid argument, the truth of the conclusion follows from the truth of the premises by necessity of thought (Prawitz 2005). Let us parse this formulation. Truth is understood constructively: sentences are true in virtue of potential evidence for them, and the facts described by true sentences are thus conceived as constructed in terms of potential evidence. (Note that one can completely forgo reference to truth, and instead speak of assertibility or acceptance of sentences.) Now, the necessity of thought by which an argument is valid is explained by the meaning of the terms involved, which compels us to accept the truth of the conclusion given the truth of the premises. Meanings of expressions, in turn, are understood through the rules governing their use: the usual truth conditions give their way to proof conditions of formulas containing an expression. One can thus provide a proof-theoretic semantics for a language (Schroeder-Heister 1991). When presenting his system of natural deduction, Gentzen remarked that the introduction rules for the logical expressions represent their “definitions,” and the elimination rules are consequences of those definitions (Gentzen 1933). For example, the introduction rule for conjunction dictates that a conjunction \(A \amp B\) may be inferred from both conjuncts \(A\) and \(B\), and this rule captures the meaning of the connective. Conversely, the elimination rule for conjunction says that from \(A \amp B\) one may infer both \(A\) and \(B\). The universal quantifier rules tell us that from the universally quantified claim \(\forall xFx\) we can infer any instance \(Fa\), and we can infer \(\forall xFx\) from the instance \(Fa\), provided that no other assumption has been made involving the name \(a\). Under certain requirements, one can show that the elimination rule is validated by the introduction rule. One of the main challenges for the proof-centered approach is that of distinguishing between rules that are genuinely meaning-determining and those that are not. Some rules for connectives, if added to a system, would lead to triviality. Prior (1960) offered the following rules for a connective “\(\tonk\)”. Its introduction rule says that from \(A\) one can infer \(A \tonk B\), and its elimination rule says that from \(A \tonk B\) one can infer \(B\). With the introduction of these rules, the system becomes trivial so long as at least one thing is provable, since from any assumption \(A\) one can derive any conclusion \(B\). Some constraints have to posed on inference rules, and much of subsequent literature has been concerned with these constraints (Belnap 1962, Dummett 1991, Prawitz 1974). To render the notions of proof and validity more systematized, Prawitz has introduced the notion of a canonical proof. A sentence might be proved in several different ways, but it is the direct, or canonical proof that is constitutive of its meaning. A canonical proof is a proof whose last step is an application of an introduction rule, and its immediate subproofs are canonical (unless they have free variables or undischarged assumptions—for details see Prawitz 2005). A canonical proof is conceived as giving direct evidence for the sentence proved, as it establishes the truth of the sentence by the rule constitutive of the meaning of its connectives. For more on canonical proofs and the ways other proofs can be reduced to them, see the entry on  proof-theoretic semantics. We have indicated how the condition of necessity can be interpreted in the proof-centered approach. The condition of formality can be accounted for as well. Note that on the present perspective as well, there is a division of the vocabulary into logical and nonlogical. This division can be used to define substitutions of an argument. A substitution of an argument is an argument obtained from the original one by replacing the nonlogical terms with terms of the same syntactic category in a uniform manner. A definition of validity that respects the condition of formality will entail that an argument is valid if and only if all its substitutions are valid, and in the present context, this is a requirement that there is a proof of all its substitutions. This condition is satisfied in any proof system where rules are given only for the logical vocabulary. Of course, in the proof-centered approach as well, there is a question of distinguishing the logical vocabulary (see the entry on  logical constants). Finally, it should be noted that a proof theoretic semantics can be given for classical logic as well as a variety of non-classical logics. However, due to the epistemic anti-realist attitude that lies at the basis of the proof-centered approach, its proponents have typically advocated  intuitionistic logic  (see Dummett 1991). For more on the proof-centered perspective and on proof-theoretic semantics, see the entry on  proof-theoretic semantics. The proof-theoretic and model-theoretic perspectives have been considered as providing rival accounts of logical consequence. However, one can also view “logical consequence” and “validity” as expressing cluster concepts: “A number of different, closely related notions go by those names. They invoke matters of modality, meaning, effectiveness, justification, rationality, and form” (Shapiro 2014). One can also note that the division between the model-theoretic and the proof-theoretic perspectives is a modern one, and it was only made possible when tools for metamathematical investigations were developed. Frege’s Begriffsschrift, for instance, which predates the development of those tools, is formulated as an axiomatic proof system, but the meanings of the connectives are given via truth conditions. Once there are two different analyses of a relation of logical consequence, one can ask about possible interactions, and we’ll do that next. One can also ask what general features such a relation has independently of its analysis as proof-theoretic or model-theoretic. One way of answering this question goes back to  Tarski,  who introduced the notion of consequence operations. For our purposes, we note only some features of such operations. Let \(Cn(X)\) be the consequences of \(X\). (One can think of the operator \(Cn\) as deriving from a prior consequence relation which, when taking \(X\) as ‘input (or premise)’ set, tells you what follows from \(X\). But one can also see the ‘process’ in reverse, and a key insight is that consequence relations and corresponding operations are, in effect, interdefinable. See the entry on  algebraic propositional logic  for details.) Among some of the minimal conditions one might impose on a consequence relation are the following two (from Tarski): If you think of \(X\) as a set of claims, then the first condition tells you that the consequences of a set of claims includes the claims themselves. The second condition demands that the consequences of \(X\) just are the consequences of the consequences of \(X\). Both of these conditions can be motivated from reflection on the model-theoretic and proof-theoretic approaches; and there are other such conditions too. (For a general discussion, see the entry on  algebraic propositional logic.)  But as with many foundation issues (e.g., ‘what are the essential features of consequence relations in general?’), even such minimal conditions are contentious in philosophical logic and the philosophy of logic. For example, some might take condition (2) to be objectionable on the grounds that, for reasons of vagueness (or more), important consequence relations over natural languages (however formalized) are not generally transitive in ways reflected in (2). (See Tennant 1994, Cobreros et al 2012, and Ripley 2013, for philosophical motivations against transitive consequence.) But we leave these issues for more advanced discussion. While the philosophical divide between Realists and Anti-realists remains vast, proof-centered and model-centered accounts of consequence have been united (at least with respect to extension) in many cases. The great soundness and completeness theorems for different proof systems (or, from the other angle, for different model-theoretic semantics) show that, in an important sense, the two approaches often coincide, at least in extension. A proof system is sound with respect to a model-theoretic semantics if every argument that has a proof in the system is model-theoretically valid. A proof system is complete with respect to a model-theoretic semantics if every model-theoretically valid argument has a proof in the system. While soundness is a principal condition on any proof system worth its name, completeness cannot always be expected. Admittedly, these definitions are biased towards the model-theoretic perspective: the model-theoretic semantic sets the standard to what is “sound” and “complete”. Leaving terminological issues aside, if a proof system is both sound and complete with respect to a model-theoretic semantics (as, significantly, in the case of first order predicate logic), then the proof system and the model-theoretic semantics agree on which arguments are valid. Completeness results can also support the adequacy of the model-theoretic account, as in Kreisel’s “squeezing argument”. We have noted a weakness of the model-theoretic account: all models are sets, and so it might be that no model represents the actual world. Kreisel has shown that if we have a proof system that is “intuitively sound” and is complete with respect to the model-theoretic semantics, we won’t be missing any models: every intuitively valid argument will have a counter-model. Let \(L\) be a first order language. Let \(Val\) denote the set of intuitively valid arguments in \(L\). Kreisel takes intuitive validity to be preservation of truth across all structures (whether sets or not). His analysis privileges the modal analysis of logical consequence—but note that the weakness we are addressing is that considering set-theoretic structures might not be enough. Let \(V\) denote the set of model-theoretic validities in \(L\): arguments that preserve truth over models. Let \(D\) be the set of deductively valid arguments, by some accepted proof system for first order logic. Now, any such proof system is “intuitively sound”, meaning that what is deductively valid by the system is intuitively valid. This gives us \(D \subseteq Val\). And obviously, by the definitions we’ve given, \(Val \subseteq V\), since an argument that preserves truth over all structures will preserve truth over set-structures. By the completeness result for first order logic, we have: \(V\) &subseteq \(D\). Putting the three inclusions together (the “squeeze”), we get that all three sets must be equal, and in particular: \(V = Val\). In this way, we’ve proven that if there is some structure that is a counterexample to a first order argument, then there is a set-theoretic one. Another arena for the interaction between the proof-theoretic and the model-theoretic perspectives has to do with the definition of the logical vocabulary. For example, one can hold a “moderate” inferentialist view which defines the meanings of logical connectives through their semantics (i.e. truth conditions) but demands that the meaning of a connective be determined by inference rules. Carnap has famously shown that the classical inference rules allow non-standard interpretations of the logical expressions (Carnap 1943). Much recent work in the field has been devoted to the exact nature and extent of Carnap’s categoricity problem (Raatikainen 2008, Murzi and Hjortland 2009, Woods 2012, Garson 2013, Peregrin 2014, Bonnay and Westerståhl 2016. See also the entry on  sentence connectives in formal logic). Finally, we should note that while model theory and proof theory are the most prominent contenders for the explication of logical consequence, there are alternative frameworks for formal semantics such as  algebraic semantics,  game-theoretic semantics and  dynamic semantics  (see Wansig 2000).