 Time and temporal reasoning have been associated with logic since the origins of scientific logic with Aristotle. The idea of a logic of tense in the modern sense has been familiar since at least the work of Jan Łukasiewicz (see, for instance,  Łukasiewicz 1970),  but the shape of what is commonly known as tense logic was standardized by Arthur Prior’s work in the 1950s and 1960s: see Prior  1956,  1967,  1968.[22]  As the topic was developed in philosophical logic, tense logic proved to be a species of modal logic; Prior’s work was heavily influenced by both Hintikka and Kripke, and by the idea that the truth of tense-logical formulas is relative to world-states or temporal stages of the world; these are the tense-theoretic analogues of the timeless possible worlds of ordinary modal logic. Thus, the central logical problems and techniques of tense logic were borrowed from modal logic. For instance, it became a research theme to work out the relations between axiomatic systems and the corresponding model theoretic constraints on temporal orderings. See, for instance,  Burgess 1984  and  van Benthem 1983. Priorian tense logic shares with modal logic a technical concentration on issues that arise from using the first-order theory of relations to explain the logical phenomena, an expectation that the important temporal operators will be quantifiers over world-states, and a rather remote and foundational approach to actual specimens of temporal reasoning. Of course, these temporal logics do yield validities, such as (if A, then it was the case that A was going to be the case), which certainly are intuitively valid. But at most, these can only play a broadly foundational role in accounting for realistic reasoning about time. It is hard to think of realistic examples in which they play a leading part. This characteristic, of course, is one that modal logic shares with most traditional and modern logical theories; the connection with everyday reasoning is rather weak. Although modern logical techniques do account with some success for the reasoning involved in verifying mathematical proofs and logic puzzles, they do not explain other cases of technical or common sense reasoning with much detail or plausibility. Even in cases like legal reasoning, where logicians and logically-minded legal theorists have put much effort into formalizing the reasoning, the utility of the results is controversial. Planning problems provide one of the most fruitful showcases for combining logical analysis with AI applications. On the one hand there are many practically important applications of automated planning, and on the other logical formalizations of planning are genuinely helpful in understanding the problems, and in designing algorithms. The classical representation of an AI planning problem, as described in  Amarel 1968,  evidently originates in early work of Herbert Simon’s, published in a 1966 CMU technical report,  Simon 1966.  In such a problem, an agent in an initial world-state is equipped with a set of actions, which are thought of as partial functions transforming world-states into world-states. Actions are feasible only in world-states that meet certain constraints.  (These constraints are now called the “preconditions” of the action.)  A planning problem then becomes a search for a series of feasible actions that successively transform the initial world-state into a desired world-state. The Situation Calculus, developed by John McCarthy, is the origin of most of the later work in formalizing reasoning about action and change. It was first described in 1969, in  McCarthy 1983;  the earliest generally accessible publication on the topic is  McCarthy & Hayes 1969. Apparently, Priorian tense logic had no influence on  Amarel 1968.  But there is no important difference between Amarel’s world-states and those of Priorian tense logic. The “situations” of the Situation Calculus are these same world-states, under a new  name.[23]  They resemble possible worlds in modal logic in providing abstract locations that support a consistent and complete collection of truths. As in tense logic, these locations are ordered, and change is represented by the variation in truths from one location to another. The crucial difference between the Situation Calculus and tense logic is that change in the situation is dynamic—changes do not merely occur, but occur for a reason. This difference, of course, is inspired by the intended use of the Situation Calculus: it is meant to formalize Simon’s representation of the planning problem, in which a single agent reasons about the scenarios in which a series of actions is  performed.[24]  In this model, what drives change is the performance of actions, so the fundamental model theoretic relation is the relation between an action a, an initial situation s in which a is performed, and a resulting situation s′ immediately subsequent to the performance of the action. Usually (though this is not absolutely necessary) the deterministic assumption is made that s′ is unique. In general, actions can be successfully performed only under certain limited circumstances. This could be modeled by allowing for cases in which there is no s′ such that RESULT(a,s,s′). But usually, it is assumed that RESULT is in fact a total function, but that in cases in which s does not meet the “preconditions” of a, there are no restrictions on the s′ satisfying RESULT(a,s,s′), so that the causal effects of a will be entirely unconstrained in such cases. A planning problem starts with a limited repertoire of actions (where sets of preconditions and effects are associated with each action), an initial situation, and a goal (which can be treated as a formula). A planning problem is a matter of finding a sequence of actions that will achieve the goal, given the initial situation. That is, given a goal G and initial situation s, the problem will consist of finding a sequence s1,...,sn of actions which will transform s into a final situation that satisfies G. This means (assuming that RESULT is a function) that G will be satisfied by the situation sn, where s0 = s and si+1 is the s′ such that RESULT(ai+1,si,s′). The planning problem is in effect a search for a sequence of actions meeting these conditions. The success conditions for the search can be characterized in a formalism like the Situation Calculus, which allows information about the results of actions to be expressed. Nothing has been said up till now about the actual language of the Situation Calculus. The crucial thing is how change is to be expressed. With tense logic in mind, it would be natural to invoke a modality like [a]A,  with the truth condition This formalization, in the style of dynamic logic, is in fact a leading candidate; see  Section 4.7,  below. But  McCarthy & Hayes 1969  deploys a language that is much closer to first-order logic. (This formalization style is characteristic of McCarthy’s work; see  McCarthy 1979.)  Actions are treated as individuals. And certain propositions whose truth values can change over time (propositional fluents) are also treated as individuals. Where s is a situation and f is a fluent, Holds(f,s) says that f is true in s. Since the pioneering work of the nineteenth and early twentieth century logicians, the process of formalizing mathematical domains has largely become a matter of routine. Although (as with set theory) there may be controversies about what axioms and logical infrastructure best serve to formalize an area of mathematics, the methods of formalization and the criteria for evaluating them are relatively unproblematic. This methodological clarity has not been successfully extended to other domains; even the formalization of the empirical sciences presents difficult problems that have not yet been  resolved.[25] The formalization of temporal reasoning, and in particular of reasoning about actions and plans, is the best-developed successful extension of modern formalization techniques to domains other than mathematical theories.  This departure has required the creation of new methodologies.  One methodological innovation will emerge in  Section 4.5: the development of a library of scenarios for testing the adequacy of various formalisms, and the creation of specialized domains like the blocks-world domain (mentioned above, in Section 4.2) that serve a laboratories for testing ideas.  For more on the blocks world, see  Genesereth & Nilsson 1987;  Davis 1991.  McCarthy’s ideas about elaboration tolerance McCarthy 1999  provide one interesting attempt to provide a criterion for the adequacy of formalizations. Another idea that has emerged in the course of formalizing common sense domains is the importance of an explicit ontology; see, for instance,  Fikes 1996 and  Lenat & Guha 1989.  Another is the potential usefulness of explicit representations of context; see  Guha, 1991.   Another is the use of simulation techniques: see, for instance,   Johnstone & Williamson 2007 To tell whether a plan achieves its goal, you need to see whether the goal holds in the plan’s final state. Doing this requires predictive reasoning, a type of reasoning that was neglected in the tense-logical literature. As in mechanics, prediction involves the inference of later states from earlier ones. But (in the case of simple planning problems at least) the dynamics are determined by actions rather than by differential equations. The investigation of this qualitative form of temporal reasoning, and of related sorts of reasoning (e.g., plan recognition, which seeks to infer goals from observed actions, and narrative explanation, which seeks to fill in implicit information in a temporal narrative) is one of the most impressive chapters in the brief history of common sense logicism. The essence of prediction is the problem of inferring what holds in the situation that ensues from performing an action, given information about the initial situation. It is often assumed that the agent has complete knowledge about the initial situation—this assumption is usual in classical formalizations of  planning.[26] A large part of the qualitative dynamics that is needed for planning consists in inferring what does not change. Take a simple plan to type the word ‘cat’ using word processing software:  the natural plan is to first enter ‘c’, then enter ‘a’, then enter ‘t’. Part of one’s confidence in this plan is that the actions are independent: for instance, entering ‘a’ does not also erase the ‘c’. The required inference can be thought of as a form of inertia. The Frame Problem is the problem of how to formalize the required inertial reasoning. The Frame Problem was named and introduced in  McCarthy & Hayes 1969. Unlike most of the philosophically interesting technical problems to emerge in AI, it has attracted the interest of philosophers; most of the relevant papers, and background information, can be found in  Ford & Pylyshyn 1996;  Pylyshyn 1987.  Both of these volumes document interactions between AI and philosophy. The quality of these interactions is discouraging. Like any realistic common sense reasoning problem, the Frame Problem is open-ended, and can depend on a wide variety of circumstances. If you put $20 in a wallet, put the wallet in your pocket, and go to the store, you can safely assume that the $20 is still in the wallet.  But if you leave the $20 on the counter at the store while shopping, you can’t safely assume it will be there later.  This may account for the temptation that makes some  philosophers[27]  want to construe the Frame Problem very broadly, so that very soon it becomes indiscernible from the problem of formalizing general common sense in arbitrary domains. Such a broad construal may serve to introduce speculative discussions concerning the nature of AI, but it loses all contact with the genuine, new logical problems in temporal reasoning that have been discovered by the AI community. It provides a forum for repeating some familiar philosophical themes, but it brings nothing new to philosophy. This way of interpreting the Frame Problem is disappointing, because philosophy can use all the help it can get; the AI community has succeeded in extending and enriching the application of logic to common sense reasoning in dramatic ways that are highly relevant to philosophy. The clearest account of these developments to be found in the volumes edited by Pylyshyn is  Morgenstern 1996.  An extended treatment can be found in  Shanahan 1997;  also see  Sandewall 1994 and  Shanahan 2009. The purely logical Frame Problem can be solved using monotonic logic, by simply writing explicit axioms stating what does not change when an action is performed. This technique can be successfully applied to quite complex formalization problems.[28] But nonmonotonic solutions to the framework have been extensively investigated and deployed; these lead to new and interesting lines of logical development. Some philosophers  (Fodor 1987,  Lormand 1996)  have felt that contrived propositions will pose special difficulties in connection with the Frame Problem. As Shanahan points out  Shanahan 1997  [p. 24]) Fodor’s “fridgeon” example is readily formalized in the Situation Calculus and poses no special problems. However, as Lormand suggests, Goodman’s examples  Goodman, 1946  do create problems if they are admitted as fluents; there will be anomalous extensions in which objects change from green to blue in order to preserve their grueness. This is one of the few points about the Frame Problem made by a philosopher  that raises a genuine difficulty for the formal solutions. But the difficulty is peripheral, since the example is not realistic. Recall that fluents are represented as first-order individuals. Although fluents are situation-dependent functions, an axiom of comprehension is certainly not assumed for fluents. In fact, it is generally supposed that the domain of fluents will be a very limited set of the totality of situation-dependent functions; typically, it will be a relatively small finite set of variables representing features of the domain considered to be important.  In particular cases these will be chosen in much the same way that a set of variables is chosen in statistical modeling. There doesn’t seem to be a systematic account in the AI literature of how to choose an appropriate set of fluents, but it would certainly be part of such an account that all fluents should correspond to projectable predicates, in Goodman’s sense. The idea behind nonmonotonic solutions to the Frame Problem is to treat inertia as a default; changes are assumed to occur only if there is some special reason for them to occur. In an action-centered account of change, this means that absence of change is assumed when an action is performed unless a reason for the change can be found in axioms for the action. For explicitness, let’s use Reiter’s default logic to illustrate the formalization. Recall that in Reiter’s theory, defaults are represented as rules, not formulas, so that they are not subject to quantification. To formalize inertia, then, we need to use default rule schemata. For each fluent f, action a, and situation s, the set of these schemata will include an instance of the following schema: This way of doing things makes any case in which a fluent changes truth value a prima facie anomaly. But it follows from Reiter’s account of extensions that such defaults are overridden when they conflict with the monotonic theory of situation dynamics. So if, for instance, there is a monotonic causal axiom for the action blacken ensuring that blackening a block will make it black in the resulting situation, then the appropriate instance of IR will be inefficacious, and there will be no extension in which a white block remains white when it is blackened. The Frame Problem somehow managed to capture the attention of a wide community—but if one is interested in understanding the complex problems that arise in generalizing formalisms like the Situation Calculus, while at the same time ensuring that they deliver plausible solutions to a wide variety of scenarios, it is more useful to consider a larger range of problems. For the AI community, the larger problems include the Frame Problem itself, the Qualification Problem, the Ramification Problem, generalizability along a number of important dimensions including incomplete information, concurrency (multiple agents), and continuous change, and finally a large assortment of specific challenges such as the scenarios mentioned later in this section. The Qualification Problem arises generally in connection with the formalization of common sense generalizations. Typically, these involve exceptions, and these exceptions—especially if one is willing to entertain far-fetched circumstances—can iterate endlessly. The same phenomenon, under a label like ‘the problem of ceteris paribus generalizations’, is familiar from analytic philosophy. It also comes up in the semantics of generic constructions found in natural languages.[29] In a sense, this problem is addressed at a general level by nonmonotonic logics, which—though they do not provide a way to enumerate exceptions—do allow common sense generalizations to be formulated as defaults, as well as enabling further qualifications to be added nondestructively. Ideally, then, the initial generalization can be stated as an axiom and qualifications can be added incrementally in the form of further axioms. The Qualification Problem was raised in  McCarthy 1986,  where it was motivated chiefly by generalizations concerning the consequences of actions; McCarthy considers in some detail the generalization that turning the ignition key in an automobile will start the car. Much the same point, in fact, can be made about virtually any action, including stacking one block on another—the standard action that is used to illustrate the Situation Calculus. A circumscriptive approach to the Qualification Problem is presented in  Lifschitz 1987; this explicitly introduces the precondition relation between an action and its preconditions into the formalism, and circumscriptively minimizes preconditions, eliminating from preferred models any “unknown preconditions” that might render an action inefficacious. Several dimensions of the Qualification Problem remain as broad, challenging research problems. For one thing, not every nonmonotonic logic provides graceful mechanisms for qualification. Default logic, for instance, does not deliver the intuitively desired conclusions. Suppose one formalizes the common sense generalization that if you press the ‘a’ key on a computer it will type ‘a’ as a normal default: If we then formalize the exception to this generalization that if you press the ‘a’ key while the Alt key is depressed the cursor moves to the beginning of the current sentence as a normal default along the same lines, we get two extensions: one in which pressing ‘a’ while the Alt key is depressed moves the cursor and another in which it adds ‘a’ to the text. The problem is that default logic does not provide for more specific defaults to override ones that are more general. This principle of specificity has been discussed at length in the literature. Incorporating it in a nonmonotonic logic can complicate the theory considerably; see, for instance,  Asher & Morreau 1991  and  Horty 1994.  And, as  Elkan 1995  points out, the Qualification Problem raises computational issues. Relatively little attention has been given to the Qualification Problem for characterizing actions, in comparison with other problems in temporal reasoning. In particular, the standard accounts of unsuccessful actions are somewhat unintuitive. In the formalization of  Lifschitz 1987,  for instance, actions with some unsatisfied preconditions are only distinguished from actions whose preconditions all succeed in that the conventional effects of the action will only be ensured when the preconditions are met. It is as if an action of spending $1,000,000 can be performed at any moment—although if you don’t have the money, no effects in particular will be guaranteed.   [30] And there is no distinction between actions that cannot even be attempted (like boarding a plane in London when you are in Sydney), actions that can be attempted, but in which the attempt can be expected to go wrong (like making a withdrawal when you have insufficient funds), actions that can be attempted with reasonable hope of success, and actions that can be attempted with guaranteed success. As J.L. Austin made clear in  Austin 1961,  the ways in which actions can be attempted, and in which attempted actions can fail, are a well developed part of common sense reasoning. Obviously, in contemplating a plan containing actions that may fail, one may need to reason about the consequences of failure. Formalizing the pathology of actions, providing a systematic theory of ways in which actions and the plans that contain them can go wrong, would be a useful addition to planning formalisms, and one that would illuminate important themes in philosophy. The challenge posed by the Ramification Problem (characterized first in  Finger 1987)  is to formalize the indirect consequences of actions, where “indirect” effects are not  delayed,[31]  but are temporally immediate but causally derivative. If one walks into a room, the direct effect is that one is now in the room. There are also many indirect effects: for instance, that one’s shirt also is now in the room. You can see from this that the formulation of the problem presupposes a distinction between direct consequences of actions (ones that attach directly to an action, and that are ensured by the successful performance of the action) and other consequences. This assumption is generally accepted without question in the AI literature on action formalisms. You can make a good case for its common sense plausibility—for instance, many of our words for actions (‘to warm’, to ‘lengthen’, ‘to ensure’) are derived from the effects that are conventionally associated with them. And in these cases, success is entailed: if someone has warmed something, this entails that it became warm. [32] A typical example is discussed in  Lin 1995: a certain suitcase has two locks, and is open if and only if both locks are open. Then (assuming that actions are not performed concurrently) opening one lock will open the suitcase if and only if the other lock is open. Here, opening a lock is an action, with direct consequences; opening a suitcase is not an action, it is an indirect effect. Obviously, the Ramification Problem is intimately connected with the Frame Problem. In approaches that adopt nonmonotonic solutions to the Frame Problem, inertial defaults will need to be overridden by conclusions about ramifications in order to obtain correct results. In case the left lock of the suitcase is open, for instance, and an action of opening the right lock is performed, then the default conclusion that the suitcase remains closed needs somehow to be suppressed. Some approaches to the Ramification Problem depend on the development of theories of common sense causation, and therefore are closely related to the causal approaches to reasoning about time and action discussed below in  Section 4.6.  See, for  instance,  Giunchiglia et al. 1997,  Thielscher 1989,  Lin 1995. Philosophical logicians have been content to illustrate their ideas with relatively small-scale examples. The formalization of even large-scale mathematical theories is relatively unproblematic. Logicist AI is the first branch of logic to undertake the task of formalizing large examples involving nontrivial common sense reasoning. In doing so, the field has had to invent new methods. An important part of the methodology that has emerged in formalizing action and change is the prominence that is given to challenges, posed in the form of scenarios. These scenarios represent formalization problems which usually involve relatively simple, realistic examples designed to challenge the logical theories in specific ways. Typically, there will be clear common sense intuitions about the inferences that should be drawn in these cases. The challenge is to design a logical formalism that will provide general, well-motivated solutions to these benchmark problems. Among the many scenarios that have been discussed in the literature are the Baby Scenario, the Bus Ride Scenario, the Chess Board Scenario, the Ferryboat Connection Scenario, the Furniture Assembly Scenario, the Hiding Turkey Scenario, the Kitchen Sink Scenario, the Russian Turkey Scenario, the Stanford Murder Mystery, the Stockholm Delivery Scenario, the Stolen Car Scenario, the Stuffy Room Scenario, the Ticketed Car Scenario, the Walking Turkey Scenario, and the Yale Shooting Anomaly. Accounts of these can be found in  Shanahan 1997  and  Sandewall 1994;  see especially  Sandewall 1994[Chapters 2  and 7]. Many of these scenarios are designed to test advanced problems that will not be discussed here—for instance, challenges dealing with multiple agents, or with continuous changes. Here, we concentrate on one of the earliest, and probably the most subtle of these scenarios: the Yale Shooting Anomaly, first reported in  Hanks & McDermott 1985  and published in  Hanks & McDermott 1986;  Hanks & McDermott 1987. The Yale Shooting Anomaly involves three actions: load, shoot, and wait. A propositional fluent Loaded tracks whether a certain pistol is loaded; another fluent, Alive, tracks whether a certain person, Fred, is alive. load has no preconditions; its only effect is Loaded. The fluent shoot has Loaded as its only precondition and Alive as a negative effect; wait has no preconditions and no effects. Causal information regarding the axioms is formalized as follows. There is no Wait Axiom—that is, wait has no preconditions and no effects. We will formalize the inertial reasoning in this scenario using a nonmonotonic logic—to be specific, we use Reiter’s default logic. The set D of defaults for this theory consists of all instances of the inertial schema IR. In the initial situation, Fred is alive and the pistol is unloaded. The monotonic theory W of the scenario consists of: (1) the action axioms Load, Shoot 1 and Shoot 2 and (2) the initial conditions IC1 and IC2. Let s1 = RESULT(load,s0), s2 = RESULT(wait,s1), and s3 = RESULT(shoot,s2). The Yale Shooting Anomaly arises because this theory allows an extension in which the actions are load; shoot; wait, and in the final situation s3, the pistol is unloaded and Fred is alive. The initial situation in the Anomaly and the three actions, with their resulting situations, can be pictured as follows. The natural, expected outcome of these axioms is that the pistol is loaded and Fred is alive after waiting, so that shooting yields a final outcome in which Fred is not alive and the pistol is unloaded. There is no problem in showing that this corresponds to an extension; the problem is the presence of the other, anomalous extension, which looks like this. Here is a narrative version of this extension. At first, Fred is alive and the pistol is unloaded. After loading, the pistol is loaded and Fred remains alive. After waiting, the pistol becomes unloaded and Fred remains alive. Shooting is then vacuous since the pistol is unloaded, so finally, after shooting, Fred remains alive and the pistol remains unloaded. The best way to see clearly that this is an extension is to work through the proof. Less formally, though, you can see that the expected extension violates just one default: the frame default for Alive is violated when Fred changes state in the last step. But the anomalous extension also violates only one default: the frame default for Loaded is violated when the pistol spontaneously becomes unloaded while waiting. So, if you just go by the number of defaults that are violated, both extensions are equally good. The Yale Shooting Anomaly represents a major obstacle in developing a theory of predictive reasoning. A plausible, well-motivated logical solution to the Frame Problem runs afoul of a simple, crisp example in which it clearly delivers the wrong results. Naturally, the literature concerning the Yale Shooting Problem is extensive. Surveys of some of this work, with bibliographical references, can be found in  Shanahan 1997;  Morgenstern 1996. Many formalisms have been proposed to deal with the problems surveyed in the previous section. Some are more or less neglected today. Several are still advocated and defended by leading experts; some of these are associated with research groups who are not only interested in developments of logical theory, but in applications in planning and cognitive robotics. The leading approaches provide solutions to the main problems mentioned in  Section 4.5,  and to many of the scenarios designed to test and illustrate theories of reasoning about action and change. It is commonly agreed that good solutions need to be generalizable to more complex cases than the early planning formalisms, and that in particular the solutions they offer should be deployable even when continuous time, concurrent actions, and various kinds of ignorance are allowed. Also, it is generally agreed that the formalisms should support several kinds of reasoning, and, in particular, not only prediction and plan verification but retrodiction, i.e., construction of a sequence of states and actions from partial information, presented in narrative form. We describe four approaches here: (1) Features and fluents (Sandewall), (2) Motivated Action Theory (Morgenstern and Stein), (3) State Minimization in the Event Calculus (Shanahan) and (4) Causal Theories (Lifschitz and others). The accounts of the first three in what follows will be fairly brief; fortunately, each approach is well documented in a single reference. The fourth approach is most likely to be interesting to philosophers and to contain elements that will be of lasting importance regardless of future developments in this area. This approach, described in  Sandewall 1994,  uses preference semantics as a way to organize nonmonotonic solutions to the problems of reasoning about action and change. Rather than introducing a single logical framework, Sandewall considers a number of temporal logics, including ones that use discrete, continuous, and branching time. The properties of the logics are systematically tested against a large suite of test scenarios. This theory grew out of direct consideration of the problems in temporal reasoning described above in  Section 4.5,  and especially the Yale Shooting Scenario. In  Morgenstern & Stein 1994, Morgenstern and Stein seek to find a general, intuitively motivated logical framework that solves the difficulties. They settle on the idea that unmotivated actions are to be minimized, where an action (“actions” construed generally enough to include any change) can be motivated directly, e.g. by an axiom, or indirectly, through chains of motivations. The key technical idea of the paper is a (rather complicated) definition of motivation in an interval-based temporal logic. In  Morgenstern 1996, Morgenstern presents a summary of the theory, along with reasons for rejecting its causal rivals. The most important of these reasons is that these theories, based on the Situation Calculus, do not appear to generalize to cases allowing for concurrency and ignorance. She also cites the failure of early causal theories to deal with retrodiction. In  Baker 1989,  Andrew Baker presented a solution to the version of the Yale Shooting problem in the Situation Calculus, using a circumscriptive inertial axiom. The very brief account of circumscription above in  Section 3 indicated that circumscription uses preferred models in which the extensions of certain predicates are minimized. In the course of this minimization, a set of parameters (including, of course, the predicates to be minimized) is allowed to vary; the rest are held constant. Which parameters vary and which are held constant is determined by the application. In the earliest circumscriptive solutions to the Frame Problem, the inertial rule CIR is stated using an abnormality predicate. This axiom uses a biconditional, so that it can be used for retrodiction; this is typical of the more recent formulations of common sense inertia. In circumscribing, the abnormality predicate is minimized while the Holds predicate is allowed to vary and all other parameters are fixed. This formalization succumbs to the Yale Shooting Anomaly in much the same way that default logic does. (Circumscription does not involve multiple extensions, so the problem emerges as the nonderivability of the conclusion that Fred is alive after the occurrence of the shooting.) In Baker’s reformulation of the problem, separate axioms ensure the existence of a situation corresponding to each Boolean combination of fluents, and the RESULT function is allowed to vary, while the Holds predicate is held constant. In this setting, the RESULT function needs to be specified for “counterfactual”actions—in particular, for shooting as well as for waiting in the Yale Shooting Anomaly. It is this feature that eliminates the incorrect model for that scenario; for details, see  Baker 1989  and  Shanahan 1997,  Chapter 6. This idea, which Shanahan calls “State-Based Minimization,” is developed and extended in  Shanahan 1997, in the context of a temporal logic deriving from the Event Calculus of Kowalski and Sergot; see  Kowalski & Sergot 1986.  Shanahan’s formalism has the advantage of being closely connected to implementations using logic programming. Recall that in the anomalous model of the Yale Shooting Scenario the gun becomes unloaded after the performance of the wait action, an action which has no conventional effects—the unloading, then, is uncaused. In the context of a nonmonotonic logic—and without such a logic, the Yale Shooting Anomaly would not arise—it is very natural to formalize this by treating uncaused eventualities as abnormalities to be minimized. This strategy was pursued by Hector Geffner in  Geffner 1992 1990,  where he formalizes this simple causal solution to the Yale Shooting Anomaly. But the solution is presented in the context of an ambitious general project in nonmonotonic logic that not only develops properties of the preferred model approach and shows how to apply it to a number of reasoning problems, but that relates nonmonotonic logic to probabilities, using ideas deriving from  Adams 1975.  In  Geffner 1992,  the causal theory is sketched; it is not developed to show its adequacy in dealing with the battery of problems presented above, and in particular the Ramification Problem is left untouched. The work beginning with  Lifschitz 1987  has contributed to a sustained line of research in the causal approach—not only by Lifschitz and students of his such as Enrico Giunchiglia and Hudson Turner, but by researchers at other sites. For work in this area, and further references, see  Thielscher 1989,  Gustaffson & Doherty 1996,  Baral 1995,  Nakashima et al. 1997,  Lifschitz 1997,  Giunchiglia & Lifschitz 1998,  Lin 1995,  Haugh 1987,  Lifschitz 1998,  Turner 1999,  McCain & Turner 1995,  Elkan 1991,  McCain & Turner 1997,  Thielscher 1996,  and  Gelfond & Lifschitz 1998. Here, we briefly describe some of theories developed by the Texas Action Group, leading up to the causal solution presented in  Turner 1999.  Turner returns to the ideas  of  Geffner 1992,  but places them in a simpler logical setting and applies them to the formalization of more complex scenarios that illustrate the interactions of causal inertia with other considerations, especially the Ramification Problem. Ramification is induced by the presence of static laws which relate the direct consequences of actions to other changes.  Let’s use a car-starting scenario to illustrate the difficulties. There is one action, turn-on, which turns on the ignition; let’s suppose that this action has no preconditions. There is a fluent Ig tracking whether the ignition is on, a fluent Dead tracking whether the battery is dead, and a fluent Run tracking whether the engine is running. A static law says that if the ignition is on and the battery isn’t dead, the engine is running. (Let’s suppose that every other source of failure has already been eliminated in this scenario; the only possible reason for not starting is the battery.) We want to consider a transition in which turn-on is performed in a situation in which the ignition is not on, the battery is not dead, and the car isn’t running. Of course, we want a planning agent to be able to infer in such a case that a performance of turn-on will result in a situation in which the ignition is on, the battery isn’t dead, and the engine is running. But contraposition of laws makes it difficult to devise a principled solution. Informally, this difficulty is this: we can conclude by contraposing our only static law that if the ignition is on and the engine isn’t running, then the battery is dead. This law not only is true in our scenario, but would be used to explain a failed attempt to start the car. But if we allow it to be used for prediction, then it is hard to see how to rule out an outcome of turn-on in which the ignition is on, the battery is dead, and the engine isn’t running. The battery is dead in this outcome because of causal inertia. The engine isn’t running because of the contraposed causal law. Readers who want to explore in some detail the problems of embedding a nonmonotonic solution to the Frame Problem in relatively expressive action languages  can look to  Gelfond & Lifschitz 1998. This paper presents an increasingly powerful and sophisticated series of action languages. Their language incorporates an ad hoc or at least purely syntactic solution to the Ramification Problem. A theory in language  B  consists of two sets of axioms: Gelfond and Lifschitz impose a weak closure condition on static laws: where s is a set of literals, s is restricted-closed with respect to a  B theory  T,  RBClT(s) if and only if every literal that would be added by starting with s and forward-chaining through the static laws of  B  is already in s. In other words: The closure operation induced by this condition is used in defining RESULT(a,s) for the language  B.  Of course, this solution treats logically equivalent static laws differently; for instance, a theory whose only static law is [P ∧ ¬Q] →R will in general determine a very different RESULT function than one whose only static law is [P ∧ ¬R] →Q. This has some somewhat counterintuitive effects. In the car-starting scenario, Gelfond and Lifschitz’ language  B  indeed yields the desired conclusion that the car will start when there is one dynamic law, when the only static law is when the initial state is s = {¬Ig, ¬Dead, ¬Run}, and when the action turn-on is performed. However, if we add a true static law saying that if the ignition is on and the engine isn’t running the battery is dead, we get an anomaly. With the addition of this law, there is a model in which preserving the fact that the car is not running makes the battery become dead when the ignition is turned on. If the two static laws, [Ig ∧ ¬Dead] → Run and [Ig ∧ ¬Run] → Dead, are reformulated using causal language, the former sounds correct, while the second is distinctly problematic: compare with This makes it very plausible to suppose that the source of the problem is a representation of underlying causal information in action language  B  that is somehow inadequate. Gelfond and Lifschitz go on to describe another action  language,  C, which invokes an explicit notion of causality—motivated, in all likelihood, in part by the need to provide a more principled solution to the problem. Instead of describing that language, we now discuss the similar theory of  Turner 1999. Turner’s idea is to treat Caused as a modal operator [ c ], making this the basis of a modal nonmonotonic logic. In the preferred models of this logic, the caused propositions coincide with the propositions that are true, and this must be the only possibility consistent with the extensional part of the model. To make this more explicit, recall that in the possible worlds interpretation of S5, it is possible to identify possible worlds with state descriptions, which we can represent as sets I of literals (atomic formulas and their negations). Making this identification, then, we can think of a model as a pair <I, S>, where S is a set of interpretations (complete, consistent sets of literals) including I. The modal operator [ c ] is given the standard semantics: where S is a set of interpretations and where I ∈ S,  S ⊨I [ c  ] A if and only if  S ⊨I′ A for all I′ ∈S. <I, S> satisfies a set of formulas T if and only if S ⊨I A for all A ∈ T. Turner’s preferred models of T are the pairs <I, S> such that: (1) <I, S> satisfies T, (2) S = {I}, and (3) <I, S> is the unique interpretation <I′, S′> meeting conditions (1) and (2) with I′ = I. Condition (2) guarantees the “universality of causation”; it validates A ↔ [ c ]A. Condition (3) “grounds” causality in noncausal information (in the models in which we are interested, this will be information about the occurrence of events), in the strongest sense: it is uniquely determined by this information. Although it is not evident from the formulation, Turner’s account of preferred models is related to the constructions of more general nonmonotonic logics, such as default logic. Consult  Turner 1999  for details. The axioms that specify the effects of actions treat these effects as caused; for instance, the axiom schema for loading would read as follows: Ramifications of the immediate effects of actions are also treated as caused. And the nonmonotonic inertial axiom schemata take the form [[[ c ]Holds(f,s)]  ∧ Holds(f,RESULT(a,s))] → [ c ]Holds(f,R ESULT(a,s)) and [[[ c ]¬Holds(f,s)]  ∧ ¬Holds(f,RESULT(a,s))] →          [ c ]¬Holds(f,RESULT(a,s)). Thus, a true proposition can be caused either because it is the direct or indirect effect of an action, or because it involves the persistence of a caused proposition. Initial conditions are also considered to be caused, by stipulation. To illustrate the workings of this approach, let’s consider the simplest case of inertia: we have a language with just one constant denoting a fluent, f, and one action-denoting constant, wait. As in the Yale Shooting Problem, there are no axioms for wait; this action can always be performed and has no associated effects. Let s1 be RESULT(wait,s 0). The theory T contains an initial condition for f, Holds(f,s0) and a statement that the initial condition is caused, [ c ] Holds(f,s 0), as well as the inertial schemata. Two models of T satisfy conditions (1) and (2): where I1 = {Holds(f,s0), Holds(f,s1)} and I2 = {Holds(f,s0), ¬Holds(f,s1)}. M1 is the intended model, in which nothing changes. It satisfies Condition (3), since if <I1, S> satisfies T it satisfies [ c ]Holds(f,s 1) by the inertial axiom Therefore, S = {I1}. M2 is an anomalous model, in which the fluent ceases spontaneously. This model does not satisfy Condition (3), since M3 = <I2,{I1, I2}> also satisfies T; in particular, it satisfies the inertial axiom for f because it fails to satisfy Holds(f,s1). So, while M1 is a preferred model, M2 is not. Turner’s approach avoids the problem of contraposition by giving causal relations the form When contraposed, this becomes which does not have the form of a causal law. This solution is less than fully satisfactory at solving the intuitive difficulties, because Turner’s semantics seems intuitively to validate formulas such as and in this form, contraposition would be restored. The task of clarifying the foundations of causal theories of action and change may not yet be complete. But the apparent usefulness of a “principle of universal causality” in accounting for a range of problems in qualitative common sense reasoning will be tantalizing to philosophers. And the causal theory, as initiated by Geffner and developed by Turner, has many interesting detailed features. For instance, while philosophical work on causality has concentrated on the causal relation, this work in logical AI shows that a great deal can be done by using only a nonrelational causal predicate. The relation between causality and conditionals can be explored and exploited in various ways.  Lewis 1977 undertakes to account for causality in terms of conditionals.  In the reverse direction, Lent and Thomason 2015 uses Turner’s causal approach to construct models for conditional logics, in the restricted case where the antecedent is the conjunction of an action expression and simple situational conditions.  The motivation for this idea is than an explicit solution to the frame problem automatically provides a semantics for such conditionals. Morgenstern’s two chief criticisms of the causal approach to reasoning about actions are that it does not give an adequate account of explanation[34]  and that the logical context in which it works (the Situation Calculus) is limited. As work on the approach continues, progress is being made in these areas. But the constraints that a successful logic of action and change must meet are so complex that it seems to be a reasonable research methodology to concentrate initially on a restricted logical setting. For another approach to nonmonotonic causal reasoning, based on input-output logics (Makinson & van der Torre 2000), see Bochman 2004. Although for many AI logicists, the goal of action formalisms is to illuminate an important aspect of common sense reasoning, most of their research is uninformed by an important source of insights into the common sense view of time—namely, natural language. Linguists concerned with the semantics of temporal constructions in natural language, like the AI community, have begun with ideas from philosophical logic but have discovered that these ideas need to be modified in order to deal with the phenomena. A chief discovery of the AI logicists has been the importance of actions and their relation to change. Similarly, an important discovery of the “natural language logicists” has been the importance of different kinds of events (including structured composite events) in interpreting natural language. From work such as this the idea of “natural language metaphysics” (see, for instance,  Bach 1989) has emerged. The goal of articulating a logical framework tailored to a representational system that is motivated by systematic evidence about meanings in natural languages is not acknowledged by all linguistic semanticists. Nevertheless, it is a significant theme in the linguistic literature. This goal is remarkably similar to those of the common sense logicists, but the research methodology is entirely different. Can the insights of these separate traditions be reconciled and unified? Is it possible to constrain theories of temporal representations and reasoning with the insights and research methodologies of both traditions? In  Steedman 1995  (and  2000,  listed in the Other Internet Resources Section), these important questions are addressed, and a theory is developed that extends action formalisms like the Situation Calculus, and that incorporates many of the insights from linguistic semantics. The project reported in  Steedman 2000  is still incomplete, but the results reported there make a convincing case that the event-based ideas from linguistics can be fruitfully combined with the action-centered formalisms in the AI literature. The possibility of this unification is one of the most exciting logical developments in this area, bringing together as it does two independent descendants of the earlier work in the logic of time.