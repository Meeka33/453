 It should be clear from the foregoing that there is still much work to be done regarding the interpretations of probability. Each interpretation that we have canvassed seems to capture some crucial insight into a concept of it, yet falls short of doing complete justice to this concept. Perhaps the full story about probability is something of a patchwork, with partially overlapping pieces and principles about how they ought to relate. In that sense, the above interpretations might be regarded as complementary, although to be sure each may need some further refinement. My bet, for what it is worth, is that we will retain the distinct notions of physical logical/evidential, and subjective probability, with a rich tapestry of connections between them. There are further signs of the rehabilitation of classical and logical probability, and in particular the principle of indifference and the principle of maximum entropy, by authors such as Paris and Vencovská (1997), Maher (2000, 2001), Bartha and Johns (2001), Novack (2010), White (2010), and Pettigrew (2016). Relevant here may also be advances in information theory and complexity theory. Information theory uses probabilities to define the information in a particular event, the degree of uncertainty in a random variable, and the mutual information between random variables (Shannon 1948, Shannon & Weaver 1949). This theory has been developed extensively to give accounts of complexity, optimal data compression and encoding (Kolmogorov 1965, Li and Vitanyi 1997, Cover and Thomas 2006; see the entry on  information  for more details). It is applied across the sciences, from its natural home in computer science and communication theory, to physics and biology. Interpreting information in these areas goes hand-in-hand with interpreting the underlying probabilities: each concept of probability has a corresponding concept of information. For example, Scarantino (2015) offers an account of ‘natural information’ in biology that is compatible with either a logical interpretation of probability or objective Bayesian interpretation, while Kraemer (2015) offers one that rests on a finite frequency interpretation. Information theory has also proved to be fruitful in the study of randomness (Kolmogorov 1965, Martin-Löf 1966), which obviously is intimately related to the notion of probability – see Eagle (2016), and the entry on  chance versus randomness.  Refinements of our understanding of randomness, in turn, should have a bearing on the frequency interpretations (recall von Mises’ appeal to randomness in his definition of a ‘collective’), and on propensity accounts (especially those that make explicit ties to frequencies). Given the apparent connection between propensities and causation adumbrated in Section 3.5, powerful causal modelling methods should also prove fruitful here. More generally, the theory of graphical causal models (also known as Bayesian networks) uses directed acyclic graphs to represent causal relationships in a system. (See Spirtes, Glymour and Scheines 1993, Pearl 2000, Woodward 2003.) The graphs and the probabilities of the system’s variables harmonize in accordance with the causal Markov condition, a sophisticated version of Reichenbach’s slogan “no correlation without causation”. (See the entry on  causal models  for more details.) Thus again, each understanding of probability has a counterpart understanding of causal networks. Regarding best-system interpretations of chance, I noted that it is somewhat unclear exactly what ‘simplicity’ and ‘strength’ consist in, and exactly how they are to be balanced. Perhaps insights from statistics and computer science may be helpful here: approaches to statistical model selection, and in particular the ‘curve-fitting’ problem, that attempt to characterize simplicity, and its trade-off with strength — e.g., the Akaike Information Criterion (see Forster and Sober 1994), the Bayesian Information Criterion (see Kieseppä 2001), Minimum Description Length theory (see Rissanen 1999) and Minimum Message Length theory (see Wallace and Dowe 1999). Physical probabilities are becoming even more crucial to scientific inquiry. Probabilities are not just used to characterize the support given to scientific theories by evidence; they appear essentially in the content of the theories themselves. This has led to fertile philosophical ground interpreting the probabilities in such theories. For example, quantum mechanics has physical probabilities at the fundamental level. The interpretation of these probabilities is related to the interpretation of the theory itself (see the entry on  philosophical issues in quantum theory).  Statistical mechanics and evolutionary theory have non-fundamental objective probabilities. Are they genuine chances? How can we account for them? See Strevens (2003) and Lyon (2011) for discussion. However, Schwarz (2018) argues that these probabilities can and should be left uninterpreted. Loewer (2012) proposes that the Lewisian best system of our world is given by “the Mentaculus”—a complete probability map of the universe. This is Albert’s (2000) package of: Another ongoing debate regarding physical probabilities concerns whether chance is compatible with determinism—see, e.g., Schaffer (2007), an incompatibilist, and Ismael (2009), a compatibilist; see Frigg (2016) for an overview. Relatedly, an important approach to objective probability that has gained popularity involves the so-called method of arbitrary functions. Originating with Poincaré (1896), it is a mathematical technique for determining probability functions for certain systems with chaotic dynamical laws mapping input conditions to outcomes. Roughly speaking, the probabilities for the outcomes are relatively insensitive to the probabilities over the various initial conditions — think of how the probabilities of outcomes of spins of a roulette wheel apparently do not depend on how the wheel is spun, sometimes vigorously, sometimes feebly. See Strevens (2003, 2013) for detailed treatments of this approach. The subjectivist theory of probability is also thriving—indeed, it has been the biggest growth area among all the interpretations, thanks to the burgeoning of formal epistemology in the last couple of decades. For each of the topics that I will briefly mention, I can only cite a few representative works. Since Joyce (1998), accuracy arguments for various Bayesian norms have been especially influential. They include arguments for conditionalization (Greaves and Wallace 2006, Briggs and Pettigrew forthcoming), the Reflection Principle (Easwaran 2013), and the Principal Principle (Pettigrew 2016). This line of research continues to develop. And these norms themselves have received further attention—e.g. Schoenfield (2017) on conditionalization, and Hall (1994, 2004), Ismael (2008) and Briggs (2009) on the Principal Principle. Yet for some problems, Bayesian modelling seems not to be sufficiently nuanced. A recently flourishing area has concerned modelling an agent’s self-locating credences, concerning who she is, or what time it is. The contents of such credences are usually taken to be richer than just propositions (thought of as sets of possible worlds); rather, they are finer-grained propositions (sets of centered worlds — see Lewis 1979). This in turn has ramifications for updating rules, in particular calling conditionalization into question—see Meacham (2008). The so-called Sleeping Beauty problem (Elga 2000) has generated much discussion in this regard. See Titelbaum (2012) for a comprehensive study and approach to such problems. These continue to be fertile areas of research. On the other hand, there is another sense in which Bayesian modelling has been regarded as too nuanced. It seems to be psychologically unrealistic to portray humans (rather than ideally rational agents) as having degrees of belief that are infinitely precise real numbers. Thus, there have been various attempts to ‘humanize’ Bayesianism, and this line of research is gaining momentum. For example, there has been a flourishing study of imprecise probability and imprecise decision theory, in which credences need not be precise numbers—for example, they could be sets of numbers, or intervals. See http://www.sipta.org/ for up-to-date research in this area. This resonates with recent work on whether imprecise probabilities are rationally required—Hájek and Smithson (2012) on the pro side, Schoenfield (2017) on the con side. The debate continues. Nor is it plausible that humans obey all the theorems of the probability calculus—we are incoherent in all sorts of ways. The last couple of decades have also seen research on degrees of incoherence—measuring the extent of departures from obedience to the probability calculus—including Zynda (1996), Schervish, Seidenfeld and Kadane (2003), and De Bona and Staffel (2017, 2018). Lin (2013) sees traditional epistemology’s notion of belief as appropriate for humans who fall short of the Bayesian ideal, but who nevertheless may obey various doxastic norms that can be given Bayesian endorsement. He models everyday practical reasoning, with qualitative beliefs and desires, providing a qualitative decision theory and representation theorem. Easwaran (2016) takes humans to genuinely have all-or-nothing beliefs, but offers an instrumentalist justification for representing those beliefs with probabilities. It also a fact of life that humans disagree with each other. How should an agent modify her credences (if at all) when she disagrees on some claim with an epistemic peer—someone who has the same evidence as her, and whom she regards as equally good at evaluating that evidence? The literature on this topic is huge (see Kopec and Titelbaum (2016) for a survey, and the entry on  disagreement),  and it connects in important ways with the interpretations of probability. Intuitively, we feel that disagreement with an epistemic peer rationally calls for moving one’s opinion in the direction of theirs, since disagreement with a peer seems to be evidence that one has made a mistake in evaluating one’s initial evidence. As Kelly (2010) argues, this ‘conciliationist’ intuition appears to commit us to the evidential interpretation of probability, with the common evidence bestowing a unique probability on the disputed claim. (See Titelbaum 2016 for dissent; for a recent defense of the Uniqueness Thesis more generally, see Horowitz and Dogramaci 2016; for a recent criticism, see Schoenfield 2014.) The intuition also appears to commit us to probabilistic enkrasia: the view that our credences are beholden to our attitudes about evidential probabilities, in much the same way as the Principal Principle portrays our credences as beholden to our attitudes about chances. (See Christensen 2013 and Elga 2010 for versions of probabilistic enkrasia principles.) Let’s grant that disagreement with a peer about some claim is evidence that one has made a mistake regarding it. This should affect one’s opinion in it only if one’s attitude about the correct way to evaluate the evidence constrains one’s attitude about the claim. However, probabilistic enkrasia has been criticised (see Williamson 2014; Lasonen-Aarnio 2015). We thus come back full circle to where we started. The classical and logical/evidential interpretations sought to capture an objective notion of probability that measures evidential support relations. Early proponents of the subjective interpretation gave us a highly permissive notion of rational credences, constrained only by the probability calculus. Less liberal subjectivists added further rationality constraints, with credences beholden to attitudes about physical probabilities, and to evidential probabilities—at an extreme, to the point of uniqueness. The three kinds of concepts of probability that we identified at the outset converge: epistemological, degrees of confidence, and physical. Future research will doubtless explore further the relationships between them—and how they provide guides to life. Kyburg (1970) contains a vast bibliography of the literature on probability and induction pre-1970. Also useful for references before 1967 is the bibliography for “Probability” in the Macmillan Encyclopedia of Philosophy. Earman (1992) and Howson and Urbach (1993) have more recent bibliographies, and give detailed presentations of the Bayesian program. Skyrms (2000) is an excellent introduction to the philosophy of probability. Von Plato (1994) is more technically demanding and more historically oriented, with another extensive bibliography that has references to many landmarks in the development of probability theory in the last century. Fine (1973) is still a highly sophisticated survey of and contribution to various foundational issues in probability, with an emphasis on interpretations. More recent philosophical studies of the leading interpretations include Childers (2013), Gillies (2000b), Galavotti (2005), Huber (2019), and Mellor (2005). Hájek and Hitchcock (2016a) is a collection of original survey articles on philosophical issues related to probability. Section IV includes chapters on most of the major interpretations of probability. It also includes coverage of the history of probability, Kolmogorov’s formalism and alternatives, and applications of probability in science and philosophy. Eagle (2010) is a valuable anthology of many significant papers in the philosophy of probability. Billingsley (1995) and Feller (1968) are classic, rather advanced textbooks on the mathematical theory of probability. Ross (2013) is less advanced and has lots of examples.