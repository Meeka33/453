 The distinction between “philosophy of neuroscience” and “neurophilosophy” came to be better clarified over the first decade of the twenty-first century, due primarily to more questions being pursued in both areas. Philosophy of neuroscience still tends to pose traditional questions from philosophy of science specifically about neuroscience. Such questions include: What is the nature of neuroscientific explanation? And, what is the nature of discovery in neuroscience? Answers to these questions are pursued either descriptively (how does neuroscience proceed?) or normatively (how should neuroscience proceed)? Some normative projects in philosophy of neuroscience are “deconstructive”, criticizing claims about the topic made by neuroscientists. For example, philosophers of neuroscience have criticized the conception of personhood assumed by researchers in cognitive neuroscience (cf. Roskies 2009). Other normative projects are constructive, proposing new theories of neuronal phenomena or methods for interpreting neuroscientific data. Such projects often integrate smoothly with theoretical neuroscience itself. For example, Chris Eliasmith and Charles Anderson developed an approach to constructing neurocomputational models in their book Neural Engineering (2003). In separate publications, Eliasmith has argued that the framework introduced in Neural Engineering provides both a normative account of neural representation and a framework for unifying explanation in neuroscience (e.g., Eliasmith 2009). Neurophilosophy continued to apply findings from the neurosciences to traditional, philosophical questions. Examples include: What is an emotion? (Prinz 2004) What is the nature of desire? (Schroeder 2004) How is social cognition made possible? (Goldman 2006) What is the neural basis of moral cognition? (Prinz 2007) What is the neural basis of happiness? (Flanagan 2009) Neurophilosophical answers to these questions are constrained by what neuroscience reveals about nervous systems. For example, in his book Three Faces of Desire, Timothy Schroeder (2004) argued that our commonsense conception of desire attributes to it three capacities: (1) the capacity to reinforce behavior when satisfied, (2) the capacity to motivate behavior, and (3) the capacity to determine sources of pleasure. Based on evidence from the literature on dopamine function and reinforcement learning theory, Schroeder argued that reward processing is the basis for all three capacities. Thus, reward is the essence of desire. During the first decade of the twenty-first century a trend arose in neurophilosophy to look toward neuroscience for guidance in moral philosophy. That should be evident from the themes we’ve just mentioned. Simultaneously, there was renewed interest in moralizing about neuroscience and neurological treatments (see Levy 2007; Roskies 2009). This new field, neuroethics, thus combined both interest in the relevance of neuroscience data for understanding moral cognition, and the relevance of moral philosophy for acquiring and regulating the application of knowledge from neuroscience. The regulatory branch of neuroethics initially focused explicitly on the ethics of treatment for people who suffer from neurological impairments, the ethics of attempts to enhance human cognitive performance (Schneider 2009), the ethics of applying “mind reading” technology to problems in forensic science (Farah and Wolpe 2004), and the ethics of animal experimentation in neuroscience (Farah 2008). More recently both of these fields of neuroethics has seen tremendous growth. The interested reader should consult the  neuroethics entry  in this Encyclopedia. Trends during the first decade of the twenty-first century in philosophy of neuroscience included renewed interest in the nature of mechanistic explanations. This was in keeping with a general trend in philosophy of science (e.g., Machamer, Darden, and Craver 2000). The application of this general approach to neuroscience isn’t surprising. “Mechanism” is a widely-used term among neuroscientists. In his book, Explaining the Brain (2007), Carl Craver contended that mechanistic explanations in neuroscience are causal explanations, and typically multi-level. For example, the explanation of the neuronal action potential involves the action potential itself, the cell in which it occurs, electro-chemical gradients, and the proteins through which ions flow across the membrane. Thus we have a composite entity (a cell) causally interacting with neurotransmitters at its receptors. Parts of the cell engage in various activities, e.g., the opening and closing of ligand-gated and voltage-gated ion channels, to produce a pattern of changes, the depolarizing current constituting the action potential. A mechanistic explanation of the action potential thus countenances entities at the cellular, molecular, and atomic levels, all of which are causally relevant to producing the action potential. This causal relevance can be confirmed by altering any one of these variables, e.g., the density of ion channels in the cell membrane, to generate alterations in the action potential; and by verifying the consistency of the purported invariance between the variables. For challenges to Craver’s account of mechanistic explanation in neuroscience, specifically concerning the action potential, see Weber 2008, and Bogen 2005. According to epistemic norms shared implicitly by neuroscientists, good explanations in neuroscience are good mechanistic explanations; and good mechanistic explanations are those that pick out invariant relationships between mechanisms and the phenomena they control. (For fuller treatment of invariance in causal explanations throughout science, see James Woodward 2003. Mechanists draw extensively on Woodward’s “interventionist” account of cause and causal explanations.) Craver’s account raised questions about the place of reduction in neuroscience. John Bickle (2003) suggested that the working concept of reduction in the neurosciences consists of the discovery of systematic relationships between interventions at lower levels of biological organization, as these are pursued in cellular and molecular neuroscience, and higher level behavioral effects, as they are described in psychology. Bickle called this perspective “reductionism-in-practice” to contrast it with the concepts of intertheoretic or metaphysical reduction that have been the focus of many debates in the philosophy of science and philosophy of mind. Despite Bickle’s reformulation of reduction, however, mechanists generally resist, or at least relativize, the “reductionist” label. Craver (2007) calls his view the “mosaic unity” of neuroscience. Bechtel (2009) calls his “mechanistic reduction(ism)”. Both Craver and Bechtel advocate multi-leveled “mechanisms-within-mechanisms”, with no level of mechanism epistemically privileged. This is in contrast to reduction(ism), ruthless or otherwise which privileges lower levels. Still we can ask: Is mechanism a kind of reductionism-in-practice? Or does mechanism, as a position on neuroscientific explanation, assume some type of autonomy for psychology? If it assumes autonomy, reductionists might challenge mechanists on this assumption. On the other hand, Bickle’s reductionism-in-practice clearly departs from inter-theoretic reduction, as the latter is understood in philosophy of science. As Bickle himself acknowledges, his latest reductionism was inspired heavily by mechanists’ criticisms of his earlier “new wave” account. Mechanists can challenge Bickle that his departure from the traditional accounts has also led to a departure from the interests that motivated those accounts. (See Polger 2004 for a related challenge.) As we will see in  section 8 below,  these issues surrounding mechanistic philosophy of neuroscience have grown more urgent, as mechanism has grown to dominate the field. The role of temporal representation in conscious experience and the kinds of neural architectures sufficient to represent objects in time generated interest. In the tradition of Husserl’s phenomenology, Dan Lloyd (2002, 2003) and Rick Grush (2001, 2009) have separately drawn attention to the tripartite temporal structure of phenomenal consciousness as an explanandum for neuroscience. This structure consists of a subjective present, an immediate past, and an expectation of the immediate future. For example, one’s conscious awareness of a tune is not just of a time-slice of tune-impression, but of a note that a moment ago was present, another that is now present, and an expectation of subsequent notes in the immediate future. As this experience continues, what was a moment ago temporally immediate is now retained as a moment in the immediate past; what was expected either occurred or didn’t in what has now become the experienced present; and a new expectation has formed of what will come. One’s experience is not static, even though the experience is of a single object (the tune). These earlier works found increased relevance with the rise of “predictive coding” models of whole brain function, developed by neuroscientists including Karl Friston (2009) less than a decade later, and brought to broader philosophical attention by Jakob Hohwy (2013) and Andy Clark (2016). According to Lloyd, the tripartite structure of consciousness raises a unique problem for analyzing fMRI data and designing experiments. The problem stems from the tension between the sameness in the object of experience (e.g., the same tune through its progression) and the temporal fluidity of experience itself (e.g., the transitions between heard notes). At the time Lloyd was writing, one standard means of analyzing fMRI data consisted in averaging several data sets and subtracting an estimate of baseline activation from the composites.  [17]  This is done to filter noise from the task-related hemodynamic response. But as Lloyd points out, this then-common practice ignores much of the data necessary for studying the neural correlates of consciousness. It produces static images that neglect the relationships between data points over the time course. Lloyd instead applies a multivariate approach to studying fMRI data, under the assumption that a recurrent network architecture underlies the temporal processing that gives rise to experienced time. A simple recurrent network has an input layer, an output layer, a hidden layer, and an additional layer that copies the prior activation state of either the hidden layer or the output layer. Allowing the output layer to represent a predicted outcome, the input layer can then represent a current state and the additional layer a prior state. This assignment mimics the tripartite temporal structure of experience in a network architecture. If the neuronal mechanisms underlying conscious experience are approximated by recurrent network architecture, one prediction is that current neuronal states carry information about immediate future and prior states. Applied to fMRI, the model predicts that time points in an image series will carry information about prior and subsequent time points. The results of Lloyd’s (2002) analysis of 21 subjects’ data sets, sampled from the publicly accessible National fMRI Data Center, support this prediction. Grush’s (2001, 2004) interest in temporal representation is part of his broader systematic project of addressing a semantic problem for computational neuroscience, namely: how do we demarcate study of the brain as an information processor from the study of any other complex causal process? This question leads back into the familiar territory of psychosemantics (see  section 3 above),  but now the starting point is internal to the practices of computational neuroscience. The semantic problem is thereby rendered an issue in philosophy of neuroscience, insofar as it asks: what does (or should) “computation” mean in computational neuroscience? Grush’s solution drew on concepts from modern control theory. In addition to a controller, a sensor, and a goal state, certain kinds of control systems employ a process model of the actual process being controlled. A process model can facilitate a variety of engineering functions, including overcoming delays in feedback and filtering noise. The accuracy of a process model can be assessed relative to its “plug-compatibility” with the actual process. Plug-compatibility is a measure of the degree to which a controller can causally couple to a process model to produce the same results it would produce by coupling with the actual process. Note that plug-compatibility is not an information relation. To illustrate a potential neuroscientific implementation, Grush considers a controller as some portion of the brain’s motor systems (e.g., premotor cortex). The sensors are the sense organs (e.g., stretch receptors on the muscles). A process model of the musculoskeletal system might exist in the cerebellum (see Kawato 1999). If the controller portion of the motor system sends spike trains to the cerebellum in the same way that it sends spikes to the musculoskeletal system, and if in return the cerebellum receives spike trains similar to real peripheral feedback, then the cerebellum emulates the musculoskeletal system (to the degree that the mock feedback resembles real peripheral feedback). The proposed unit over which computational operations range is the neuronal realization of a process model and its components, or in Grush’s terms an “emulator” and its “articulants”. The details of Grush’s framework are too sophisticated to present in short compass. (For example, he introduces a host of conceptual devices to discuss the representation of external objects.) But in a nutshell, he contends that understanding temporal representation begins with understanding the emulation of the timing of sensorimotor contingencies. Successful sequential behavior (e.g., spearing a fish) depends not just on keeping track of where one is in space, but where one is in a temporal order of movements and the temporal distance between the current, prior, and subsequent movements. Executing a subsequent movement can depend on keeping track of whether a prior movement was successful and whether the current movement is matching previous expectations. Grush posits emulators—process models in the central nervous system—that anticipate, retain, and update mock sensorimotor feedback by timing their output proportionally to feedback from an actual process (Grush 2005). Lloyd’s and Grush’s approaches to studying temporal representation are varied in their emphases. But they are unified in their implicit commitment to localizing cognitive functions and decomposing them into subfunctions using both top-down and bottom-up constraints. (See Bechtel and Richardson 1993 for more details on this general explanatory strategy.) As we mentioned a few paragraphs above, both anticipated in important and interesting ways more recent neuroscientific and philosophical work on predictive coding and the brain. Both developed mechanistic explanations that pay little regard to disciplinary boundaries. One of the principal lessons of Bickle’s and Craver’s work is that neuroscientific practice in general is structured in this fashion. The ontological consequences of adopting this approach continue to be debated.