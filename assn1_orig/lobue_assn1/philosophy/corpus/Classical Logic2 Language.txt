 Here we develop the basics of a formal language, or to be precise, a class of formal languages. Again, a formal language is a recursively defined set of strings on a fixed alphabet. Some aspects of the formal languages correspond to, or have counterparts in, natural languages like English. Technically, this “counterpart relation” is not part of the formal development, but we will mention it from time to time, to motivate some of the features and results. We begin with analogues of singular terms, linguistic items whose function is to denote a person or object. We call these terms. We assume a stock of individual constants. These are lower-case letters, near the beginning of the Roman alphabet, with or without numerical subscripts: We envisage a potential infinity of individual constants. In the present system each constant is a single character, and so individual constants do not have an internal syntax. Thus we have an infinite alphabet. This could be avoided by taking a constant like \(d_{22}\), for example, to consist of three characters, a lowercase “\(d\)” followed by a pair of subscript “2”s. We also assume a stock of individual variables. These are lower-case letters, near the end of the alphabet, with or without numerical subscripts: In ordinary mathematical reasoning, there are two functions terms need to fulfill. We need to be able to denote specific, but unspecified (or arbitrary) objects, and sometimes we need to express generality. In our system, we use some constants in the role of unspecified reference and variables to express generality. Both uses are recapitulated in the formal treatment below. Some logicians employ different symbols for unspecified objects (sometimes called “individual parameters”) and variables used to express generality. Constants and variables are the only terms in our formal language, so all of our terms are simple, corresponding to proper names and some uses of pronouns. We call a term closed if it contains no variables. In general, we use \(v\) to represent variables, and \(t\) to represent a closed term. Some authors also introduce function letters, which allow complex terms corresponding to: “\(7+4\)” and “the wife of Bill Clinton”, or complex terms containing variables, like “the father of \(x\)” and “\(x/y\)”. Logic books aimed at mathematicians are likely to contain function letters, probably due to the centrality of functions in mathematical discourse. Books aimed at a more general audience (or at philosophy students), may leave out function letters, since it simplifies the syntax and theory. We follow the latter route here. This is an instance of a general tradeoff between presenting a system with greater expressive resources, at the cost of making its formal treatment more complex. For each natural number \(n\), we introduce a stock of \(n\)-place predicate letters. These are upper-case letters at the beginning or middle of the alphabet. A superscript indicates the number of places, and there may or may not be a subscript. For example, are three-place predicate letters. We often omit the superscript, when no confusion will result. We also add a special two-place predicate symbol “\(=\)” for identity. Zero-place predicate letters are sometimes called “sentence letters”. They correspond to free-standing sentences whose internal structure does not matter. One-place predicate letters, called “monadic predicate letters”, correspond to linguistic items denoting properties, like “being a man”, “being red”, or “being a prime number”. Two-place predicate letters, called “binary predicate letters”, correspond to linguistic items denoting binary relations, like “is a parent of” or “is greater than”. Three-place predicate letters correspond to three-place relations, like “lies on a straight line between”. And so on. The non-logical terminology of the language consists of its individual constants and predicate letters. The symbol “\(=\)”, for identity, is not a non-logical symbol. In taking identity to be logical, we provide explicit treatment for it in the deductive system and in the model-theoretic semantics. Most authors do the same, but there is some controversy over the issue (Quine [1986, Chapter 5]). If \(K\) is a set of constants and predicate letters, then we give the fundamentals of a language \(\LKe\)   built on this set of non-logical terminology. It may be called the first-order language with identity on \(K\). A similar language that lacks the symbol for identity (or which takes identity to be non-logical) may be called  \(\mathcal{L}1K\), the first-order language without identity on \(K\). If \(V\) is an \(n\)-place predicate letter in \(K\), and \(t_1, \ldots,t_n\) are terms of \(K\), then \(Vt_1 \ldots t_n\) is an atomic formula of  \(\LKe\).   Notice that the terms \(t_1, \ldots,t_n\) need not be distinct. Examples of atomic formulas include: The last one is an analogue of a statement that a certain relation \((A)\) holds between three objects \((a, b, c)\). If \(t_1\) and \(t_2\) are terms, then \(t_1 =t_2\) is also an atomic formula of  \(\LKe\). It corresponds to an assertion that \(t_1\) is identical to \(t_2\). If an atomic formula has no variables, then it is called an atomic sentence. If it does have variables, it is called open. In the above list of examples, the first and second are open; the rest are sentences. We now introduce the final items of the lexicon: We give a recursive definition of a formula of  \(\LKe\): A formula corresponding to \(\neg \theta\) thus says that it is not the  case that \(\theta\). The symbol “\(\neg\)” is called  “negation”, and is a unary connective. The ampersand “\(\amp\)” corresponds to the English “and” (when “and” is used to connect sentences). So \((\theta \amp \psi)\) can be read “\(\theta\) and \(\psi\)”. The formula \((\theta \amp \psi)\) is called the “conjunction” of \(\theta\) and \(\psi\). The wedge “\(\vee\)” corresponds to “either … or … or both”, so \((\theta \vee \psi)\) can be read “\(\theta\) or \(\psi\)”.  The formula \((\theta \vee \psi)\) is called the “disjunction” of \(\theta\) and \(\psi\). The arrow “\(\rightarrow\)” roughly corresponds to “if … then … ”, so \((\theta \rightarrow \psi)\) can be read “if \(\theta\) then \(\psi\)” or “\(\theta\) only if \(\psi\)”. The symbols “\(\amp\)”, “\(\vee\)”, and  “\(\rightarrow\)” are called “binary connectives”,  since they serve to “connect” two formulas into  one. Some authors introduce \((\theta \leftrightarrow \psi)\) as an abbreviation  of \(((\theta \rightarrow \psi) \amp(\psi \rightarrow \theta))\). The symbol  “\(\leftrightarrow\)” is an analogue of the locution “if and  only if”. The symbol “\(\forall\)” is called a universal quantifier, and is an analogue of “for all”; so \(\forall v\theta\) can be read “for all \(v, \theta\)”. The symbol “\(\exists\)” is called an existential quantifier, and is an analogue of “there exists” or “there is”; so \(\exists v \theta\) can be read “there is a \(v\) such that \(\theta\)”. Clause (8) allows us to do inductions on the complexity of formulas. If a certain property holds of the atomic formulas and is closed under the operations presented in clauses (2)–(7), then the property holds of all formulas. Here is a simple example: Theorem 1. Every formula of  \(\LKe\)  has the same number of left and right parentheses. Moreover, each left parenthesis corresponds to a unique right parenthesis, which occurs to the right of the left parenthesis. Similarly, each right parenthesis corresponds to a unique left parenthesis, which occurs to the left of the given right parenthesis. If a parenthesis occurs between a matched pair of parentheses, then its mate also occurs within that matched pair. In other words, parentheses that occur within a matched pair are themselves matched. Proof: By clause (8), every formula is built up from the atomic formulas using clauses (2)–(7). The atomic formulas have no parentheses. Parentheses are introduced only in clauses (3)–(5), and each time they are introduced as a matched set. So at any stage in the construction of a formula, the parentheses are paired off. We next define the notion of an occurrence of a variable being free or bound in a formula. A variable that immediately follows a quantifier (as in “\(\forall x\)” and “\(\exists y\)”) is neither free nor bound. We do not even think of those as occurrences of the variable. All variables that occur in an atomic formula are free. If a variable occurs free (or bound) in \(\theta\) or in \(\psi\), then that same occurrence is free (or bound) in \(\neg \theta, (\theta \amp \psi), (\theta \vee \psi)\), and \((\theta \rightarrow \psi)\). That is, the (unary and binary) connectives do not change the status of variables that occur in them. All occurrences of the variable \(v\) in \(\theta\) are bound in \(\forall v \theta\) and \(\exists v \theta\). Any free occurrences of \(v\) in \(\theta\) are bound by the initial quantifier. All other variables that occur in \(\theta\) are free or bound in \(\forall v \theta\) and \(\exists v \theta\), as they are in \(\theta\). For example, in the formula   \((\forall\)x(Axy  \(\vee Bx) \amp Bx)\), the occurrences of “\(x\)” in   Axy and in the first \(Bx\) are bound by the quantifier. The occurrence of “\(y\)” and last occurrence of “\(x\)” are free. In \(\forall x(Ax \rightarrow \exists\)xBx), the “\(x\)” in \(Ax\) is bound by the initial universal quantifier, while the other occurrence of \(x\) is bound by the existential quantifier. The above syntax allows this “double-binding”. Although it does not create any ambiguities (see below), we will avoid such formulas, as a matter of taste and clarity. The syntax also allows so-called vacuous binding, as in \(\forall\)x\(Bc\). These, too, will be avoided in what follows. Some treatments of logic rule out vacuous binding and double binding as a matter of syntax. That simplifies some of the treatments below, and complicates others. Free variables correspond to place-holders, while bound variables are used to express generality. If a formula has no free variables, then it is called a sentence. If a formula has free variables, it is called open. Before turning to the deductive system and semantics, we mention a few features of the language, as developed so far. This helps draw the contrast between formal languages and natural languages like English. We assume at the outset that all of the categories are disjoint. For example, no connective is also a quantifier or a variable, and the non-logical terms are not also parentheses or connectives. Also, the items within each category are distinct. For example, the sign for disjunction does not do double-duty as the negation symbol, and perhaps more significantly, no two-place predicate is also a one-place predicate. One difference between natural languages like English and formal languages like  \(\LKe\) is that the latter are not supposed to have any ambiguities. The policy that the different categories of symbols do not overlap, and that no symbol does double-duty, avoids the kind of ambiguity, sometimes called “equivocation”, that occurs when a single word has two meanings: “I’ll meet you at the bank.” But there are other kinds of ambiguity. Consider the English sentence: John is married, and Mary is single, or Joe is crazy. It can mean that John is married and either Mary is single or Joe is crazy, or else it can mean that either both John is married and Mary is single, or else Joe is crazy. An ambiguity like this, due to different ways to parse the same sentence, is sometimes called an “amphiboly”. If our formal language did not have the parentheses in it, it would have amphibolies. For example, there would be a “formula” \(A \amp B \vee\) C. Is this supposed to be \(((A \amp B) \vee C)\), or is it \((A \amp(B \vee C))\)? The parentheses resolve what would be an amphiboly. Can we be sure that there are no other amphibolies in our language? That is, can we be sure that each formula of  \(\LKe\) can be put together in only one way? Our next task is to answer this question. Let us temporarily use the term “unary marker” for the negation symbol \((\neg)\) or a quantifier followed by a variable (e.g.,  \(\forall x,  \exists z)\). Lemma 2. Each formula consists of a string of zero or more unary markers followed by either an atomic formula or a formula produced using a binary connective, via one of clauses (3)–(5). Proof: We proceed by induction on the complexity of the formula or, in other words, on the number of formation rules that are applied. The Lemma clearly holds for atomic formulas. Let \(n\) be a natural number, and suppose that the Lemma holds for any formula constructed from \(n\) or fewer instances of clauses (2)–(7). Let \(\theta\) be a formula constructed from \(n+1\) instances. The Lemma holds if the last clause used to construct \(\theta\) was either (3), (4), or (5). If the last clause used to construct \(\theta\) was (2), then \(\theta\) is \(\neg \psi\).  Since \(\psi\) was constructed with \(n\) instances of the rule, the Lemma holds for \(\psi\) (by the induction hypothesis), and so it holds for \(\theta\).  Similar reasoning shows the Lemma to hold for \(\theta\) if the last clause was (6) or (7). By clause (8), this exhausts the cases, and so the Lemma holds for \(\theta\), by induction. Lemma 3. If a formula  \(\theta\) contains a left parenthesis, then it ends with a right parenthesis, which matches the leftmost left parenthesis in  \(\theta\). Proof: Here we also proceed by induction on the number of instances of (2)–(7) used to construct the formula. Clearly, the Lemma holds for atomic formulas, since they have no parentheses. Suppose, then, that the Lemma holds for formulas constructed with \(n\) or fewer instances of (2)–(7), and let  \(\theta\) be constructed with \(n+1\)  instances. If the last clause applied was (3)–(5), then the Lemma holds since  \(\theta\) itself begins with a left parenthesis  and ends with the matching right parenthesis. If the last clause applied was  (2), then  \(\theta\) is  \(\neg \psi\),  and the induction hypothesis applies to  \(\psi\).  Similarly, if the last clause applied was (6) or (7), then  \(\theta\)  consists of a quantifier, a variable, and a formula to which we can apply the  induction hypothesis. It follows that the Lemma holds for  \(\theta\). Lemma 4. Each formula contains at least one atomic formula. The proof proceeds by induction on the number of instances of (2)–(7) used to construct the formula, and we leave it as an exercise. Theorem 5. Let \(\alpha, \beta\) be nonempty sequences of characters on our alphabet, such that \(\alpha \beta\) (i.e \(\alpha\) followed by \(\beta)\) is a formula. Then \(\alpha\) is not a formula. Proof: By Theorem 1 and Lemma 3, if \(\alpha\) contains a left parenthesis, then the right parenthesis that matches the leftmost left parenthesis in \(\alpha \beta\) comes at the end of \(\alpha \beta\), and so the matching right parenthesis is in \(\beta\).  So, \(\alpha\) has more left parentheses than right parentheses. By Theorem \(1, \alpha\) is not a formula. So now suppose that \(\alpha\) does not contain any left parentheses. By Lemma \(2, \alpha \beta\) consists of a string of zero or more unary markers followed by either an atomic formula or a formula produced using a binary connective, via one of clauses (3)–(5). If the latter formula was produced via one of clauses (3)–(5), then it begins with a left parenthesis. Since \(\alpha\) does not contain any parentheses, it must be a string of unary markers. But then \(\alpha\) does not contain any atomic formulas, and so by Lemma \(4, \alpha\) is not a formula. The only case left is where \(\alpha \beta\) consists of a string of unary markers followed by an atomic formula, either in the form \(t_1 =t_2\) or \(Pt_1 \ldots t_n\).  Again, if \(\alpha\) just consisted of unary markers, it would not be a formula, and so \(\alpha\) must consist of the unary markers that start \(\alpha \beta\), followed by either \(t_1\) by itself, \(t_1 =\) by itself, or the predicate letter \(P\), and perhaps some (but not all) of the terms \(t_1, \ldots,t_n\).  In the first two cases, \(\alpha\) does not contain an atomic formula, by the policy that the categories do not overlap. Since \(P\) is an \(n\)-place predicate letter, by the policy that the predicate letters are distinct, \(P\) is not an \(m\)-place predicate letter for any \(m \ne n\). So the part of \(\alpha\) that consists of \(P\) followed by the terms is not an atomic formula. In all of these cases, then, \(\alpha\) does not contain an atomic formula. By Lemma \(4, \alpha\) is not a formula. We are finally in position to show that there is no amphiboly in our language. Theorem 6. Let \(\theta\) be any formula of \(\LKe\).  If \(\theta\) is not atomic, then there is one and only one among (2)–(7) that was the last clause applied to construct \(\theta\).  That is, \(\theta\) could not be produced by two different clauses. Moreover, no formula produced by clauses (2)–(7) is atomic. Proof: By Clause (8), either \(\theta\) is atomic or it was produced by one of clauses (2)–(7). Thus, the first symbol in \(\theta\) must be either a predicate letter, a term, a unary marker, or a left parenthesis. If the first symbol in \(\theta\) is a predicate letter or term, then \(\theta\) is atomic. In this case, \(\theta\) was not produced by any of (2)–(7), since all such formulas begin with something other than a predicate letter or term. If the first symbol in \(\theta\) is a negation sign “\(\neg\)”, then was \(\theta\) produced by clause (2), and not by any other clause (since the other clauses produce formulas that begin with either a quantifier or a left parenthesis). Similarly, if \(\theta\) begins with a universal quantifier, then it was produced by clause (6), and not by any other clause, and if \(\theta\) begins with an existential quantifier, then it was produced by clause (7), and not by any other clause. The only case left is where \(\theta\) begins with a left parenthesis. In this case, it must have been produced by one of (3)–(5), and not by any other clause. We only need to rule out the possibility that \(\theta\) was produced by more than one of (3)–(5). To take an example, suppose that \(\theta\) was produced by (3) and (4). Then \(\theta\) is \((\psi_1 \amp \psi_2)\) and \(\theta\) is also \((\psi_3 \vee \psi_4)\), where \(\psi_1, \psi_2, \psi_3\), and \(\psi_4\) are themselves formulas. That is, \((\psi_1 \amp \psi_2)\) is the very same formula as \((\psi_3 \vee \psi_4)\). By Theorem \(5, \psi_1\) cannot be a proper part of \(\psi_3\), nor can \(\psi_3\) be a proper part of \(\psi_1\). So \(\psi_1\) must be the same formula as \(\psi_3\). But then “\(\amp\)” must be the same symbol as “\(\vee\)”, and this contradicts the policy that all of the symbols are different. So \(\theta\) was not produced by both Clause (3) and Clause (4). Similar reasoning takes care of the other combinations. This result is sometimes called “unique readability”. It shows that each formula is produced from the atomic formulas via the various clauses in exactly one way. If \(\theta\) was produced by clause (2), then its main connective is the initial “\(\neg\)”. If \(\theta\) was produced by clauses (3), (4), or (5), then its main connective is the introduced “\(\amp\)”, “\(\vee\)”, or “\(\rightarrow\)”, respectively. If \(\theta\) was produced by clauses (6) or (7), then its main connective is the initial quantifier. We apologize for the tedious details. We included them to indicate the level of precision and rigor for the syntax.