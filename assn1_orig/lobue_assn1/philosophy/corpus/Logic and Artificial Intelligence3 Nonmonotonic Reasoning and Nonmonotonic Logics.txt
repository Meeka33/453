 Aristotle believed that most reasoning, including reasoning about what to do and about sublunary natural phenomena, dealt with things that hold “always or for the most part.” But Aristotelian logic deals only with patterns of inference that hold without exception. We find at the very beginning of logic a discrepancy between the scope of logical theory and common sense reasoning. Nonmonotonic logic is the first sustained attempt within logical theory to remedy this discrepancy. As such, it represents a potential for a sweeping expansion of the scope of logic, as well as a significant body of technical results. The consequence relations of classical logics are monotonic. That is, if a set Γ of formulas implies a consequence C then a larger set Γ ∪ A will also imply C. A logic is nonmonotonic if its consequence relation lacks this property. Preferred models provide a general way to induce a nonmonotonic consequence relation. Invoke a function that for each Γ produces a subset  MΓ of the models of Γ; in general, we will expect  MΓ to be a proper subset of these models. We then say that Γ implies C if C is satisfied by every model in  MΓ. As long as we do not suppose that  MΓ∪{A} ⊆  MΓ, we can easily have an implication relation between Γ and C without imposing this relation on supersets of  Γ.[14] This model theoretic behavior corresponds to expectation-guided reasoning, where the expectations allow certain cases to be neglected. Here is an important difference between common sense and mathematics. Mathematicians are trained to reject a proof by cases unless the cases exhaust all the possibilities; but typical instances of common sense reasoning neglect some alternatives. In fact, it is reasonable to routinely ignore improbable possibilities. Standing in a kitchen in California, wondering if there is time to wash the dishes before leaving for work, one might not take the possibility of an earthquake into account. There seem to be many legitimate reasons for neglecting certain cases in common sense reasoning. A qualitative judgment that the probability of a case is negligible is one reason. But, for instance, in a planning context it may be reasonable to ignore even nonnegligible probabilities, as long as there is no practical point in planning on these cases. The motivations for nonmonotonicity seem to involve a number of complex factors; probability (perhaps in some qualitative sense), normality, expectations that are reasonable in the sense that one can’t be reasonably blamed for having them, mutual acceptance, and factors having to do with limited rationality.  It may well be that no one has succeeded in disentangling and clarifying these motivating considerations. In the early stages of its emergence in logical AI, many researchers seem to have thought of nonmonotonic reasoning as a general method for reasoning about uncertainty; but by the end of the 1980s, implementations of fully quantitative probabilistic reasoning were not only possible in principle, but were clearly preferable in many sorts of applications to methods involving nonmonotonic logic. A plausible and realistic rationale for nonmonotonic logic has to fit it into a broader picture of reasoning about uncertainty that also includes probabilistic  reasoning.[15] Three influential papers on nonmonotonic logic appeared in 1980:  McDermott & Doyle 1980,  Reiter 1980,  and  McCarthy 1980.  In each case, the formalisms presented in these papers were the result of a gestation period of several years or more. To set out the historical influences accurately, it would be necessary to interview the authors, and this  has not been done. However, there seem to have been two motivating factors: strategic considerations having to do with the long-range goals of AI, and much more specific, tactical considerations arising from the analysis of the reasoning systems that were being deployed in the 1970s. Section 2.2  drew attention to McCarthy’s proposed goal of formalizing common sense reasoning. The brief discussion above in  Section 3.1  suggests that monotonicity may be an obstacle in pursuing this goal. An additional motive was found in  Minsky 1974,  which was widely read at the time. This paper presents an assortment of challenges for AI, focusing at the outset on the problem of natural language  understanding.[16]  Minsky advocates frame-based knowledge representation  techniques[17]  and (conceiving of the use of these representations as an alternative to logic), he throws out a number of loosely connected challenges for the logical approach, including the problem of building large-scale representations, of reasoning efficiently, of representing control knowledge, and of providing for the flexible revision of defeasible beliefs. In retrospect,  most AI researchers would tend to agree that these problems are general challenges to any research program in AI (including the one Minsky himself advocated at the time) and that logical techniques are an important element in addressing some, perhaps all, of the issues. (For instance, a well structured logical design can be a great help in scaling up knowledge representation.) Minsky apparently intended to provide general arguments against logical methods in AI, but  McDermott & Doyle 1980  and  McCarthy 1980  interpret  Minsky 1974  as a challenge that can be met by developing logics that lack the monotonicity property. Perhaps unintentionally, the paper seems to have provided some incentive to the nonmonotonic logicians by stressing monotonicity as a source of the alleged shortcomings of logic. In fact, the term ‘monotonicity’ apparently makes its first appearance in print in Minsky’s 1974 paper. The development of nonmonotonic logic also owes a great deal to the applied side of AI. In fact, the need for a nonmonotonic analysis of a number of AI applications was as persuasive as the strategic considerations urged by McCarthy, and in many ways more influential on the shape of the formalisms that emerged. Here, we mention three such applications that appear to have been important for some of the early nonmonotonic logicians: belief revision, closed-world reasoning, and planning. In a TMS, part of the support for a belief can consist in the absence of some other belief. This introduces nonmonotonicity. For instance, it provides for defaults; that Wednesday is the default day for scheduling a meeting means the belief that the meeting will be on Wednesday depends on the absence of special-case beliefs entailing that it will not be on Wednesday. The TMS algorithm and its refinements had a significant impact on AI applications, and this created the need for a logical analysis. (In even fairly simple cases, it can be hard in the absence of analytic tools to see what consequences a TMS should deliver.) This provided a natural and highly specific challenge for those seeking to develop a nonmonotonic logic. The TMS also provided specific intuitions: the idea that the key to nonmonotonicity has to do with inferences based on unprovability was important for the modal approaches to nonmonotonic logic and for default logic. And the TMS’s emphasis on interactions between arguments began a theme in nonmonotonic logic that remains important to this day. (See the discussion of argument-based approaches, in  Section 3.4,  below.) The study of databases belongs to computer science, not specifically to AI. But one of the research paradigms in the scientific analysis of databases uses logical models of the representations and reasoning  (see  Minker 1997  for a recent survey of the field), and this area has interacted with logical AI. The deductive database paradigm was taking shape at about the same time that many AI researchers were thinking through the problems of nonmonotonic logic, and provided several specific examples of nonmonotonic reasoning that called for analyses. Of these, perhaps the most important is the closed-world assumption, according to which—at least as far as simple facts are concerned, represented in the database as positive or negative literals—the system assumes that it knows all that there is to be known. It is the closed world assumption that justifies a negative answer to a query “Is there a direct flight from Detroit to Bologna?” when the system finds no such flight in its data. This is another case of inference from the absence of a proof; a negative is proved, in effect, by the failure of a systematic attempt to prove the positive. This idea, which was investigated in papers such as  Reiter 1978  and  Clark 1978  also provided a challenge for nonmonotonic logics, as well as specific intuitions—note that again, the idea of inference rules depending on the absence of a proof is present here. Rational planning is impossible without the ability to reason about the outcomes of a series of contemplated actions. Predictive reasoning of this sort is local; in a complex world with many features, we assume that most things will be unchanged by the performance of an action. But this locality has proved to be difficult to formalize. The problem of how to formalize this “causal inertia”[18] is known as the Frame Problem. It is very natural to suppose that inertia holds by default; variables are unchanged by the performance of an action unless there is a special reason to think that they will change. This suggests that nonmonotonic temporal formalisms should provide an appropriate foundation for reasoning about action and change. So attempts to formalize the reasoning needed in planning also created a need for nonmonotonic logics. One of the earliest attempts to formalize nonmonotonic reasoning,  Sandewall 1972, addresses the frame problem. Inertial defaults are an especially important and instructive case study; no more will be said about them here, since they are discussed in detail in  Section 4.4,  below. The three 1980 papers mentioned at the beginning of  Section 3.2  represent three approaches to nonmonotonic logic that remain important subfields to this day: circumscription (McCarthy), modal approaches (Doyle & McDermott) and default logic (Reiter). In  McCarthy 1993a,  McCarthy urges us, when considering the early history of circumscription, to take into account a group of three papers: McCarthy  1986,  1980,  and  1987.  The first paper connects the strategic ideas of  McCarthy & Hayes 1969  with the need for a nonmonotonic logic, and sketches the logical ideas of domain circumscription, which is now classified as the simplest case of circumscription. The second paper provides more thorough logical foundations, and introduces the more general and powerful predicate circumscription approach. The third paper concentrates on developing techniques for formalizing challenging common sense examples. All forms of circumscription involve restricting attention to models in which certain sets are minimized; for this reason, circumscription can be grouped with the preferred models approaches to nonmonotonicity: see  Section 3.4,  below. McCarthy’s formalism is fairly conservative; though it raises interesting logical issues in higher-order logic and complexity, it uses familiar logical frameworks. And much of the focus is on the development of formalization techniques. The other varieties of nonmonotonic logic, including default logic and the modal nonmonotonic logics, raise issues of the sort that are familiar to philosophical logicians, having to do with the design of new logics, the systematic investigation of questions concerning validity, and managing the proliferation of alternative logics. As the discussion above of truth maintenance indicated, it is very natural to think of nonmonotonic inferences as being hedged. That is, a nonmonotonic inference may require not merely the presence of a set of proved conclusions, but the absence of certain other conclusions. The general form of such a rule is: An important special case of DR is a normal default, a simple rule to the effect that C holds by default, conditionally on assumptions A1,…,An. This can be formalized by taking the condition that must be absent to simply be the negation of the conclusion. At first sight, it is somewhat perplexing how to formalize this notion of nonmonotonic inference, since it seems to require a circular definition of provability that can’t be replaced with an inductive definition, as in the nonmonotonic case. The difficulty with the early theory of  Sandewall 1972  is that it does not address this difficulty successfully.  McDermott & Doyle 1980  and  Reiter 1980  use fixpoint definitions to solve the problem. In both cases, the logical task is (1) to develop a formalism in which rules like DR can be expressed, and (2) to define the relation between a theory DT (which may incorporate such rules) and the theories E which could count as reasonable consequences of DT. In the terminology that later became standard, we need to define the relation between a theory DT and its extensions. In retrospect, we can identify two sorts of approaches to nonmonotonic logic: those based on preference and those based on conflict. Theories of the first sort (like circumscription) involve a relatively straightforward modification of the ordinary model-theoretic definition of logical consequence that takes into account a preference relation over models. Theories of the second sort (like default logic) involve a more radical rethinking of logical ideas. The possibility of multiple extensions—different possible coherent, inferentially complete conclusion sets that can be drawn from a single set of premises—means that we have to think of logical consequence not as a function taking a set of axioms into its logical closure, but as a relation between a set of axioms and alternative logical closures. Since logical consequence is so fundamental, this represents a major theoretical departure. With multiple extensions, we can still retrieve a consequence relation between a theory and a formula in various ways, the simplest being to say that DT nonmonotonically implies C if C is a member of every extension of DT. Still, the conflict-based account of consequence provides a much richer underlying structure than the preferential one. Reiter approaches the formalization problem conservatively. Nonmonotonicity is not expressed in the language of default logic, which is the same as the language of first-order logic. But a theory may involve a set of default rules—rules of the form DR.  Reiter 1980 provides a fixpoint definition of the extensions of such a theory, and develops the theoretical groundwork for the approach, proving a number of the basic theorems. Of these theorems, we mention one in particular, which will be used in  Section 4.5,  in connection with the Yale Shooting Anomaly. The idea is to take a conjectured extension (which will be a set T*) and to use this set for consistency checks in a proof-like process that successively applies default rules in <W,D> to stages that begin with W. We define a default proof process T0,T1,… for W, D, relative to T*, as follows. that is nonvacuously applicable to Ti relative to T*, and let In other words, as long as we can nonvacuously close the stage we are working on under an applicable default, we do so; otherwise, we do nothing. A theorem of Reiter’s says that, under these circumstances: Thus, we can show that T is an extension by (1) using T for consistency checks in a default reasoning process from <W, D>, (2) taking the limit T′ of this process, and (3) verifying that in fact T′ = T. The modal approach represents a “higher level of nonmonotonic involvement” than default logic. The unprovability construct is represented explicitly in the language, by means of a modal operator L informally interpreted as ‘provable’ (or, as  in  McDermott & Doyle 1980,  by the dual of this operator).[19]  Although McDermott and Doyle’s terminology is different from Reiter’s, the logical ideas are very similar—the essence of their approach, like Reiter’s, is a fixpoint definition of the extensions of a nonmonotonic logic. Incorporating nonmonoticity in the object language creates some additional complexities, which in the early modal approach show up mainly in proliferation of the logics and difficulties in evaluating the merits of the alternatives. As better foundations for the modal approach emerged, it became possible to prove the expected theorems concerning equivalence of modal formalisms with default  logic.[20] Reiter’s paper  Reiter 1980  appears to have developed primarily out of tactical considerations. The earlier paper  Reiter 1978  is largely concerned with providing an account of database queries. Unlike the other seminal papers in nonmonotonic logic, Reiter’s shows specific influence from the earlier and independent work on nonmonotonicity in logic programming—the work seems to have been largely inspired by the need to provide logical foundations for the nonmonotonic reasoning found in deductive databases. Doyle and McDermott’s paper shows both strategic and tactical motivation—citing the earlier literature in logicist AI, it motivates nonmonotonic logic as part of a program of modeling common sense rationality. But the theory is also clearly influenced by the need to provide a formal account of truth maintenance. Nonmonotonic logic is a complex, robust research field. Providing a survey of the subject is made difficult by the fact that there are many different foundational paradigms for formalizing nonmonotonic reasoning, and the relations between these paradigms is not simple. An adequate account of even a significant part of the field requires a something like a book-length treatment. A number of books are available, including  Łukaszewicz 1990,  Brewka 1991,  Besnard 1992,  Marek & Truszczynski 1994,  Antoniou 1997,  Brewka et al. 1997,  Schlechta 1997,  Antonelli 2005,  Makinson 2005b,  and  Horty 2012.  Two collections are especially useful:  Ginsberg 1987  and  Gabbay et al. 1994.  The former is a useful source for readers interested in the early history of the subject, and has an excellent introduction. The handbook chapters in  Gabbay et al. 1994  provide overviews of important topics and approaches. My current recommendation for readers interested in a quick, readable introduction to the topic would be  Brewka et al. 1997  and self-selected chapters of  Gabbay et al. 1994. Some other sources include  Bochman 2004, Makinson 2005, Antoniou and Wang 2007, Bochman 2007,  and Schlechta 2007. We rely on these references for technical background, and will concentrate on intellectual motivation, basic ideas, and potential long-term signifgicance for logic. At the outset in  Section 3.1,  it was mentioned how preferred models could be used to characterize a nonmonotonic consequence relation. This general model theory of nonmonotonicity emerged in  Shoham 1988,  five years after the work discussed in  Section 3.2,  and represents a much more general and abstract approach. Preferential semantics relies on a function S taking a set K of models into a subset S(K) of K. The crucial definition of preferential entailment stipulates that A is a (nonmonotonic) consequence of Γ if every model M of S(Models(Γ)) implies A. Shoham’s theory is based on a partial order ≼ over models: S(K) can then be characterized as the set of models in K that are ≼-minimal in K. To ensure that no set can preferentially entail a contradiction unless it classically entails a contradiction, infinite descending ≼ chains need to be disallowed. This treatment of nonmonotonicity is similar to the earlier modal semantic theories of conditionals—the similarities are particularly evident using the more general theories of conditional semantics, such as the one presented in  Chellas 1975.  Of course, the consequence relation of the classical conditional logics is monotonic, and conditional semantics uses possible worlds, not models. But the left-nonmonotonicity of conditionals (the fact that A  C does not imply [A  ∧  B]   C) creates issues that parallel those in nonmonotonic logics. Early work in nonmonotonic logic does not seem to be aware of the analogy with conditional logic. But the interrelations between the two have become an important theme more recently; see, for instance,  Gärdenfors & Makinson 1994,  Boutilier 1992,  Pearl 1994,  Gabbay 1995,  Benferat et al. 1997,  Delgrande 1998,  Arlo-Costa & Shapiro 1992,  Alcourrón 1995,  Asher 1995,  and  Thomason 2007. Preference semantics raises an opportunity for formulating and proving representation theorems relating conditions over preference relations to properties of the abstract consequence relation. This line of investigation began with  Lehmann & Magidor 1992. Neither Doyle or McDermott pursued the modal approach much beyond the initial stages of  McDermott 1982, and  McDermott & Doyle 1980.  With a helpful suggestion from Robert Stalnaker (see  Stalnaker 1993),  however, Robert C. Moore produced a modal theory that improves in many ways on the earlier ideas. Moore gives the modal operator of his system an epistemic interpretation, based on the conception of a default rule as one that licenses a conclusion for a reasoning agent unless something that the agent knows blocks the conclusion. In Moore’s autoepistemic logic, an extension E of a theory T is a superset of T that is stable, i.e., that is deductively closed, and that satisfies the following two rules: It is also usual to impose a groundedness condition on autoepistemic extensions of T, ensuring that every member of an extension has some reason tracing back to T. Various such conditions have been considered; the simplest one restricts extensions to those satisfying Autoepistemic logic remains a popular approach to nonmonotonic logic, in part because of its usefulness in providing theoretical foundations for logic programming. For more recent references, see  Marek & Truszczynski 1991,  Moore 1995b,  Marek & Truszczynski 1989,  Konolige 1994,  Antoniou 1997,  Moore 1993,  and  Deneker et al. 2003. Epistemic logic has inspired other approaches to nonmonotonic logic. Like other modal theories of nonmonotonicity, these use modality to reflect consistency in the object language, and so allow default rules along the lines of DR to be expressed. But instead of consistency, these use ignorance. See  Halpern & Moses 1985  and  Levesque 1987  for variations on this idea. These theories are explained, and compared to other nonmonotonic logics, in  Meyer & van der Hoek 1995.  In more recent work, Levesque’s ideas are systematically presented and applied to the theory of knowledge bases in  Levesque & Lakemeyer 2000. This brief historical introduction to nonmonotonic logic leaves untouched a number of general topics that might well be of interest to a nonspecialist. These include graph-based and proof-theoretic approaches to nonmonotonic logic, results that interrelate the various formalisms, complexity results, tractable special cases of nonmonotonic reasoning, relations between nonmonotonic and abductive reasoning, relations to probability logics, the logical intuitions and apparent patterns of validity underlying nonmonotonic logics, and the techniques used to formalize domains using nonmonotonic logics. For these and other topics, the reader is referred to the literature. As a start, the chapters in  Gabbay et al. 1994  are highly recommended.