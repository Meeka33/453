 We have seen that there are various sorts of thing a narrow content could be: a description, a conceptual role, a function from contexts to broad contents, a diagonal proposition, or a set of maximal epistemic possibilities. It is a further question which items of the relevant sort (which diagonal propositions or epistemic possibilities, for example) constitute the narrow content of a particular state of a particular subject. How can we find out what the narrow content of a mental state is? Even more centrally, what is it about a mental state that makes it appropriate to describe it as having a particular narrow content? In the remainder of this section, I consider several strategies for determining narrow content. I do not address the issue of whether these strategies should be regarded as giving the essential nature of narrow content, or merely as heuristic devices for approximating it in practice. Arguably, it is these differences over the appropriate strategy for determining narrow contents that are the most important differences between rival views of narrow content. Although we have considered several different views about the sort of semantic entities narrow contents might be, all these views, with the exception of conceptual role semantics, are close cousins of the view that narrow contents are sets of centered worlds. The most substantive differences between rival views concern how to determine which centered worlds are included in the narrow content of a particular state of a particular subject. A first strategy fits neatly with the view of narrow content as a diagonal proposition. If we want to know the narrow content of a particular mental state, we simply construct the diagonal proposition. That is, we first envision a variety of situations or environments in which the mental state could be embedded, i.e. a set of contexts or centered worlds that contain, at the center, the very mental state whose content we are interested in. For each of these contexts, we use our knowledge of broad content and how it is determined to discover the broad content that the mental state would have in that context. And then we determine whether, in the world of that context, a belief with that broad content would be true. There are three main problems with this strategy. First, it treats broad content as fundamental, and narrow content as derivative. However, for many advocates of narrow content (e.g. Chalmers 2002), narrow content is at least as fundamental as broad content. In fact, it is tempting to regard broad content as determined by narrow content in conjunction with facts about context. But the strategy we are considering can only be applied to determine narrow content if we already have an independent way of determining broad content. A second problem for the diagonalization strategy is a problem of scope (Chalmers, 2002). Although the diagonalization strategy yields a truth-conditional notion of content, the only centered worlds at which the diagonal proposition is evaluated will be worlds that contain at their center the mental state we are interested in. In effect this means that every mental state represents itself as existing. But it is puzzling why I could not have mental states whose content has nothing to do with their own existence. Chalmers offers these examples (Chalmers 2002, p. 625): it seems that my thought that I am a philosopher should be true in worlds centered on a philosopher even if he is not currently thinking that he is a philosopher. Again, it seems that the thought that someone is thinking should be false, not undefined, at centered worlds that do not contain a thinking person. Third, there is the problem of what to “hold constant” in determining which possible contexts to consider (Block and Stalnaker 1999). The strategy requires us to consider contexts that include the mental state whose content we are interested in. But exactly what counts as a context that includes a particular mental state? And how closely, and in which respects, must the version of the state in the other worlds resemble the version in the actual world? Block and Stalnaker argue in some detail that the likely candidates for what to hold constant all give the wrong results. Consider how we might find the diagonal proposition associated with Oscar's belief that water is wet. Suppose that a belief is, or is associated with, a mental analog of a sentence. We will suppose that, like a sentence, a mental token can be identified separately from its meaning. Then Oscar's mental token, like the sentence “Water is wet,” could, in some possible mental language, mean that dogs have fur. Now if in diagonalizing we consider all possible worlds centered on someone who possesses the same mental sentence as Oscar's water-sentence, regardless of its meaning, we get a diagonal proposition that is much too unconstrained to serve as a narrow content. We surely do not want to say that the narrow content of Oscar's belief that water is wet has the value True in a world that contains no remotely watery substance, but in which dogs are furry and the mental token in question means that dogs are furry. So it is not sufficient to hold a syntactically identified mental token constant in deciding which worlds to include in the diagonal proposition. We must somehow consider worlds in which the token carries the same meaning it carries in the actual world. However, if we consider only worlds in which the token has the broad meaning that water is wet, the diagonal proposition will be too constrained to play the role of narrow content: it will be false, not true, in a world centered on Twin Oscar. Still another possibility is to hold constant, not the broad content of the mental token, but its narrow content. This will give the results we want, but at the cost of making the account completely circular; diagonalizing cannot be a useful strategy for discovering narrow contents if we must already know the narrow content of a mental token in order to apply the strategy. The subtraction strategy (Brown 1992) is an attempt to identify the narrow contents of a subject's beliefs by considering all of the contents of the subject's belief and subtracting those that are not narrow. The contents that remain must be narrow. More precisely, if a content of my belief is something I believe, then a narrow content of my belief is something that I believe and that is believed by every possible duplicate of me (possibly within some restricted class of worlds). I say “ordinary content” instead of “broad content” here, since the subtraction strategy presupposes that not all ordinary contents are broad. To see why this strategy might be appealing, we can consider an analogy with perception. (A similar analogy with action is also possible.) Consider the perceptual state of someone looking at an apple. We can characterize this state in terms of what the person sees, just as we can characterize Oscar's belief state in terms of what he believes. In this case, our subject sees an apple, so one way to characterize her perceptual state is as “seeing an apple,” just as one way to describe Oscar's belief state is as “believing that water is wet.” However, we can easily construct scenarios in which our perceiver is in exactly the same narrow state, but does not see an apple — perhaps because everything except the apple's facing surface has been cut away. We can easily find a content of the subject's perception that is a content of the very same perceptual state but which characterizes it more narrowly: the subject sees the facing surface of the apple, and it is by virtue of seeing the facing surface that she sees the apple as a whole. Characterizing the perceptual state as “seeing the facing surface of an apple” is a narrower characterization in a very simple sense: if we consider alternative situations in which the subject is intrinsically exactly the same, but her environment is different, we will observe that in every situation in which she sees an apple, she also sees its facing surface, but that there are additional situations in which she sees the facing surface but does not thereby see an entire apple. Similarly, if we consider Oscar's belief that water is wet, we notice that a Twin Earth-like scenario provides a situation in which he is in the very same cognitive state but does not believe that water is wet. So we look for other contents of his belief, other things he believes, that characterize his state more narrowly. In this case we notice that in every situation in which he is in exactly the same intrinsic state and believes that water is wet, he also believes that the colorless, odorless liquid called ‘water’ is wet, but not conversely. So this latter belief seems to characterize his intrinsic state more narrowly than does the content “water is wet.” It is important to notice that neither the perceptual content of seeing the facing surface of an apple, nor the belief content of believing that the colorless, odorless liquid called ‘water’ is wet, is completely narrow. We can find still more remote possibilities in which the subject's perceptual or belief state is exactly the same, but he or she does not have this perceptual or belief content. (The “apple” could be a wax imitation or a holographic projection; Oscar's Twin could live in an environment in which the word ‘liquid’ refers to very finely granulated solids.) To find truly narrow contents we will need to press our subtraction strategy still further, and appeal to objects that are very different from the ordinary objects of perception or belief — perhaps colored shapes or sense-data in the case of perception; perhaps beliefs about the subject's perceptual inputs and behavioral outputs in the case of belief (cf. McDermott 1986). This suggests an important point which is rarely mentioned (but see Recanati 1994 for a related observation). Narrowness need not be construed as an all-or-nothing property. We can understand it instead as a matter of degree: one content of a mental state is narrower than another the further away from the actual world we need to go in logical space in order to find a world in which the subject's intrinsic properties are the same but the state does not have that content. (Alternatively, we could relativize the notion of narrow content to a set of possibilities; the more possibilities the set includes, the fewer contents will count as narrow. For many purposes we never consider Twin-Earth possibilities; for such purposes the proposition that water is wet may count as a narrow content of a subject's belief.) On this way of thinking of things, narrowness as it is usually defined is a limiting case: narrowness relative to the set of all metaphysically possible worlds. The concept of narrowness may be useful even if the limiting case never occurs, just as the concept of flatness is useful even though in this world the limiting case of absolute flatness never occurs. Possible problems for the subtraction strategy include the following. (1) The strategy presupposes that all the narrow contents of our beliefs are included in the ordinary contents of belief, so that once we have subtracted the non-narrow contents away the narrow contents will remain. On many conceptions of narrow content, however, narrow content is a more specialized and technical notion than this, and we cannot suppose that the ordinary contents of belief will include narrow contents. (2) The conception of narrow content with which the subtraction strategy fits most naturally is the descriptive content conception discussed in section 2.1. It inherits the principal objection to that view, namely that it is not clear that ordinary language can offer a narrow vocabulary sufficient to describe the narrow contents of our thoughts. Two points should be made in response to this worry. First, while the subtraction strategy assumes that the narrow contents of belief are a subset of the ordinary contents of belief, it need not be committed to the view that all of these ordinary contents are describable in natural language. Second, as noted above, we can think of completely narrow content as a limiting ideal case. The subtraction strategy offers a way of relating broad beliefs to the narrower beliefs on which they depend. This may be useful even if the process does not terminate in beliefs which are absolutely narrow. (3) Although the subtraction strategy offers a way of determining one's total narrow content, it is not clear whether or how it could be applied to more specific belief states. (4) The subtraction strategy also shares with the diagonalization strategy the problem that it gives us a method of identifying narrow contents only if we already have an independent method of identifying contents in general. This strategy is proposed by Dennett (1982). The idea is that a (centered) world is included in one's narrow content if and only if it is a world to which one is ideally suited. Place a subject in some environments, and everything will work out extremely well: the subject's attempts to satisfy his or her desires will succeed every time. Other environments will be much less friendly; somehow the subject's actions will never turn out to have quite the desired effects. Dennett's thought is roughly that we can capture the way the world is from the subject's point of view by taking the set of centered worlds to which the subject is ideally adapted. One attraction of this strategy is that it does not make narrow content parasitic on broad content; another is that it does not require the subject to be able to answer questions or reflect on the content of the subject's thoughts, so that it could easily be applied to cats and dogs as well as to humans. A possible problem for the ideal environment strategy is that, while it may give us a way to determine a subject's total view of the world, it does not provide a way of parceling out narrow contents to more specific states. A second problem is that the strategy does not seem to properly discriminate cognitive content from other sorts of information a subject's body may carry. A baby is better adapted to worlds in which extreme heat can damage its body than to worlds in which it cannot. When the baby touches something hot it automatically jerks away. This action has a useful purpose in a world in which heat is damaging, but would be pointless in a world in which it was not. But it does not follow that the baby believes that extreme heat is damaging. (See Stalnaker 1989, White 1991.) A third problem is that, in some cases in which an individual's states do seem contentful, the ideal environment strategy, as stated above, seems to yield the wrong content. In the most obvious sense, I am better suited to worlds that do not contain a homicidal maniac who wants to kill me than I am to worlds that do contain such a maniac, even if I believe that such a maniac exists. So it seems that the ideal environment strategy will not correctly include the content of this belief among those it attributes to me. (Related examples are offered by Stalnaker 1989, White 1991, and Chalmers 2002.) As Stalnaker notes (1999: 182–183), Dennett is better understood to mean, not that the worlds I am best suited to are those in which I would do best, but rather that they are those with which I am best prepared to cope. But refining this account is a challenging task. (For instance, martial arts training might prepare me to cope with dangers that I do not believe to exist, raising the worry that the ideal environment strategy on this interpretation will attribute to me beliefs I do not in fact have.) The epistemic strategy is recommended by Chalmers (2002, 2003, 2006). The framework that gives rise to this strategy was presented in section 3.5. Narrow contents are to be thought of as effecting a partition of scenarios, which are similar to the centered worlds employed by the diagonalization strategy, into those endorsed by the thought and those excluded by it. But how exactly are we to determine which scenarios are which? On the diagonalization strategy, we make use of our preexisting grasp of ordinary content to determine what ordinary content the thought would express if it were located at the center of a particular centered world, and then determine whether that ordinary content is true at that centered world. The epistemic strategy is radically different, and treats narrow content as at least as fundamental as ordinary content. So, what function from scenarios to truth values constitutes the narrow content of my thought that the lakes contain water? Put slightly differently, which scenarios does this narrow content include and which does it exclude? To find out whether the narrow content of the thought that the lakes contain water includes a given scenario, I consider the hypothesis that the scenario is actual. For example, if I consider the hypothesis that a scenario in which the oceans and lakes around me contain H2O is actual, then I will be led by a priori reasoning to the conclusion that the lakes contain water; hence, the narrow content of my thought that the lakes contain water includes this Twin Earthly scenario. Similarly, if I consider the hypothesis that a scenario in which the oceans and lakes contain XYZ is actual, I will still conclude that the lakes contain water, since in Twin Earth scenarios my water-thoughts are about XYZ. So the Twin Earthly scenario is also included in the narrow content of my thought that the lakes contain water. By contrast, the narrow content of my thought that water is H2O will separate these two scenarios. If I consider the hypothesis that an Earthly scenario is actual, I will conclude that water is H2O, so the narrow content of the thought that water is H2O includes Earthly scenarios. However, if I consider the hypothesis that a Twin Earthly scenario is actual, I will conclude that water is not H2O (rather, it is XYZ), so the narrow content of my thought that water is H2O excludes Twin Earthly scenarios. It is crucial that when I consider the hypothesis that the Twin-Earthly (or any other) scenario is actual, and ask whether, in that case, my thought that lakes contain water is true, I am not asking whether, had a Twin-Earthly world obtained, lakes would have contained water. The answer to that question is “no,” but it is a different question. When I ask this latter question, I am considering the Twin-Earthly world as counterfactual. Presupposing that the world is not actually that way, I ask what would be true if it were that way. Such questions, in which we consider alternative worlds as counterfactual, are the appropriate way to determine issues of metaphysical possibility. The sort of question relevant to epistemic possibility is different. It involves considering scenarios as actual, not as counterfactual: seeing what is the case if the world is that way, not seeing what would be the case if the world were that way. Questions about epistemic possibility, in which we consider scenarios as actual, are naturally posed in indicative conditionals: if the substance in the lakes is XYZ, is it water? A full account must say much more than this about precisely what it is to consider a scenario as actual, and what it is for a scenario to be endorsed by a particular belief. In order to consider a scenario, we must have a complete description of some sort. On the other hand, there must be restrictions on thevocabulary in which the description is expressed. In particular, it cannot contain expressions like ‘water’, for which Twin Earth examples can be constructed. Chalmers offers a detailed account that addresses such questions. (This is presented briefly in Chalmers 2002 and Chalmers 2003, and in much more detail in Chalmers 2006.) Here is the short version (Chalmers 2002, p. 611): Unlike the diagonalization strategy, the epistemic strategy does not depend on a prior determination of the broad content of the expression or state. Moreover, it does not require that narrow contents be evaluated only in scenarios that contain a token of the mental state at their center. Potential problems for the epistemic strategy include: (1) whether it can be applied to nonhuman animals, many of whom presumably also have contentful mental states; (2) whether a canonical language that satisfies the necessary constraints is possible (see e.g. Schroeter 2004; Soames 2005, pp. 216–218; Sawyer 2007); and (3) whether a version of the “what is held constant”  problem for the diagonalization strategy also poses problems for the epistemic strategy. (This last point is discussed a bit further in section 6.2.)