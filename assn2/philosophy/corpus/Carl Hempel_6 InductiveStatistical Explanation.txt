 In his studies of inductive reasoning, Hempel (1960, 1962a, 1966b) discusses the ambiguity of induction, which arises because incompatible conclusions can appear to be equally-well supported by inductive arguments, where all the premises of both arguments are true. This problem has no analogue for deductive arguments: inconsistent conclusions cannot be validly derived from consistent premises, even if they are false. Inconsistent conclusions can receive inductive support from consistent premises, however, even when they are all true. Consider Hempel’s illustration of conflicting predictions for a patient’s recovery: This information, taken by itself, would surely lend strong support to the hypothesis, But suppose that we also have the information: This information by itself would lend strong support to the contradictory of (h1): Thus, Hempel observed, (e1) and (e2) are logically compatible and could all be part of the evidence available when Jones’ prognosis is being considered. The solution Hempel endorsed was the requirement of total evidence, according to which, in reasoning about the world, arguments must be based upon all the available evidence, though he noted that evidence can be omitted when it is irrelevant and its omission does not affect the level of support. Here, when (e1) is combined with (e2), \((\neg\)h1) is better supported than (h1). Hempel referred to statistical explanations as “inductive-statistical” in contrast with his prior discussion of “deductive-nomological” explanations (Hempel 1958). Because generalizations of both kinds are supposed to be lawlike, however, more appropriate names for them might have been “universal-deductive” and “statistical-inductive” (or, perhaps, “statistical-probabilistic”, respectively (Fetzer 1974). In formalizing these arguments, Hempel symbolized statistical premises attributing a certain probability to outcome \(G\), under conditions \(F\), by “\(P(G, F) = r\)”, where these explanations then assume this form: Figure 7. An I-S Explanation Schema While Hempel initially interpreted the bracketed variable [\(r\)] as the measure of inductive support which the explanans confers upon the explanandum, \(Gi\), (in conformity with the requirement of total evidence)—where the value of [\(r\)] equals that of \(r\)—it occurred to him that, since inductive and deductive explanations are typically offered on the basis of the knowledge that their explanandum events have already occurred, viewing statistical explanations as inductive arguments whose purpose is to establish how we know that they have occurred (or that they will occur) is not the correct perspective for understanding them: the point of an explanation is not to provide evidence for the occurrence of the explanandum, but to exhibit it as nomically expectable. And the probability attached to an I-S explanation is the probability of the conclusion relative to the explanatory premises, not relative to the total class \(\mathbf{K}\) [representing our total knowledge at that time]. (Hempel 1968, original emphasis) In order to formalize this conception, he advanced the concept of a maximally specific predicate related to “Gi” in \(\mathbf{K}\), where, when “\(P(G, F) = r\)”, then, if the premises include a predicate stronger than “\(F\)”, say, “\(M\)”, which implies the presence of more properties than does “\(F\)”, then “\(P(G, M) = r\)”, where, as before, \(r = P(G, F)\). Thus additional predicates could be included in the explanans of an adequate scientific explanation, provided that they made no difference to the nomic expectability of the explanandum. Including the position of the moon or the day of the week a match is struck, for example, might be irrelevant, but his condition did not exclude them. Wesley C. Salmon (1970), however, would pursue this issue by arguing that Hempel had embraced the wrong standard upon which to base explanatory relevance relations, where explanations, in Salmon’s view, strictly speaking, do not even qualify as arguments. Salmon offered a series of counterexamples to Hempel’s approach. In the case of I-S explanations, Hempel required that the nomic expectability of the explanandum must be equal to or greater than .5, which preserved the symmetry thesis for explanations of this kind. This implied that events with low probability could not be explained. There were persuasive illustrations, moreover, demonstrating that explanations could satisfy Hempel’s criteria, yet not explain why their explanandum events occur. For example, Most colds clear up within a week, with or without vitamin C, and similarly for neurotic symptoms. Salmon thought that this problem was peculiar to statistical explanations, but was corrected by Henry Kyburg (1965), who offered examples of the following kind: For Hempel, a property \(F\) is explanatorily relevant to the occurrence of an outcome \(G\) if there is a lawful relationship that relates \(F\) to \(G\). If table salt dissolves in water, so does Morton’s table salt, Morton’s finest table salt, Morton’s finest table salt bought on sale, and so on. Salmon concluded that Hempel’s conception of I-S explanation was wrong. A student of Reichenbach (1938, 1949), Salmon had adopted and defended the limiting frequency interpretation of physical probability, where “\(P(G/F) = r\)” means that \(G\) occurs with a limiting frequency \(r\) in a reference class of instances of \(F\), which has to be infinite for limits to exist (Salmon 1967, 1971). On this approach, a property \(H\) is explanatorily relevant to the occurrence of an attribute \(G\) within a reference class \(F\) just in case (SR): that is, the limiting frequency for \(G\) in \(F\)-and-\(H\) differs from the limiting frequency for \(G\) in \(F\)-and-not-\(H\). Properties whose presence does not make a difference to the limiting frequency for the occurrence of \(G\) in \(F\) would therefore qualify as statistically irrelevant. Relative frequencies were typically relied upon in practice, but in principle they had to be limiting. Salmon also rejected Hempel’s requirement that the nomic expectability of a statistical explanation must be equal to or greater than .5, which led him to abandon the notion of explanations as arguments. Even events of low probability were explainable within the context of Salmon’s approach, which he compared with Hempel’s approach as follows: I-S model (Hempel): an explanation is an argument that renders the explanandum highly probable; S-R model (Salmon): an explanation is an assembly of facts statistically relevant to the explanandum regardless of the degree of probability that results. (Salmon 1971, original emphasis) Salmon’s work created a sensation, since Hempel’s dominance of the philosophy of science, especially in relation to the theory of explanation, now had a significant rival. The students of explanation who recognized that probabilities as properties of infinite classes posed substantial problems were few, and those who realized that Salmon’s position confronted difficulties of its own fewer still. Salmon was ingenious in devising “screening off” criteria to insure that the properties cited in an S-R explanation were restricted to those that were statistically relevant. He may have appreciated that the available evidence was restricted to relative frequencies in finite sequences, but that could be discounted as typical of the distinction between a context \(\mathbf{K}\) representing our total knowledge at that time and the truth condition, since scientific knowledge is always tentative and fallible. A deeper problem arose from the existence of properties that were statistically relevant but not explanatorily relevant, however. If women whose middle initials began with vowels, for example, experienced miscarriages with a different frequency than women whose middle initials began with consonants—even after other properties were taken into account—then that property would have to qualify as statistically relevant and therefore explanatorily relevant (Fetzer 1974, 1981). This result implied that statistical relevance cannot adequately define explanatory relevance, and Salmon would gradually move toward the propensity approach in Salmon (1980, 1989). The advantage of propensities over frequencies are considerable, since, on the propensity account, a probabilistic law no longer simply affirms that a certain percentage of the reference class belongs to the attribute class. What it asserts instead is that every member of the reference class possesses a certain disposition, which in the case of statistical laws is of probabilistic strength and in the case of universal laws of universal strength. On its single-case formulation, moreover, short runs and long runs are simply finite and infinite sequences of single cases, where each trial has propensities that are equal and independent from trial to trial and classic theorems of statistical inference apply. Envisioning dispositions as single-case causal tendencies brought the further benefit that the differences between them could be formalized using causal conditionals “__ \(=n=\gt \ldots\)” of variable strength \(n\) in the case of probabilistic laws and of universal strength \(u\) in the case of universal laws “__ \(=u=\gt \ldots\)” (Fetzer 1974, 1981). This completes the conception of natural laws as subjunctive conditionals attributing permanent properties to universals as dispositions and thereby justifies two basic models of explanation, the universal-deductive (U-D) model and the probabilistic-inductive (P-I) model, where, for example, why a match of kind \(K\) lighted when struck could assumes the following form by adopting appropriate formulations when temporal constants “\(t\)” and quantifiers “\((t)\)” are included and “\(t^*\)” occurs some specific interval of time after “\(t\)”: Figure 8. A Universal-Deductive Explanation Explanations for indeterministic phenomena are equally straightforward. If the half-life of 3.05 minutes is a permanent property of atoms of polonium-\(218 (^{218}\)P), for example, then a probabilistic law could express its meaning as a disposition \(H\) of strength .5 to undergo decay \(D\) during any 3.05 minute interval \(I\), which could be formally displayed as follows: Figure 9. A Probabilistic-Inductive Explanation Here the value of [\(r\)] can be justified as a degree of nomic expectability that applies to every single case of the reference property and implies that, for collections of atoms of \(^{218}\)P, approximately half will undergo decay during any 3.05 minute interval, which enables lawlike sentences of probabilistic form to be subjected to empirical test, on the basis of relative frequencies, especially by attempting to refute them. Salmon also expressed enthusiasm for this approach, which he regarded as “a straightforward generalization” of Hempel’s account (Salmon 1989: 83–89; Fetzer 1992). And it does appear to be a suitable successor, not only to Hempel’s I-S but also his D-N approach. Even this intensional explication would still be vulnerable to the problems of irrelevant properties and the modus tollens paradox but for the adoption of a condition to exclude the occurrence of predicates that are not nomically relevant to the explanandum event from the explanans of an adequate scientific explanation. This criterion, known as the requirement of strict maximal specificity, permits the reformulation of Hempel’s four conditions of adequacy by replacing the redundant empirical content condition with this new requirement (Fetzer 1981; Salmon 1989). Explanations not only display the nomic expectability of their explanandum events—which, in the case of those that occur with high probability, would enable them to have been predicted, as Hempel proposed but—more importantly—explain them by specifying all and only those properties nomically responsible for their occurrence, even when they occur with low probability (Fetzer 1992). Bromberger’s flagpole counterexample provides a severe test of this requirement. The reason why the inference from the length of the shadow to the height of the flagpole is non-explanatory is because the length of the shadow is not nomically responsible for the flagpole’s height. Hempel’s original conditions could not cope with situations of this kind, where inferences using laws support predictions and retrodictions that are not also explanations, even though they satisfy all four. This alternative condition thus requires that the properties cited in the antecedent of the lawlike premise(s) must be nomically relevant to the explanandum or may not be included there (Fetzer 1981, 1992). The height of the flagpole, but not the length of the shadow, qualifies as nomically relevant, which resolves the quandary—at the expense of acknowledging classes of arguments that are predictive or retrodictive but not explanatory, even when they involve inferences from law(s) that satisfy Hempel’s original criteria. The reformulated conditions are: By formulating (CA-1*) in this way, the covering law conception of the subsumption of singular events by general laws is preserved; but abandonment of the high-probability requirement led both Salmon (1971) and Alberto Coffa (1973) to question whether or not explanations still properly qualify as “arguments”. At the least, however, they would appear to be special kinds of “explanatory arguments”, even when they involve low probabilities. These revised conditions implicitly require abandoning Hempel’s commitment to extensional methodology to capture the notion of nomic responsibility, but the benefits of an intensional approach appear to be profound. As Salmon has observed, [on such an explication] the distinction between description and prediction, on the one hand, and explanation, on the other, is that the former can proceed in an extensional language framework, while the latter demands an intensional language framework. It remains to be seen whether the intensional logic can be satisfactorily formulated (Salmon 1989: 172, original emphasis). This approach to explanation incorporates the causal relevance criterion, according to which a property \(H\) is causally relevant to the occurrence of an attribute \(G\) relative to a reference property \(F\)—within the context of a causal explanation—just in case (CR): that is, the propensity for \(G\), given \(F \amp H\), differs from the propensity for \(G\), given \(F \amp \neg H\), where the presence or absence of \(H\) affects the single-case causal tendency for \(G\). The universal generalization of sentential functions like these thus produces lawlike sentences, while their instantiation to individual constants or to ambiguous names produces (what are known as) nomological conditionals (Fetzer 1981: 49–54). The introduction of the probabilistic causal calculus \(C\), moreover, responds to Salmon’s concerns by providing a formalization within intensional logic that resolves them (Fetzer & Nute 1979, 1980).