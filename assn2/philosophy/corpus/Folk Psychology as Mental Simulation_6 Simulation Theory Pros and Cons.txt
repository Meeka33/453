 ST has sparked a lively debate, which has been going on since the end of the 1980s. This debate has dealt with a great number of theoretical and empirical issues. On the theoretical side, we have seen philosophical discussions of the relation between ST and functionalism (Gordon 1986; Goldman 1989; Heal 2003; Stich & Ravenscroft 1992), and of the role of tacit knowledge in cognitive explanations (Davies 1987; Heal 1994; Davies & Stone 2001), just to name a few. Examples of empirical debates are: how to account for mindreading deficits in Autism Spectrum Disorders (Baron-Cohen 2000; Currie & Ravenscroft 2002), or how to explain the evolution of mindreading (Carruthers 2009; Lurz 2011). It goes without saying that discussing all these bones of contention would require an entire book (most probably, a series of books). In the last section of this entry, we confine ourselves to briefly introducing the reader to a small sample of the main open issues concerning ST. We wrote that ST proposes that mirroring processes (i.e., activations of mirror mechanisms in the perception mode): (A) are (low-level) simulation processes, and (B) contribute (either constitutively or causally) to mindreading (Gallese et al. 2004; Gallese & Goldman 1998; Goldman 2006, 2008b; Hurley 2005). Both (A) and (B) have been vehemently contested by ST’s opponents. Beginning with (A), it has been argued that mirroring processes do not qualify as simulation processes, because they fail to satisfy the definition of “simulation process” (Gallagher 2007; Herschbach 2012; Jacob 2008; Spaulding 2012) and/or because they are better characterized in different terms, e.g., as enactive perceptual processes (Gallagher 2007) or as elements in an information-rich process (Spaulding 2012). As for (B), the main worry runs as follows. Granting that mirroring processes are simulation processes, what evidence do we have for the claim that they contribute to mindreading? This, in particular, has been asked with respect to the role of mirroring processes in “action understanding” (i.e., the interpretation of an agent’s behavior in terms of the agent’s intentions, goals, etc.). After all, the neuroscientific evidence just indicates that action mirroring correlates with episodes of action understanding, but correlation is not causation, let alone constitution. In fact, there are no studies examining whether disruption of the monkey mirror neuron circuit results in action understanding deficits, and the evidence on human action understanding following damage to the action mirror mechanism is inconclusive at best (Hickok 2009). In this regard, some authors have suggested that the most plausible hypothesis is instead that action mirroring follows (rather than causes or constitutes) the understanding of others’ mental states (Csibra 2007; Jacob 2008). For example, Jacob (2008) proposes that the job of mirroring processes in the action domain is just that of computing a representation of the observed agent’s next movement, on the basis of a previous representation of the agent’s intention. Similar deflationary accounts of the action mirror mechanism have been given by Brass et al. (2007), Hickok (2014), and Vannuscorps and Caramazza (2015)—these accounts typically take the STS (superior temporal sulcus, a brain region lacking mirror neurons) to be the critical neural area for action understanding. There are various ways to respond to these criticisms. A strong response argues that they are based on a misunderstanding of the relevant empirical findings, as well as on a mischaracterization of the role that ST attributes to the action mirror mechanism in action understanding (Rizzolatti & Sinigaglia 2010, 2014). A weaker response holds that the focus on action understanding is a bit of a red herring, given that the most robust evidence in support of the central role played by mirroring processes in mindreading comes from the emotion domain (Goldman 2008b). We will consider the weaker response here. Goldman and Sripada (2005) discuss a series of paired deficits in emotion production and face-based emotion mindreading. These deficits—they maintain—are best explained by the hypothesis that one attributes emotions to someone else through simulating these emotions in oneself: when the ability to undergo the emotion breaks down, the mindreading capacity breaks down as well. Barlassina (2013) elaborates on this idea by considering Huntington’s Disease (HD), a neurodegenerative disorder resulting in, among other things, damage to the disgust mirror mechanism. As predicted by ST, the difficulties individuals with HD have in experiencing disgust co-occur with an impairment in attributing disgust to someone else on the basis of observing her facial expression—despite perceptual abilities and knowledge about disgust being preserved in this clinical population. Individuals suffering from HD, however, exhibit an intact capacity for disgust mindreading on the basis of non-facial visual stimuli. For this reason, Barlassina concludes by putting forward an ST-TT hybrid model of disgust mindreading on the basis of visual stimuli. ST’s central claim is that we reuse our own cognitive mechanisms to arrive at a representation of other people’s mental states. This claim raises a number of issues concerning how ST conceptualizes the self-other relation. We will discuss a couple of them. Gallagher (2007: 355) writes that given the large diversity of motives, beliefs, desires, and behaviours in the world, it is not clear how a simulation process … can give me a reliable sense of what is going on in the other person’s mind. There are two ways of interpreting Gallagher’s worry. First, it can be read as saying that if mindreading is based on mental simulation, then it is hard to see how mental state attributions could be epistemically justified. This criticism, however, misses the mark entirely, since ST is not concerned with whether mental state attributions count as knowledge, but only with how, as a matter of fact, we go about forming such attributions. A second way to understand Gallagher’s remarks is this: as a matter of fact, we are pretty successful in understanding other minds; however, given the difference among individual minds, this pattern of successes cannot be explained in terms of mental simulation. ST has a two-tier answer to the second reading of Gallagher’s challenge. First, human beings are very similar with regard to cognitive processes such as perception, theoretical reasoning, practical reasoning, etc. For example, there is a very high probability that if both you and I look at the same scene, we will have the same visual experience. This explains why, in the large majority of cases, I can reuse my visual mechanism to successfully simulate your visual experiences. Second, even though we are quite good at recognizing others’ mental states, we are nonetheless prone to egocentric errors, i.e., we tend to attribute to a target the mental state that we would undergo if we were in the target’s situation, rather than the actual mental state the target is in (Goldman 2006). A standard example is the curse of knowledge bias, where we take for granted that other people know what we know (Birch & Bloom 2007). ST has a straightforward explanation of such egocentric errors (Gordon 1995; Goldman 2006): if we arrive at attributing mental states via mental simulation, the attribution accuracy will depend on our capacity to “quarantine” our genuine mental states, when they do not match the target’s, and to replace them with more appropriate simulated mental states. This “adjustment” process, however, is a demanding one, because our genuine mental states exert a powerful tendency. Thus, Gallagher is right when he says that, on some occasions, “if I project the results of my own simulation onto the other, I understand only myself in that other’s situation, but I don’t understand the other” (Gallagher 2007: 355). However, given how widespread egocentric errors are, this counts as a point in favour of ST, rather than as an argument against it (but see de Vignemont & Mercier 2016, and Saxe 2005). Carruthers (1996, 2009, 2011) raises a different problem for ST: no version of ST can adequately account for self-attributions of mental states. Recall that, according to Goldman (2006), simulation-based mindreading is a three-stage process in which we first mentally simulate a target’s mental state, we then introspect and categorize the simulated mental state, and we finally attribute the categorized state to the target. Since Goldman’s model has it that attributions of mental states to others asymmetrically depend on the ability to introspect one’s own mental states, it predicts that: (A) introspection is (ontogenetically and phylogenetically) prior to the ability to represent others’ mental states; (B) there are cases in which introspection works just fine, but where the ability to represent others’ mental states is impaired (presumably, because the mechanism responsible for projecting one’s mental states to the target is damaged). Carruthers (2009) argues that neither (A) nor (B) are borne out by the data. The former because there are no creatures that have introspective capacities but at the same time lack the ability to represent others’ mental states; the latter because there are no dissociation cases in which an intact capacity for introspection is paired with an impairment in the ability to represent others’ mental states. How might a Simulation Theorist respond to this objection? As we said in  section 4,  Gordon’s (1986, 1995, 1996) Radical Simulationism does not assign any role to introspection in mindreading. Rather, Gordon proposes that self-ascriptions are guided by ascent routines through which we answer the question “Do I believe that p?” by answering the lower-order question “Is it the case that p?” Carruthers (1996, 2011) thinks that this won’t do either. Here is one of the many problems that Carruthers raises for this suggestion—we can call it “The Scope Problem”: this suggestion appears to have only a limited range of application. For even if it works for the case of belief, it is very hard to see how one might extend it to account for our knowledge of our own goals, decisions, or intentions—let alone our knowledge of our own attitudes of wondering, supposing, fearing, and so on. (Carruthers 2011: 81) Carruthers’ objections are important and deserve to be taken seriously. To discuss them, however, we would need to introduce a lot of further empirical evidence and many complex philosophical ideas about self-knowledge. This is not a task that we can take up here (the interested reader is encouraged to read, in addition to Gordon (2007) and Goldman (2009), the SEP entries on  self-knowledge  and on  introspection).  The take-home message should be clear enough nonetheless: anybody who puts forward an account of mindreading should remember that such an account has to cohere with a plausible story about the cognitive mechanisms underlying self-attribution. The development of mindreading capacities in children has been one of the central areas of empirical investigation. In particular, developmental psychologists have put a lot of effort into detailing how the ability to attribute false beliefs to others develops. Until 2005, the central experimental paradigm to test this ability was the verbal false belief task (Wimmer & Perner 1983). Here is a classic version of it. A subject is introduced to two dolls, Sally and Anne, and three objects: Sally’s ball, a basket, and a box. Sally puts her ball in the basket and leaves the scene. While Sally is away, Anne takes the ball out of the basket and puts it into the box. Sally then returns. The subject is asked where she thinks Sally will look for the ball. The correct answer, of course, is that Sally will look inside the basket. To give this answer, the subject has to attribute to Sally the false belief that the ball is in the basket. A number of experiments have found that while four-year old children pass this task, three-year old children fail it (for a review, see Wellman et al. 2001). For a long time, the mainstream interpretation of these findings has been that children acquire the ability to attribute false beliefs only around their fourth birthday (but see Clements & Perner 1994 and Bloom & German 2000). In 2005, this developmental timeline was called into question. Kristine Onishi and Renée Baillargeon (2005) published the result of a non-verbal version of the false belief task, which they administered to 15-month old infants. The experiment involves three steps. First, the infants see a toy between two boxes, one yellow and one green, and then an actor hiding the toy inside the green box. Next, the infants see the toy sliding out of the green box and hiding inside the yellow box. In the true belief condition (TB), the actor notices that the toy changes location, while in the false belief condition (FB) she does not. Finally, half of the infants see the actor reaching into the green box, while the other half sees the actor reaching into the yellow box. According to the violation-of-expectation paradigm, infants reliably look for a longer time at unexpected events. Therefore, if the infants expected the actor to search for the toy on the basis of the actor’s belief about its location, then when the actor had a true belief that the toy was hidden in one box, the infants should look longer when the actor reached into the other box instead. Conversely, the infants should look longer at one box when the actor falsely believed that the toy was hidden in the other box. Strikingly, these predictions were confirmed in both the (TB) and (FB) conditions. On this basis, Onishi and Baillargeon (2005) concluded that children of 15 months possess the capacity to represent others’ false beliefs. This and subsequent versions of non-verbal false belief tasks attracted a huge amount of interest (at the current stage of research, there is evidence that sensitivity to others’ false beliefs is present in infants as young as 7 months—for a review, see Baillargeon at al. 2016). Above all, the following two questions have been widely discussed: why do children pass the non-verbal false belief task at such an early age, but do not pass the verbal version before the age of 4? Does passing the non-verbal false belief task really indicate the capacity to represent others’ false beliefs? (Perner & Ruffman 2005; Apperly & Butterfill 2009; Baillargeon et al. 2010; Carruthers 2013; Helming et al. 2014). Goldman and Jordan (2013) maintain that ST has a good answer to both questions. To begin with, they argue that it is implausible to attribute to infants such sophisticated meta-representational abilities as the ability to represent others’ false beliefs. Thus, Goldman and Jordan favour a deflationary view, according to which infants are sensitive to others’ false beliefs, but do not represent them as such. In particular, they propose that rather than believing that another subject S (falsely) believes that p, infants simply imagine how the world is from S’s perspective—that is, they simply imagine that p is the case. This—Goldman and Jordan say—is a more primitive psychological competence than mindreading, since it does not involve forming a judgment about others’ mental states. This brings us to Goldman and Jordan’s answer to the question “why do children pass the verbal false belief task only at four?” Passing this task requires fully-fledged mindreading abilities and executive functions such as inhibitory control. It takes quite a lot of time—around 3 to 4 years—before these functions and abilities come online.