 Some features of information are intuitive. We are used to information being encoded, transmitted and stored. One also expects it to be additive (information \(a +\) information \(b =\) information \(a + b)\) and non-negative, like other things in life, such as probabilities and interest rates. If you ask a question, the worst scenario is that you receive no answer or a wrong answer, which will leave you with zero new information. Similar properties of information are quantifiable. They are investigated by the mathematical theory of communication (MTC) with the primary aim of devising efficient ways of encoding and transferring data. The name for this branch of probability theory comes from Shannon’s seminal work (Shannon and Weaver [1949]). Shannon pioneered this field and obtained many of its principal results, but he acknowledged the importance of previous work done by other researchers and colleagues at Bell laboratories, most notably Nyquist and Hartley (see Cherry [1978] and Mabon [1975]). After Shannon, MTC became known as information theory, an appealing but unfortunate label, which continues to cause endless misunderstandings. Shannon came to regret its widespread popularity, and we shall avoid using it in this context. This second part of the article outlines some of the key ideas behind MTC, with the aim of understanding the relation between MTC and some philosophical theories of semantic information. The reader with no taste for mathematical formulae may wish to go directly to section 2.2, where some conceptual implications of MTC are outlined. The reader interested in knowing more may start by reading Weaver [1949], Pierce [1980], Shannon and Weaver [1949 rep. 1998], then Jones [1979], and finally Cover and Thomas [1991]. The latter two are technical texts. Floridi [2010] provides a brief and simplified analysis oriented to philosophy students. MTC has its origin in the field of electrical engineering, as the study of communication limits. It develops a quantitative approach to information as a means to answer two fundamental problems: the ultimate level of data compression (how small can a message be, given the same amount of information to be encoded?) and the ultimate rate of data transmission (how fast can data be transmitted over a channel?). The two solutions are the entropy \(H\) in equation [9] (see below) and the channel capacity \(C\). The rest of this section illustrates how to get from the problems to the solutions. To have an intuitive sense of the approach, let us return to our example. Recall the telephone conversation with the mechanic. In Figure 2, the wife is the informer, the mechanic is the informee, “the battery is flat” is the (semantic) message (the informant), there is a coding and decoding procedure through a natural language (English), a channel of communication (the telephone system) and some possible noise. Informer and informee share the same background knowledge about the collection of usable symbols (technically known as the alphabet; in the example this is English). Figure 3. Communication model (adapted from Shannon and Weaver [1949]) MTC is concerned with the efficient use of the resources indicated in Figure 3. Now, the conversation with the mechanic is fairly realistic and hence more difficult to model than a simplified case. We shall return to it later but, in order to introduce MTC, imagine instead a very boring device that can produce only one symbol. Edgar Alan Poe wrote a short story in which a raven can answer only “nevermore” to any question. Poe’s raven is called a unary device. Imagine you ring the garage and your call is answered by Poe’s raven. Even at this elementary level, Shannon’s simple model of communication still applies. It is obvious that the raven (a unary device) produces zero amount of information. Simplifying, we already know the outcome of the communication exchange, so our ignorance (expressed by our question) cannot be decreased. Whatever the informational state of the system is, asking appropriate questions (e.g., “Will I be able to make the car start?”, “Can you come to fix the car?”) of the raven does not make any difference. Note that, interestingly enough, this is the basis of Plato’s famous argument in the Phaedrus against the value of semantic information provided by written texts: [Socrates]: Writing, Phaedrus, has this strange quality, and is very like painting; for the creatures of painting stand like living beings, but if one asks them a question, they preserve a solemn silence. And so it is with written words; you might think they spoke as if they had intelligence, but if you question them, wishing to know about their sayings, they always say only one and the same thing [they are unary devices, in our terminology]. And every word, when [275e] once it is written, is bandied about, alike among those who understand and those who have no interest in it, and it knows not to whom to speak or not to speak; when ill-treated or unjustly reviled it always needs its father to help it; for it has no power to protect or help itself. As Plato well realises a unary source answers every question all the time with only one message, not with silence or message, since silence counts as a message, as we saw in 2.5, when discussing the nature of secondary information. It follows that a completely silent source also qualifies as a unary source. And if silencing a source (censorship) may be a nasty way of making a source uninformative, it is well known that crying wolf is a classic case in which an informative source degrades to the role of uninformative unary device. Consider now a binary device that can produce two symbols, like a fair coin \(A\) with its two equiprobable symbols \(\{h, t\}\); or, as Matthew 5:37 suggests, “Let your communication be Yea, yea; Nay, nay: for whatsoever is more than these cometh of evil”. Before the coin is tossed, the informee (for example a computer) is in a state of data deficit greater than zero: the informee does not “know” which symbol the device will actually produce. Shannon used the technical term “uncertainty” to refer to data deficit. In a non-mathematical context this can be a very misleading term because of the strong epistemological connotations of this term. Remember that the informee can be a simple machine, and psychological, mental or doxastic states are clearly irrelevant. Once the coin has been tossed, the system produces an amount of information that is a function of the possible outputs, in this case 2 equiprobable symbols, and equal to the data deficit that it removes. Let us now build a slightly more complex system, made of two fair coins \(A\) and \(B\). The \(AB\) system can produce 4 ordered outputs: \(\langle h, h\rangle , \langle h, t\rangle , \langle t, h\rangle , \langle t, t\rangle\). It generates a data deficit of 4 units, each couple counting as a symbol in the source alphabet. In the \(AB\) system, the occurrence of each symbol \(\langle \cdot, \cdot \rangle\) removes a higher data deficit than the occurrence of a symbol in the \(A\) system. In other words, each symbol provides more information. Adding an extra coin would produce a 8 units of data deficit, further increasing the amount of information carried by each symbol in the \(ABC\) system, and so on. We are now ready to generalise the examples. Call the number of possible symbols \(N\). For \(N = 1\), the amount of information produced by a unary device is 0. For \(N = 2\), by producing an equiprobable symbol, the device delivers 1 unit of information. And for \(N = 4\), by producing an equiprobable symbol the device delivers the sum of the amount of information provided by a device producing one of two equiprobable symbols (coin \(A\) in the example above) plus the amount of information provided by another device producing one of two equiprobable symbols (coin \(B)\), that is, 2 units of information, although the total number of symbols is obtained by multiplying \(A\)’s symbols by \(B\)’s symbols. Now, our information measure should be a continuous and monotonic function of the probability of the symbols. The most efficient way of satisfying these requirements is by using the logarithm to the base 2 of the number of possible symbols (the logarithm to the base 2 of a number \(n\) is the power to which 2 must be raised to give the number \(n\), for example \(\log_2 8 = 3\), since \(2^3 = 8)\). Logarithms have the useful property of turning multiplication of symbols into addition of information units. By taking the logarithm to the base 2 (henceforth log simply means \(\log_2)\) we have the further advantage of expressing the units in bits. The base is partly a matter of convention, like using centimetres instead of inches, partly a matter of convenience, since it is useful when dealing with digital devices that use binary codes to represent data. Given an alphabet of \(N\) equiprobable symbols, we can now use equation [1]: to rephrase some examples more precisely: Some communication devices and their information power The basic idea is all in equation [1]. Information can be quantified in terms of decrease in data deficit (Shannon’s “uncertainty”). Unfortunately, real coins are always biased. To calculate how much information they produce one must rely on the frequency of the occurrences of symbols in a finite series of tosses, or on their probabilities, if the tosses are supposed to go on indefinitely. Compared to a fair coin, a slightly biased coin must produce less than 1 bit of information, but still more than 0. The raven produced no information at all because the occurrence of a string \(S\) of “nevermore” was not informative (not surprising, to use Shannon’s more intuitive, but psychologistic vocabulary), and that is because the probability of the occurrence of “nevermore” was maximum, so overly predictable. Likewise, the amount of information produced by the biased coin depends on the average informativeness (also known as average surprisal, another unfortunate term to refer to the average statistical rarity) of the string \(S\) of \(h\) and \(t\) produced by the coin. The average informativeness of the resulting string \(S\) depends on the probability of the occurrence of each symbol. The higher the frequency of a symbol in \(S\), the less information is being produced by the coin, up to the point when the coin is so biased to produce always the same symbol and stops being informative at all, behaving like the raven or the boy who cries wolf. So, to calculate the average informativeness of \(S\) we need to know how to calculate \(S\) and the informativeness of the \(i^{\text{th}}\) symbol in general. This requires understanding what the probability of the \(i^{\text{th}}\) symbol \((P_i)\) to occur is. The probability \(P_i\) of the \(i^{\text{th}}\) symbol can be “extracted” from equation [1], where it is embedded in \(\log(N)\), a special case in which the symbols are equiprobable. Using some elementary properties of the logarithmic function, we have: The value of \(1/N = P\) can range from 0 to 1. If Poe’s raven is our source, the probability of it saying “good morning” is 0. In the case of the coin, \(P(h) + P(t) = 1\), no matter how biased the coin is. Probability is like a cake that gets sliced more and more thinly depending on the number of guests, but never grows beyond its original size and, in the worst case scenario, can at most be equal to zero, but never become “negative”. More formally, this means: The sigma notation in [3] is simply a shortcut that indicates that if we add all probabilities values from \(i = 1\) to \(i =\) N their sum is equal to 1. We can now be precise about the raven: “nevermore” is not informative at all because \(P_{nevermore} = 1\). Clearly, the lower the probability of occurrence of a symbol, the higher is the informativeness of its actual occurrence. The informativeness \(u\) of the \(i^{\text{th}}\) symbol can be expressed by analogy with \(-\log(P)\) in equation [4]: Next, we need to calculate the length of a general string \(S\). Suppose that the biased coin, tossed 10 times, produces the string: \(\langle h, h, t, h, h, t, t, h, h, t\rangle\). The (length of the) string \(S\) (in our case equal to 10) is equal to the number of times the \(h\) type of symbol occurs added to the numbers of times the \(t\) type of symbol occurs. Generalising for \(i\) types of symbols: Putting together equations [4] and [5] we see that the average informativeness for a string of \(S\) symbols is the sum of the informativeness of each symbol divided by the sum of all symbols: Term [6] can be simplified thus: Now \(S_i /S\) is the frequency with which the \(i^{\text{th}}\) symbol occurs in \(S\) when \(S\) is finite. If the length of \(S\) is left undetermined (as long as one wishes), then the frequency of the \(i^{\text{th}}\) symbol becomes its probability \(P_i\). So, further generalising term [7], we have: Finally, by using equation [4] we can substitute for \(u_i\) and obtain Equation [9] is Shannon’s formula for \(H =\) uncertainty, which we have called data deficit (actually, Shannon’s original formula includes a positive constant \(K\) which amounts to a choice of a unit of measure, bits in our case; apparently, Shannon used the letter \(H\) because of R.V.L. Hartley’s previous work). Equation [9] indicates that the quantity of information produced by a device corresponds to the amount of data deficit erased. It is a function of the average informativeness of the (potentially unlimited) string of symbols produced by the device. It is easy to prove that, if symbols are equiprobable, [9] reduces to [1] and that the highest quantity of information is produced by a system whose symbols are equiprobable (compare the fair coin to the biased one). To arrive at [9] we have used some very simple examples: a raven and a handful of coins. Things in life are far more complex, witness our Monday morning accident. For example, we have assumed that the strings of symbols are ergodic: the probability distribution for the occurrences of each symbol is assumed to be stable through time and independently of the selection of a certain string. Our raven and coins are discrete and zero-memory sources. The successive symbols they produce are statistically independent. But in real life occurrences of symbols are often interdependent. Sources can be non-ergodic and have a memory. Symbols can be continuous, and the occurrence of one symbol may depend upon a finite number \(n\) of preceding symbols, in which case the string is known as a Markov chain and the source an \(n^{\text{th}}\) order Markov source. Consider for example the probability of hearing “n” (followed by the string “ing”) after having received the string of letters “Good mor_” over the phone, when you called the garage. And consider the same example through time, in the case of a child (the son of the mechanic) who is learning how to answer the phone instead of his father. In brief, MTC develops the previous analysis to cover a whole variety of more complex cases. We shall stop here, however, because in the rest of this section we need to concentrate on other central aspects of MTC. The quantitative approach just sketched plays a fundamental role in coding theory (hence in cryptography) and in data storage and transmission techniques. MTC is primarily a study of the properties of a channel of communication and of codes that can efficiently encipher data into recordable and transmittable signals. Since data can be distributed either in terms of here/there or now/then, diachronic communication and synchronic analysis of a memory can be based on the same principles and concepts (our coin becomes a bistable circuit or flip-flop, for example). Two concepts that play a pivotal role both in communication analysis and in memory management are so important to deserve a brief explanation: redundancy and noise. Consider our \(AB\) system. Each symbol occurs with 0.25 probability. A simple way of encoding its symbols is to associate each of them with two digits, as follows: In Code 1 a message conveys 2 bits of information, as expected. Do not confuse bits as bi-nary units of information (recall that we decided to use log\(_2\) also as a matter of convenience) with bits as bi-nary digits, which is what a 2-symbols system like a CD-ROM uses to encode a message. Suppose now that the \(AB\) system is biased, and that the four symbols occur with the following probabilities: This biased system produces less information, so by using Code 1 we would be wasting resources. A more efficient Code 2 (see below) should take into account the symbols’ probabilities, with the following outcomes: In Code 2, known as Fano Code, a message conveys 1.75 bits of information. One can prove that, given that probability distribution, no other coding system will do better than Fano Code. In real life, a good codification is also modestly redundant. Redundancy refers to the difference between the physical representation of a message and the mathematical representation of the same message that uses no more bits than necessary. Compression procedures work by reducing data redundancy, but redundancy is not always a bad thing, for it can help to counteract equivocation (data sent but never received) and noise (data received but unwanted). A message + noise contains more data than the original message by itself, but the aim of a communication process is fidelity, the accurate transfer of the original message from sender to receiver, not data increase. We are more likely to reconstruct a message correctly at the end of the transmission if some degree of redundancy counterbalances the inevitable noise and equivocation introduced by the physical process of communication and the environment. Noise extends the informee’s freedom of choice in selecting a message, but it is an undesirable freedom and some redundancy can help to limit it. That is why the manual of your car includes both verbal explanations and pictures to convey (slightly redundantly) the same information. We are now ready to understand Shannon’s two fundamental theorems. Suppose the 2-coins biased system \(AB\) produces the following message: \(\langle t, h\rangle \langle h, h\rangle \langle t, t\rangle \langle h, t\rangle \langle h, t\rangle\). Using Fano Code we obtain: 11001111010. The next step is to send this string through a channel. Channels have different transmission rates \((C)\), calculated in terms of bits per second (bps). Shannon’s fundamental theorem of the noiseless channel states that: In other words, if you devise a good code you can transmit symbols over a noiseless channel at an average rate as close to \(C/H\) as one may wish but, no matter how clever the coding is, that average can never exceed \(C/H\). We have already seen that the task is made more difficult by the inevitable presence of noise. However, the fundamental theorem for a discrete channel with noise comes to our rescue: Roughly, if the channel can transmit as much or more information than the source can produce, then one can devise an efficient way to code and transmit messages with as small an error probability as desired. These two fundamental theorems are among Shannon’s greatest achievements. They are limiting results in information theory that constrain any conceptual analysis of semantic information. They are thus comparable to Gödel’s, Turing’s, and Church’s theorems in logic and computation. With our message finally sent, we may close this section and return to a more philosophical approach. For the mathematical theory of communication (MTC), information is only a selection of one symbol from a set of possible symbols, so a simple way of grasping how MTC quantifies information is by considering the number of yes/no questions required to determine what the source is communicating. One question is sufficient to determine the output of a fair coin, which therefore is said to produce 1 bit of information. A 2-fair-coins system produces 4 ordered outputs: \(\langle h, h\rangle , \langle h, t\rangle , \langle t, h\rangle , \langle t, t\rangle\) and therefore requires at least two questions, each output containing 2 bits of information, and so on. This erotetic (the Greek word for “question”) analysis clarifies two important points. First, MTC is not a theory of information in the ordinary sense of the word. In MTC, information has an entirely technical meaning. Consider some examples. According to MTC, two equiprobable “yes”’s contain the same quantity of information, no matter whether their corresponding questions are “have the lights of your car been left switched on for too long, without recharging the battery?” or “would you marry me?”. If we knew that a device could send us, with equal probabilities, either this article or the whole Stanford Encyclopedia of Philosophy, by receiving one or the other we would receive very different amounts of bytes of data but actually only one bit of information in the MTC sense of the word. On June 1 1944, the BBC broadcasted a line from Verlaine’s Song of Autumn: “Les sanglots longs des violons de Autumne”. The message contained almost 1 bit of information, an increasingly likely “yes” to the question whether the D-Day invasion was imminent. The BBC then broadcasted the second line “Blessent mon coeur d’une longueur monotone”. Another almost meaningless string of letters, but almost another bit of information, since it was the other long-expected “yes” to the question whether the invasion was to take place immediately. German intelligence knew about the code, intercepted those messages and even notified Berlin, but the high command failed to alert the Seventh Army Corps stationed in Normandy. Hitler had all the information in Shannon’s sense of the word, but failed to understand (or believe in) the crucial importance of those two small bits of data. As for ourselves, we were not surprised to conclude in the previous section that the maximum amount of information (again, in the MTC sense of the word) is produced by a text where each character is equally distributed, that is by a perfectly random sequence. According to MTC, the classic monkey randomly pressing typewriter keys is indeed producing a lot of information. Second, since MTC is a theory of information without meaning (not in the sense of meaningless, but in the sense of not yet meaningful), and since we have seen that [information \(-\) meaning = data], “mathematical theory of data communication” is a far more appropriate description of this branch of probability theory than “information theory”. This is not a mere question of labels. Information, as semantic content (more on this shortly), can also be described erotetically as data + queries. Imagine a piece of (propositional) information such as “the earth has only one moon”. It is easy to polarise almost all its semantic content by transforming it into a [query + binary answer], such as [does the earth have only one moon? + yes]. Subtract the “yes”—which is at most 1 bit of information, in the equiprobable case of a yes or no answer—and you are left with virtually all the semantic content, fully de-alethicised (from aletheia, the Greek word for truth; the query is neither true nor false). To use a Fregean expression, semantic content is unsaturated information, where the latter is semantic information that has been “eroteticised” and from which a quantity of information has been subtracted equal to \(-\log P(\text{yes})\), with \(P\) being the probability of the yes-answer. The datum “yes” works as a key to unlock the information contained in the query. MTC studies the codification and transmission of information by treating it as data keys, that is, as the amount of details in a signal or message or memory space necessary to saturate the informee’s unsaturated information. As Weaver [1949] remarked “the word information relates not so much to what you do say, as to what you could say. The mathematical theory of communication deals with the carriers of information, symbols and signals, not with information itself. That is, information is the measure of your freedom of choice when you select a message” (p. 12). Since MTC deals not with semantic information itself but with the data that constitute it, that is, with messages comprising uninterpreted symbols encoded in well-formed strings of signals, it is commonly described as a study of information at the syntactic level. MTC can be successfully applied in ICT (information and communication technologies) because computers are syntactical devices. What remains to be clarified is how \(H\) in equation [9] should be interpreted. \(H\) is also known in MTC as entropy. It seems we owe this confusing label to John von Neumann, who recommended it to Shannon: “You should call it entropy for two reasons: first, the function is already in use in thermodynamics under the same name; second, and more importantly, most people don’t know what entropy really is, and if you use the word entropy in an argument you will win every time” (quoted by Golan [2002]). Von Neumann proved to be right on both accounts, unfortunately. Assuming the ideal case of a noiseless channel of communication, \(H\) is a measure of three equivalent quantities: \(H\) can equally indicate (a) or (b) because, by selecting a particular alphabet, the informer automatically creates a data deficit (uncertainty) in the informee, which then can be satisfied (resolved) in various degrees by the informant. Recall the erotetic game. If you use a single fair coin, I immediately find myself in a 1 bit deficit predicament: I do not know whether it is head or tail. Use two fair coins and my deficit doubles, but use the raven, and my deficit becomes null. My empty glass (point (b) above) is an exact measure of your capacity to fill it (point (a) above). Of course, it makes sense to talk of information as quantified by \(H\) only if one can specify the probability distribution. Regarding (c), MTC treats information like a physical quantity, such as mass or energy, and the closeness between equation [9] and the formulation of the concept of entropy in statistical mechanics was already discussed by Shannon. The informational and the thermodynamic concept of entropy are related through the concepts of probability and randomness (“randomness” is better than “disorder” since the former is a syntactical concept whereas the latter has a strongly semantic value, that is, it is easily associated to interpretations, as I used to try to explain to my parents when I was young). Entropy is a measure of the amount of “mixedupness” in processes and systems bearing energy or information. Entropy can also be seen as an indicator of reversibility: if there is no change of entropy then the process is reversible. A highly structured, perfectly organised message contains a lower degree of entropy or randomness, less information in Shannon sense, and hence it causes a smaller data deficit, which can be close to zero (remember the raven). By contrast, the higher the potential randomness of the symbols in the alphabet, the more bits of information can be produced by the device. Entropy assumes its maximum value in the extreme case of uniform distribution, which is to say that a glass of water with a cube of ice contains less entropy than the glass of water once the cube has melted, and a biased coin has less entropy than a fair coin. In thermodynamics, we know that the greater the entropy, the less available the energy. This means that high entropy corresponds to high energy deficit, but so does entropy in MTC: higher values of \(H\) correspond to higher quantities of data deficit.