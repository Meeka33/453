 How do views about consciousness, intentionality, and their relationship impinge on the question of what it is to have a mind? Given a thin enough conception of intentionality, we might readily accept that having intentionality hardly suffices for having a mind. After all, street signs, books, and computer files could be said to represent or refer to things, and whatever we want from a distinction between the minded and the mindless (or between what has and lacks genuine understanding) we intend it to be more restrictive than that. We may say that the intentionality of true minds is obviously a rather different matter than the intentionality of the symbols that minded beings use. But still we may wonder exactly what makes for that difference. Could consciousness perhaps be what confers “minded” status? Given a paltry enough conception of consciousness, it’s hard to see how it could—or even how it could be necessary to mind at all. If one thinks of consciousness as brute, non-intentional sensation, for instance, it may seem not to have much to do with mind in any robust sense. But maybe if the right forms of consciousness are recognized, then these, rightly related to intentionality, will be enough to make a mind. If one sees consciousness as sufficient for fairly rich and important forms of intentionality, one has some prospect for arguing that, since it alone can endow what has it with intentionality of the right sort, only what has it, has a mind. Such a proposal requires we recognize some significant, “in kind” difference between the minded and the mindless, between those capable of understanding and those devoid of comprehension. Such recognition is not universal. On Dennett’s (1981) view, there is fundamentally nothing more to your being, as he puts it, “a true believer”—and thus nothing more ultimately to mind and understanding—than being interpretable by observers in the right way, via “the intentional stance”, so as to facilitate successful predictions of your behavior. Dennett points out that even the behavior of simple mechanisms like thermostats and natural phenomena like lightning can be regarded in this way. He of course recognizes a significant psychological difference between human beings and such marginal case “intentional systems”. This is, however, conceived of as a (considerable) difference of degree. Given your patterns of behavior, observers will be able to use much more complex intentional state attributions to predict what you will do, so as to get a much bigger pay-off in predictive value, than they will in other cases. But there is no more profound division among “intentional systems” than this, to be marked by talking about what is conscious and what isn’t. For consciousness is either just some mythical raw feel of atomic, non-intentional, ineffable “qualia”, or else it has to do with the fact that a predictive advantage accrues from regarding systems as possessed of some assortment of capacities, the exercise of at least some of which one is inclined to call “conscious”. But such differences are again a matter of degree, and the capacities in question exhibit no deep unity (Dennett 1991). However, many will think there is more to mind (and to consciousness) than Dennett allows. In offering an alternative to his perspective, one might try to locate a significant difference in kind between minded and mindless beings in something other than consciousness. Here however, we will briefly consider how one might put consciousness to work. Ways of making consciousness integral to mind are potentially very diverse. The issue will look rather different, for example, depending on whether one adopts this or that reductive intentionalist view. Suppose you hold that consciousness is reducible to states generated by sensory mechanisms, feeding the information they bear to certain other cognitive faculties. Then, to maintain consciousness is essential to mind, you would presumably need to maintain a broadly empiricist view that required such sensory input for mind. And a question would then arise about the sufficiency of consciousness for mind—at least for mind at a human level—as one may doubt that such perception by itself brings with it the requisite kind of understanding. On the other hand, if one’s reductive representationalism appeals to some sort of higher-order or self-representational account, one would need a conception of mind that explained why some such self-monitoring is necessary to—and how it could be enough for—mindedness. Generally, recent views that (explicitly or implicitly) make consciousness central to mind have tended not to base their arguments on reductive intentionalism. Some relevant strategies of argument unreliant on reductivist accounts can be found under the rubric of “Phenomenal Intentionality Theory”. Such views aim to show how all intentionality may be traced to phenomenal intentionality.  This approach trades on the idea that there is a difference between what comes by intentionality in some secondary or derivative way (as when an image or inscription is interpreted as referring to something), and what has intentionality in an original or more fundamental way. The proposal would be: consciousness, and only consciousness, brings with it original intentionality. It would be natural to assume further that only beings that have original intentionality truly have minds. Consider how this works in the case of Searle (1990, 1992). He distinguishes between what he calls “intrinsic” intentionality on the one hand, and merely “as if” and “interpreter relative” intentionality, on the other. We sometimes may speak as if artifacts (like thermostats) had thoughts and wants (“Mr. Thermostat thinks it’s too warm in here”)—but this isn’t to be taken literally. And we may impose conditions of satisfaction on our acts and creations (words, pictures, diagrams, etc.) by our interpretation of them—but they have no intentionality independent of our interpretive practices. The intentionality of mental states, on the other hand, is neither a mere manner of speech, nor is our possession of it derived from others’ interpretive stance towards us. But then: what accounts for the fact that some states of affairs in the world have intrinsic intentionality—that they are directed at objects under aspects—and why they are directed under the aspects they are (why they have the content they do)? With conscious states of mind, Searle says, their phenomenal or subjective character determines their aspectual shape. Where non-conscious states of mind are concerned, there is nothing to do the job but their relationship to consciousness. The right relationship, he holds, is this: non-conscious states of mind must be potentially conscious. If some psychological theories (of language, of vision) postulate an unconscious so deeply buried that its mental representations cannot even potentially become conscious, so much the worse for those theories. Searle’s views have aroused a number of criticisms. (See the peer commentary in response to Searle 1990.) Among the problem areas are these. First, how are we to spell out the requirement that intrinsically intentional states be potentially conscious, without making it either too easy or to difficult to satisfy? Second, just why is it that the intrinsic intentionality of non-conscious states needs accounting for, while that of conscious states is somehow unproblematic? Third, it appears Searle’s argument does not offer some general reason to rule out all efforts to give “naturalistic” accounts of conditions sufficient to impose—without the help of consciousness—genuine and not merely interpreter relative intentionality. Kriegel’s 2011 version of Phenomenal Intentionality Theory pursues a line of thought significantly like Searle’s. Drawing a distinction between original and interpreter-relative intentionality, he argues consciousness is the sole locus of the former: consciousness “injects intentionality into the world”. However, unlike Searle, Kriegel has consciousness confer intentionality both on otherwise meaningless symbols and on unconscious mental or cognitive life in much the same way. What has nonexperiential intentionality gets its aboutness by being such as would be seen to have it in the experience of an “ideal interpreter”. Kriegel, in effect, takes the sort of “interpretivist” view of intentionality advocated by Dennett, while (unlike Dennett) confining it to cases where consciousness is missing. Thus it seems he should want to reject theories like Dennett’s that purport to account for consciousness in terms of intentional or representational notions that do not presuppose it. This creates a difficulty, inasmuch as Kriegel also proposes a self-representationalist account of consciousness that seems to follow that general reductive representationalist strategy. He responds to this problem by distinguishing the sort of natural (or objective) “tracking” mental representation that figures as a posit in psychological theorizing from the sort of mental representation we are familiar with from reflection on our own minds, with its susceptibility to the appearance/reality distinction. Consciousness, Kriegel argues, is what accounts for the latter. There are a number of other ways in which one might account for mindedness in terms of consciousness that invoke some contrast between a kind of intentionality that is original or basic, and kinds that are either derived or merely “as if”.  But there are also consciousness-based accounts of mind that do not depend on a derived/underived intentionality distinction. Consider, for example, the argument proposed by Kirk Ludwig (1996a). He argues that consciousness is needed to rightly draw the boundaries of an individual’s mind—for there is nothing to determine whose state of mind a given non-conscious state of mind is, unless that state consists in a disposition to produce a conscious mental state of the right sort. Alleged mental processes that did not tend to produce someone’s conscious states of mind appropriately would be no one’s, which is to say that they would not be mental states at all. Roughly: consciousness provides the unity of mind without which there would be no mind. Only with consciousness do we have a suitable boundary between what’s in a mind, and what’s outside of it. And Ludwig argues that it is therefore a mistake to attribute to our minds many of the unconscious inferences with which psychological theorists have long been wont to populate the brain’s visual system. Another approach that find consciousness at the root of mind is suggested by Declan Smithies (2012b). Smithies picks up on the sort of distinction highlighted by Kriegel, between the kind of mental intentionality we attribute to ourselves, and the kind that figures in cognitivist theorizing about vision and language processing. Smithies, however, focuses on this as a distinction between personal and subpersonal levels, and makes no use of the derived/underived intentionality contrast. He argues that, intuitively, vision without consciousness (specifically, blindsight) fails to furnish a subject with the understanding of demonstrative reference, hence thought about perceptual particulars, that is provided when one consciously sees what is spoken of/thought about. Partly on this basis, he concludes that the sort of mental states—like belief—that are attributable to whole persons depend on consciousness. If we indulge the notion of beings behaviorally similar to us, but, like Chalmersian zombies, utterly bereft of subjective experience—we should not suppose they would literally have personal-level psychology: they would have no genuine beliefs, but at most act as if they did. A partly similar perspective is found in Siewert 2014 who uses consideration of blindsight scenarios to argue that the only perception that will supply the mind with an understanding of type demonstratives (e.g., “this shape”) and so be capable of functioning in empirical concept acquisition, is the experiential, conscious kind. (If this shape looks no way to you at all, you will not understand by vision just which shape this shape is, and thus won’t be in a position to learn visually, e.g., this shape is a circle.) Partly on these grounds it is argued that mind requires consciousness. And given a cognitively rich view of experience, we can see how consciousness (of the right sort) will be sufficient for mind as well. Both Smithies’ and Siewert’s views (like Searle’s) rely crucially on the idea that the difference between the presence and absence of understanding should be discernible from the first-person point of view. Doubts about them are likely to challenge the legitimacy of that reliance (see, e.g., Shoemaker’s 1994 criticism of Searle).