 The general philosophical and scientific outlook in the nineteenth century tended toward the empirical: platonistic aspects of rationalistic theories of mathematics were rapidly losing support. Especially the once highly praised faculty of rational intuition of ideas was regarded with suspicion. Thus it became a challenge to formulate a philosophical theory of mathematics that was free of platonistic elements. In the first decades of the twentieth century, three non-platonistic accounts of mathematics were developed: logicism, formalism, and intuitionism. There emerged in the beginning of the twentieth century also a fourth program: predicativism. Due to contingent historical circumstances, its true potential was not brought out until the 1960s. However it deserves a place beside the three traditional schools that are discussed in most standard contemporary introductions to philosophy of mathematics, such as (Shapiro 2000) and (Linnebo 2017). The logicist project consists in attempting to reduce mathematics to logic. Since logic is supposed to be neutral about matters ontological, this project seemed to harmonize with the anti-platonistic atmosphere of the time. The idea that mathematics is logic in disguise goes back to Leibniz. But an earnest attempt to carry out the logicist program in detail could be made only when in the nineteenth century the basic principles of central mathematical theories were articulated (by Dedekind and Peano) and the principles of logic were uncovered (by Frege). Frege devoted much of his career to trying to show how mathematics can be reduced to logic (Frege 1884). He managed to derive the principles of (second-order) Peano arithmetic from the basic laws of a system of second-order logic. His derivation was flawless. However, he relied on one principle which turned out not to be a logical principle after all. Even worse, it is untenable. The principle in question is Frege’s Basic Law V: In words: the set of the \(F\)s is identical with the set of the \(G\)s iff the \(F\)s are precisely the \(G\)s. In a famous letter to Frege, Russell showed that Frege’s Basic Law V entails a contradiction (Russell 1902). This argument has come to be known as Russell’s paradox (see  section 2.4). Russell himself then tried to reduce mathematics to logic in another way. Frege’s Basic Law V entails that corresponding to every property of mathematical entities, there exists a class of mathematical entities having that property. This was evidently too strong, for it was exactly this consequence which led to Russell’s paradox. So Russell postulated that only properties of mathematical objects that have already been shown to exist, determine classes. Predicates that implicitly refer to the class that they were to determine if such a class existed, do not determine a class. Thus a typed structure of properties is obtained: properties of ground objects, properties of ground objects and classes of ground objects, and so on. This typed structure of properties determines a layered universe of mathematical objects, starting from ground objects, proceeding to classes of ground objects, then to classes of ground objects and classes of ground objects, and so on. Unfortunately, Russell found that the principles of his typed logic did not suffice for deducing even the basic laws of arithmetic. He needed, among other things, to lay down as a basic principle that there exists an infinite collection of ground objects. This could hardly be regarded as a logical principle. Thus the second attempt to reduce mathematics to logic also faltered. And there matters stood for more than fifty years. In 1983, Crispin Wright’s book on Frege’s theory of the natural numbers appeared (Wright 1983). In it, Wright breathes new life into the logicist project. He observes that Frege’s derivation of second-order Peano Arithmetic can be broken down in two stages. In a first stage, Frege uses the inconsistent Basic Law V to derive what has come to be known as Hume’s Principle: The number of the \(F\)s = the number of the \(G\)s if and only if \(F\approx G\), where \(F \approx G\) means that the \(F\)s and the \(G\)s stand in one-to-one correspondence with each other. (This relation of one-to-one correspondence can be expressed in second-order logic.) Then, in a second stage, the principles of second-order Peano Arithmetic are derived from Hume’s Principle and the accepted principles of second-order logic. In particular, Basic Law V is not needed in the second part of the derivation. Moreover, Wright conjectured that in contrast to Frege’s Basic Law V, Hume’s Principle is consistent. George Boolos and others observed that Hume’s Principle is indeed consistent (Boolos 1987). Wright went on to claim that Hume’s Principle can be regarded as a truth of logic. If that is so, then at least second-order Peano arithmetic is reducible to logic alone. Thus a new form of logicism was born; today this view is known as neo-logicism (Hale & Wright 2001). Most philosophers of mathematics today doubt that Hume’s Principle is a principle of logic. Indeed, even Wright has in recent years sought to qualify this claim: he now argues that Hume’s Principle is analytic of our concept of number, and therefore at least a law of reason. Wright’s work has drawn the attention of philosophers of mathematics to the kind of principles of which Basic Law V and Hume’s Principle are examples. These principles are called abstraction principles. At present, philosophers of mathematics attempt to construct general theories of abstraction principles that explain which abstraction principles are acceptable and which are not, and why (Weir 2003; Fine 2002). Also, it has emerged that in the context of weakened versions of second-order logic, Frege’s Basic Law V is consistent. But these weak background theories only allow very weak arithmetical theories to be derived from Basic Law V (Burgess 2005). Intuitionism originates in the work of the mathematician L.E.J. Brouwer (van Atten 2004), and it is inspired by Kantian views of what objects are (Parsons 2008, chapter 1). According to intuitionism, mathematics is essentially an activity of construction. The natural numbers are mental constructions, the real numbers are mental constructions, proofs and theorems are mental constructions, mathematical meaning is a mental construction… Mathematical constructions are produced by the ideal mathematician, i.e., abstraction is made from contingent, physical limitations of the real life mathematician. But even the ideal mathematician remains a finite being. She can never complete an infinite construction, even though she can complete arbitrarily large finite initial parts of it. This entails that intuitionism resolutely rejects the existence of the actual (or completed) infinite; only potentially infinite collections are given in the activity of construction. A basic example is the successive construction in time of the individual natural numbers. From these general considerations about the nature of mathematics, based on the condition of the human mind (Moore 2001), intuitionists infer to a revisionist stance in logic and mathematics. They find non-constructive existence proofs unacceptable. Non-constructive existence proofs are proofs that purport to demonstrate the existence of a mathematical entity having a certain property without even implicitly containing a method for generating an example of such an entity. Intuitionism rejects non-constructive existence proofs as ‘theological’ and ‘metaphysical’. The characteristic feature of non-constructive existence proofs is that they make essential use of the principle of excluded third or one of its equivalents, such as the principle of double negation In classical logic, these principles are valid. The logic of intuitionistic mathematics is obtained by removing the principle of excluded third (and its equivalents) from classical logic. This of course leads to a revision of mathematical knowledge. For instance, the classical theory of elementary arithmetic, Peano Arithmetic, can no longer be accepted. Instead, an intuitionistic theory of arithmetic (called Heyting Arithmetic) is proposed which does not contain the principle of excluded third. Although intuitionistic elementary arithmetic is weaker than classical elementary arithmetic, the difference is not all that great. There exists a simple syntactical translation which translates all classical theorems of arithmetic into theorems which are intuitionistically provable. In the first decades of the twentieth century, parts of the mathematical community were sympathetic to the intuitionistic critique of classical mathematics and to the alternative that it proposed. This situation changed when it became clear that in higher mathematics, the intuitionistic alternative differs rather drastically from the classical theory. For instance, intuitionistic mathematical analysis is a fairly complicated theory, and it is very different from classical mathematical analysis. This dampened the enthusiasm of the mathematical community for the intuitionistic project. Nevertheless, followers of Brouwer have continued to develop intuitionistic mathematics onto the present day (Troelstra & van Dalen 1988). David Hilbert agreed with the intuitionists that there is a sense in which the natural numbers are basic in mathematics. But unlike the intuitionists, Hilbert did not take the natural numbers to be mental constructions. Instead, he argued that the natural numbers can be taken to be symbols. Symbols are strictly speaking abstract objects. Nonetheless, it is essential to symbols that they can be embodied by concrete objects, so we may call them quasi-concrete objects (Parsons 2008, chapter 1). Perhaps physical entities could play the role of the natural numbers. For instance, we may take a concrete ink trace of the form | to be the number 0, a concretely realized ink trace || to be the number 1, and so on. Hilbert thought it doubtful at best that higher mathematics could be directly interpreted in a similarly straightforward and perhaps even concrete manner. Unlike the intuitionists, Hilbert was not prepared to take a revisionist stance toward the existing body of mathematical knowledge. Instead, he adopted an instrumentalist stance with respect to higher mathematics. He thought that higher mathematics is no more than a formal game. The statements of higher-order mathematics are uninterpreted strings of symbols. Proving such statements is no more than a game in which symbols are manipulated according to fixed rules. The point of the ‘game of higher mathematics’ consists, in Hilbert’s view, in proving statements of elementary arithmetic, which do have a direct interpretation (Hilbert 1925). Hilbert thought that there can be no reasonable doubt about the soundness of classical Peano Arithmetic — or at least about the soundness of a subsystem of it that is called Primitive Recursive Arithmetic (Tait 1981). And he thought that every arithmetical statement that can be proved by making a detour through higher mathematics, can also be proved directly in Peano Arithmetic. In fact, he strongly suspected that every problem of elementary arithmetic can be decided from the axioms of Peano Arithmetic. Of course solving arithmetical problems in arithmetic is in some cases practically impossible. The history of mathematics has shown that making a “detour” through higher mathematics can sometimes lead to a proof of an arithmetical statement that is much shorter and that provides more insight than any purely arithmetical proof of the same statement. Hilbert realized, albeit somewhat dimly, that some of his convictions can actually be considered to be mathematical conjectures. For a proof in a formal system of higher mathematics or of elementary arithmetic is a finite combinatorial object which can, modulo coding, be considered to be a natural number. But in the 1920s the details of coding proofs as natural numbers were not yet completely understood. On the formalist view, a minimal requirement of formal systems of higher mathematics is that they are at least consistent. Otherwise every statement of elementary arithmetic can be proved in them. Hilbert also saw (again, dimly) that the consistency of a system of higher mathematics entails that this system is at least partially arithmetically sound. So Hilbert and his students set out to prove statements such as the consistency of the standard postulates of mathematical analysis. Of course such statements would have to be proved in a ‘safe’ part of mathematics, such as elementary arithmetic. Otherwise the proof does not increase our conviction in the consistency of mathematical analysis. And, fortunately, it seemed possible in principle to do this, for in the final analysis consistency statements are, again modulo coding, arithmetical statements. So, to be precise, Hilbert and his students set out to prove the consistency of, e.g., the axioms of mathematical analysis in classical Peano arithmetic. This project was known as Hilbert’s program (Zach 2006). It turned out to be more difficult than they had expected. In fact, they did not even succeed in proving the consistency of the axioms of Peano Arithmetic in Peano Arithmetic. Then Kurt Gödel proved that there exist arithmetical statements that are undecidable in Peano Arithmetic (Gödel 1931). This has become known as his Gödel’s first incompleteness theorem. This did not bode well for Hilbert’s program, but it left open the possibility that the consistency of higher mathematics is not one of these undecidable statements. Unfortunately, Gödel then quickly realized that, unless (God forbid!) Peano Arithmetic is inconsistent, the consistency of Peano Arithmetic is independent of Peano Arithmetic. This is Gödel’s second incompleteness theorem. Gödel’s incompleteness theorems turn out to be generally applicable to all sufficiently strong but consistent recursively axiomatizable theories. Together, they entail that Hilbert’s program fails. It turns out that higher mathematics cannot be interpreted in a purely instrumental way. Higher mathematics can prove arithmetical sentences, such as consistency statements, that are beyond the reach of Peano Arithmetic. All this does not spell the end of formalism. Even in the face of the incompleteness theorems, it is coherent to maintain that mathematics is the science of formal systems. One version of this view was proposed by Curry (Curry 1958). On this view, mathematics consists of a collection of formal systems which have no interpretation or subject matter. (Curry here makes an exception for metamathematics.) Relative to a formal system, one can say that a statement is true if and only if it is derivable in the system. But on a fundamental level, all mathematical systems are on a par. There can be at most pragmatical reasons for preferring one system over another. Inconsistent systems can prove all statements and therefore are pretty useless. So when a system is found to be inconsistent, it must be modified. It is simply a lesson from Gödel’s incompleteness theorems that a sufficiently strong consistent system cannot prove its own consistency. There is a canonical objection against Curry’s formalist position. Mathematicians do not in fact treat all apparently consistent formal systems as being on a par. Most of them are unwilling to admit that the preference of arithmetical systems in which the arithmetical sentence expressing the consistency of Peano Arithmetic are derivable over those in which its negation is derivable, for instance, can ultimately be explained in purely pragmatical terms. Many mathematicians want to maintain that the perceived correctness (incorrectness) of certain formal systems must ultimately be explained by the fact that they correctly (incorrectly) describe certain subject matters. Detlefsen has emphasized that the incompleteness theorems do not preclude that the consistency of parts of higher mathematics that are in practice used for solving arithmetical problems that mathematicians are interested in can be arithmetically established (Detlefsen 1986). In this sense, something can perhaps be rescued from the flames even if Hilbert’s instrumentalist stance towards all of higher mathematics is ultimately untenable. Another attempt to salvage a part of Hilbert’s program was made by Isaacson (Isaacson 1987). He defends the view that in some sense, Peano Arithmetic may be complete after all (Isaacson 1987). He argues that true sentences undecidable in Peano Arithmetic can only be proved by means of higher-order concepts. For instance, the consistency of Peano Arithmetic can be proved by induction up to a transfinite ordinal number (Gentzen 1938). But the notion of an ordinal number is a set-theoretic, and hence non-arithmetical, concept. If the only ways of proving the consistency of arithmetic make essential use of notions which arguably belong to higher-order mathematics, then the consistency of arithmetic, even though it can be expressed in the language of Peano Arithmetic, is a non-arithmetical problem. And generalizing from this, one can wonder whether Hilbert’s conjecture that every problem of arithmetic can be decided from the axioms of Peano Arithmetic might not still be true. As was mentioned earlier, predicativism is not ordinarily described as one of the schools. But it is only for contingent reasons that before the advent of the second world war predicativism did not rise to the level of prominence of the other schools. The origin of predicativism lies in the work of Russell. On a cue of Poincaré, he arrived at the following diagnosis of the Russell paradox. The argument of the Russell paradox defines the collection C of all mathematical entities that satisfy \(\neg x\in x\). The argument then proceeds by asking whether C itself meets this condition, and derives a contradiction. The Poincaré-Russell diagnosis of this argument states that this definition does not pick out a collection at all: it is impossible to define a collection S by a condition that implicitly refers to S itself. This is called the vicious circle principle. Definitions that violate the vicious circle principle are called impredicative. A sound definition of a collection only refers to entities that exist independently from the defined collection. Such definitions are called predicative. As Gödel later pointed out, a platonist would find this line of reasoning unconvincing. If mathematical collections exist independently of the act of defining, then it is not immediately clear why there could not be collections that can only be defined impredicatively (Gödel 1944). All this led Russell to develop the simple and the ramified theory of types, in which syntactical restrictions were built in that make impredicative definitions ill-formed. In simple type theory, the free variables in defining formulas range over entities to which the collection to be defined do not belong. In ramified type theory, it is required in addition that the range of the bound variables in defining formulas do not include the collection to be defined. It was pointed out in  section 2.1  that Russell’s type theory cannot be seen as a reduction of mathematics to logic. But even aside from that, it was observed early on that especially in ramified type theory it is too cumbersome to formalize ordinary mathematical arguments. When Russell turned to other areas of analytical philosophy, Hermann Weyl took up the predicativist cause (Weyl 1918). Like Poincaré, Weyl did not share Russell’s desire to reduce mathematics to logic. And right from the start he saw that it would be in practice impossible to work in a ramified type theory. Weyl developed a philosophical stance that is in a sense intermediate between intuitionism and platonism. He took the collection of natural numbers as unproblematically given. But the concept of an arbitrary subset of the natural numbers was not taken to be immediately given in mathematical intuition. Only those subsets which are determined by arithmetical (i.e., first-order) predicates are taken to be predicatively acceptable. On the one hand, it emerged that many of the standard definitions in mathematical analysis are impredicative. For instance, the minimal closure of an operation on a set is ordinarily defined as the intersection of all sets that are closed under applications of the operation. But the minimal closure itself is one of the sets that are closed under applications of the operation. Thus, the definition is impredicative. In this way, attention gradually shifted away from concern about the set-theoretical paradoxes to the role of impredicativity in mainstream mathematics. On the other hand, Weyl showed that it is often possible to bypass impredicative notions. It even emerged that most of mainstream nineteenth century mathematical analysis can be vindicated on a predicative basis (Feferman 1988). In the 1920s, History intervened. Weyl was won over to Brouwer’s more radical intuitionistic project. In the meantime, mathematicians became convinced that the highly impredicative transfinite set theory developed by Cantor and Zermelo was less acutely threatened by Russell’s paradox than previously suspected. These factors caused predicativism to lapse into a dormant state for several decades. Building on work in generalized recursion theory, Solomon Feferman extended the predicativist project in the 1960s (Feferman 2005). He realized that Weyl’s strategy could be iterated into the transfinite. Also those sets of numbers that can be defined by using quantification over the sets that Weyl regarded as predicatively justified, should be counted as predicatively acceptable, and so on. This process can be propagated along an ordinal path. This ordinal path stretches as far into the transfinite as the predicative ordinals reach, where an ordinal is predicative if it measures the length of a provable well-ordering of the natural numbers. This calibration of the strength of predicative mathematics, which is due to Feferman and (independently) Schütte, is nowadays fairly generally accepted. Feferman then investigated how much of standard mathematical analysis can be carried out within a predicativist framework. The research of Feferman and others (most notably Harvey Friedman) shows that most of twentieth century analysis is acceptable from a predicativist point of view. But it is also clear that not all of contemporary mathematics that is generally accepted by the mathematical community is acceptable from a predicativist standpoint: transfinite set theory is a case in point.