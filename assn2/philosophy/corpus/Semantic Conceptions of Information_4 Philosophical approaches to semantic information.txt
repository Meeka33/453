 What is the relation between MTC and the sort of semantic information that we have called factual? The mathematical theory of communication approaches information as a physical phenomenon. Its central question is whether and how much uninterpreted data can be encoded and transmitted efficiently by means of a given alphabet and through a given channel. MTC is not interested in the meaning, “aboutness”, relevance, reliability, usefulness or interpretation of information, but only in the level of detail and frequency in the uninterpreted data, being these symbols, signals or messages. Philosophical approaches differ from MTC in two main respects. First, they seek to give an account of information as semantic content, investigating questions like “how can something count as information? and why?”, “how can something carry information about something else?”, “how can semantic information be generated and flow?”, “how is information related to error, truth and knowledge?”, “when is information useful?”. Wittgenstein, for example, remarks that Second, philosophical theories of semantic information also seek to connect it to other relevant concepts of information and more complex forms of epistemic, mental and doxastic phenomena. For instance, Dretske [1981] and Barwise and Seligman [1997] attempt to ground information, understood as factual semantic contents, on environmental information. The approach is also known as the naturalization of information. A similar point can be made about Putnam’s twin earths argument, the externalization of semantics and teleosemantics. Philosophical analyses usually adopt a propositional orientation and an epistemic outlook, endorsing, often implicitly, the prevalence or centrality of factual information within the map outlined in Figure 1. They tend to base their analyses on cases such as “Paris is the capital of France” or “The Bodleian Library is in Oxford”. How relevant is MTC to similar researches? In the past, some research programs tried to elaborate information theories alternative to MTC, with the aim of incorporating the semantic dimension. Donald M. Mackay [1969] proposed a quantitative theory of qualitative information that has interesting connections with situation logic (see below). According to MacKay, information is linked to an increase in knowledge on the receiver’s side: “Suppose we begin by asking ourselves what we mean by information. Roughly speaking, we say that we have gained information when we know something now that we didn’t know before; when ‘what we know’ has changed.” (Mackay [1969], p. 10). Around the same years, Doede Nauta [1972] developed a semiotic-cybernetic approach. Nowadays, few philosophers follow these lines of research. The majority agrees that MTC provides a rigorous constraint to any further theorising on all the semantic and pragmatic aspects of information. The disagreement concerns the crucial issue of the strength of the constraint. At one extreme of the spectrum, any philosophical theory of semantic-factual information is supposed to be very strongly constrained, perhaps even overdetermined, by MTC, somewhat as mechanical engineering is by Newtonian physics. Weaver’s optimistic interpretation of Shannon’s work is a typical example. At the other extreme, any philosophical theory of semantic-factual information is supposed to be only weakly constrained, perhaps even completely underdetermined, by MTC, somewhat as tennis is constrained by Newtonian physics, that is in the most uninteresting, inconsequential and hence disregardable sense (see for example Sloman [1978] and Thagard [1990]). The emergence of MTC in the 1950s generated earlier philosophical enthusiasm that has gradually cooled down through the decades. Historically, philosophical theories of semantic-factual information have moved from “very strongly constrained” to “only weakly constrained”. Recently, we find positions that carefully appreciate MTC for what it can provide in terms of a robust and well-developed statistical theory of correlations between states of different systems (the sender and the receiver) according to their probabilities. This can have important consequences in mathematically-friendly contexts, such as some approaches to naturalised epistemology (Harms [1998]) or scientific explanation (Badino [2004]). Although the philosophy of semantic information has become increasingly autonomous from MTC, two important connections have remained stable between MTC and even the most recent philosophical accounts: The communication model has remained virtually unchallenged, even if nowadays theoretical accounts are more likely to consider as basic cases multiagent and distributed systems interacting in parallel, rather than individual agents related by simple, sequential channels of communication. In this respect, the philosophy of information (Floridi [2011]; Allo [2010]) is less Cartesian than “social”. IRP refers to the inverse relation between the probability of \(p\)—which may range over sentences of a given language (as in Bar-Hillel and Carnap) or events, situations or possible worlds (as in Dretske)—and the amount of semantic information carried by \(p\) (recall that Poe’s raven, as a unary source provides no information because its answers are entirely predictable). It states that information goes hand in hand with unpredictability. Popper [1935] is often credited as the first philosopher to have advocated IRP explicitly. However, systematic attempts to develop a formal calculus involving it were made only after Shannon’s breakthrough. We have seen that MTC defines information in terms of probability space distribution. Along similar lines, the probabilistic approach to semantic information defines the semantic information in \(p\) in terms of logical probability space and the inverse relation between information and the probability of \(p\). This approach was initially suggested by Bar-Hillel and Carnap [1953] (see also Bar-Hillel [1964]) and further developed by Kemeny [1953], Smokler [1966], Hintikka and Suppes [1970] and Dretske [1981]. The details are complex but the original idea is simple. The semantic content (\(\CONT\)) in \(p\) is measured as the complement of the a priori probability of \(p\): \(\CONT\) does not satisfy the two requirements of additivity and conditionalization, which are satisfied by another measure, the informativeness (\(\INF\)) of \(p\), which is calculated, following equations [9] and [10], as the reciprocal of \(P(p)\), expressed in bits, where \(P(p) = 1 - \CONT(p)\): Things are complicated by the fact that the concept of probability employed in equations [10] and [11] is subject to different interpretations. In Bar-Hillel and Carnap [1953], the probability distribution is the outcome of a logical construction of atomic statements according to a chosen formal language. This introduces a problematic reliance on a strict correspondence between observational and formal language. In Dretske, the solution is to make probability values refer to the observed states of affairs \((s)\), that is: where \(I(s)\) is Dretske’s notation to refer to the information contained in \(s\). The modal approach further modifies the probabilistic approach by defining semantic information in terms of modal space and in/consistency. The information conveyed by \(p\) becomes the set of all possible worlds, or (more cautiously) the set of all the descriptions of the relevant possible states of the universe, that are excluded by \(p\). The systemic approach, developed especially in situation logic (Barwise and Perry 1983, Israel and Perry 1990, Devlin 1991; Barwise and Seligman 1997 provide a foundation for a general theory of information flow) also defines information in terms of states space and consistency. However, it is less ontologically demanding than the modal approach, since it assumes a clearly limited domain of application. It is also compatible with Dretske’s probabilistic approach, although it does not require a probability measure on sets of states. The informational content of \(p\) is not determined a priori, through a calculus of possible states allowed by a representational language, but in terms of factual content that \(p\) carries with respect to a given situation. Information tracks possible transitions in a system’s states space under normal conditions. Both Dretske and situation theorists require some presence of information already immanent in the environment (environmental information), as nomic regularities or constraints. This “semantic externalism” can be controversial. The inferential approach defines information in terms of entailment space: information depends on valid inference relative to an information agent’s theory or epistemic state. Each of the previous extensionalist approaches can be given an intentionalist interpretation by considering the relevant space as a doxastic space, in which information is seen as a reduction in the degree of personal uncertainty, given a state of knowledge of the informee. Wittgenstein addressed this distinction in his Remarks on the Philosophy of Psychology: In using the notion of a language game, Wittgenstein seem to have in mind here the information game we have already encountered above. Insofar as they subscribe to the Inverse Relationship Principle, the extensionalist approaches outlined in the previous section can be affected by what has been defined, with a little hyperbole, as the Bar-Hillel-Carnap Paradox (Floridi [2004]). In a nutshell, we have seen that, following IRP, the less probable or possible \(p\) is the more semantic information \(p\) is assumed to be carrying. This explains why most philosophers agree that tautologies convey no information at all, for their probability or possibility is 1. But it also leads one to consider contradictions — which describe impossible states, or whose probability is 0 — as the sort of messages that contain the highest amount of semantic information. It is a slippery slope. Make a statement less and less likely and you gradually increase its informational content, but at certain point the statement “implodes” (in the quotation below, it becomes “too informative to be true”). Bar-Hillel and Carnap [1953] were among the first to make explicit this prima facie counterintuitive inequality. Note how their careful wording betrays the desire to defuse the problem: Since its formulation, BCP has been recognised as an unfortunate, yet perfectly correct and logically inevitable consequence of any quantitative theory of weakly semantic information. It is “weakly” semantic because truth values play no role in it. As a consequence, the problem has often been either ignored or tolerated (Bar-Hillel and Carnap [1953]) as the price of an otherwise valuable approach. Sometimes, however, attempts have been made to circumscribe its counterintuitive consequences. This has happen especially in Information Systems Theory (Winder [1997])—where consistency is an essential constraint that must remain satisfied for a database to preserve data integrity—and in Decision Theory, where inconsistent information is obviously of no use to a decision maker. In these cases, whenever there are no possible models that satisfy a statement or a theory, instead of assigning to it the maximum quantity of semantic information, three strategies have been suggested: The latter approach is close to the strongly semantic approach, to which we shall now turn. The general hypothesis is that BCP indicates that something has gone essentially amiss with the theory of weakly semantic information. It is based on a semantic principle that is too weak, namely that truth-values are independent of semantic information. A semantically stronger approach, according to which information encapsulates truth, can avoid the paradox and is more in line with the ordinary conception of what generally counts as factual information, as we have seen in section 3.2.3. MTC already provides some initial reassurance. MTC identifies the quantity of information associated with, or generated by, the occurrence of a signal (an event or the realisation of a state of affairs) with the elimination of possibilities (reduction in uncertainty) represented by that signal (event or state of affairs). In MTC, no counterintuitive inequality comparable to BCP occurs, and the line of argument is that, as in the case of MTC, a theory of strongly semantic information, based on alethic and discrepancy values rather than probabilities, can also successfully avoid BCP (Floridi [2005]; see Bremer and Cohnitz [2004] chap. 2 for an overview; Sequoiah-Grayson [2007] defends the theory of strongly semantic information against recent independent objections from Fetzer [2004] and Dodig-Crnkovic [2005]). Before describing this approach, note that some have proposed a different alethic approach, one that uses truthlikeness, or verisimilitude, to explicate the notion of semantic information (Frické 1997; Cevolani 2011, 2014; D’Alfonso 2011). Typically these seek to identify factual information with likeness to the complete truth about all empirical matters or about some restricted relevant domain of factual interest. These also avoid the BCP, and also do not use probabilities. However, truthlikeness is different from truth itself in as much as a truth bearer can be like the truth without actually being true, i.e. while being false, so that verisimilitude accounts of information can permit false views or theories to possess information. (Indeed false statements can sometimes carry more information than than their true negations on this account; Frické 1997). By contrast, on Floridi’s conception semantic-factual information is defined, in terms of data space, as well-formed, meaningful and truthful data. This constrains the probabilistic approach introduced above, by requiring first a qualification of the content as truthful. Once the content is so qualified, the quantity of semantic information in \(p\) is calculated in terms of distance of \(p\) from the situation/resource \(w\) that \(p\) is supposed to model. Total distance is equivalent to a \(p\) true in all cases (all possible worlds or probability 1), including \(w\) and hence minimally informative, whereas maximum closeness is equivalent to the precise modelling of \(w\) at the agreed level of abstraction. Suppose there will be exactly three guests for dinner tonight. This is our situation \(w\). Imagine we are told that The degree of informativeness of (T) is zero because, as a tautology, (T) applies both to \(w\) and to \(\neg w\). (V) performs better, and (P) has the maximum degree of informativeness because, as a fully accurate, precise and contingent truth, it “zeros in” on its target \(w\). Generalising, the more distant some semantic-factual information \(\sigma\) is from its target \(w\), the larger is the number of situations to which it applies, the lower its degree of informativeness becomes. A tautology is a true \(\sigma\) that is most “distant” from the world. Let us now use ‘\(\theta\)’ to refer to the distance between a true \(\sigma\) and \(w\). Using the more precise vocabulary of situation logic, \(\theta\) indicates the degree of support offered by \(w\) to \(\sigma\). We can now map on the \(x\)-axis of a Cartesian diagram the values of \(\theta\) given a specific \(\sigma\) and a corresponding target \(w\). In our example, we know that \(\theta(\T) = 1\) and \(\theta(\P) = 0\). For the sake of simplicity, let us assume that \(\theta(\V) = 0.25\). We now need a formula to calculate the degree of informativeness \(\iota\) of \(\sigma\) in relation to \(\theta(s)\). Floridi (2004, 210–11) mathematically derives and motivates the use of the complement of the square value of \(\theta(\sigma)\), that is, [13]: Figure 5 shows the graph generated by equation [13] when we include also negative values of distance for false \(\sigma\); \(\theta\) ranges from \(-1 (=\) contradiction) to \(1 (=\) tautology): Figure 5. Degree of informativeness If \(\sigma\) has a very high degree of informativeness \(\iota\) (very low \(\theta)\) we want to be able to say that it contains a large quantity of semantic information and, vice versa, the lower the degree of informativeness of \(\sigma\) is, the smaller the quantity of semantic information conveyed by \(\sigma\) should be. To calculate the quantity of semantic information contained in \(\sigma\) relative to \(\iota(\sigma)\) we need to calculate the area delimited by equation [13], that is, the definite integral of the function \(\iota(\sigma)\) on the interval \([0, 1]\). As we know, the maximum quantity of semantic information (call it \(\alpha)\) is carried by (P), whose \(\theta = 0\). This is equivalent to the whole area delimited by the curve. Generalising to \(\sigma\) we have: Figure 6 shows the graph generated by equation [14]. The shaded area is the maximum amount of semantic information \(\alpha\) carried by \(\sigma\): Figure 6. Maximum amount of semantic information \(\alpha\) carried by \(\sigma\) Consider now (V), “there will be some guests tonight”. (V) can be analysed as a (reasonably finite) string of disjunctions, that is (V) = [“there will be one guest tonight” or “there will be two guests tonight” or … “there will be \(n\) guests tonight”], where \(n\) is the reasonable limit we wish to consider (things are more complex than this, but here we only need to grasp the general principle). Only one of the descriptions in (V) will be fully accurate. This means that (V) also contains some (perhaps much) information that is simply irrelevant or redundant. We shall refer to this “informational waste” in (V) as vacuous information in (V). The amount of vacuous information (call it \(\beta)\) in (V) is also a function of the distance \(\theta\) of (V) from \(w\), or more generally: Since \(\theta(\V) = 0.25\), we have Figure 7 shows the graph generated by equation [16]: Figure 7. Amount of semantic information \(\gamma\) carried by \(\sigma\) The shaded area is the amount of vacuous information \(\beta\) in (V). Clearly, the amount of semantic information in (V) is simply the difference between \(\alpha\) (the maximum amount of information that can be carried in principle by \(\sigma)\) and \(\beta\) (the amount of vacuous information actually carried by \(\sigma)\), that is, the clear area in the graph of Figure 7. So, the amount of semantic information \(\gamma\) in \(\sigma\) is: Note the similarity between [14] and [15]. When \(\theta(\sigma) = 1\), that is, when the distance between \(\sigma\) and \(w\) is maximum, then \(\alpha = \beta\) and \(\gamma(\sigma) = 0\). This is what happens when we consider (T). (T) is so distant from \(w\) as to contain only vacuous information. In other words, (T) contains as much vacuous information as (P) contains relevant information.