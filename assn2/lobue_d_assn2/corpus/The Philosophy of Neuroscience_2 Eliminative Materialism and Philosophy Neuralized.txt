 Eliminative materialism (EM), in the form advocated most aggressively by Paul and Patricia Churchland, is the conjunction of two claims. First, our common sense “belief-desire” conception of mental events and processes, our “folk psychology”, is a false and misleading account of the causes of human behavior. Second, like other false conceptual frameworks from both folk theory and the history of science, it will be replaced by, rather than smoothly reduced or incorporated into, a future neuroscience. The Churchlands’ characterized folk psychology as the collection of common homilies invoked (mostly implicitly) to explain human behavior causally. You ask why Marica is not accompanying me this evening. I reply that our grandson needed sitting. You nod sympathetically. You understand my explanation because you share with me a generalization that relates beliefs about taking care of grandchildren, desires to help daughters and to spend time with grandchildren compared to enjoying a night out, and so on. This is just one of a huge collection of homilies about the causes of human behavior that EM claims to be flawed beyond potential revision. Although this example involves only beliefs and desires, folk psychology contains an extensive repertoire of propositional attitudes in its explanatory nexus: hopes, intentions, fears, imaginings, and more. EMists predict that a future, genuinely scientific psychology or neuroscience will eventually eschew all of these, and replace them with incommensurable states and dynamics of neuro-cognition. EM is physicalist in one traditional philosophical sense. It postulates that some future brain science will be ultimately the correct account of (human) behavior. It is eliminative in predicting the future rejection of folk psychological kinds from our post-neuroscientific ontology. EM proponents often employ scientific analogies (Feyerabend 1963; Paul Churchland, 1981). Oxidative reactions as characterized within elemental chemistry bear no resemblance to phlogiston release. Even the “direction” of the two processes differ. Oxygen is gained when an object burns (or rusts), phlogiston was said to be lost. The result of this theoretical change was the elimination of phlogiston from our scientific ontology. There is no such thing. For the same reasons, according to EM, continuing development in neuroscience will reveal that there are no such things as beliefs, desires, and the rest of the propositional attitudes as characterized by common sense. Here we focus only on the way that neuroscientific results have shaped the arguments for EM. Surprisingly, only one argument has been strongly influenced. (Most arguments for EM stress failures of folk psychology as an explanatory theory of behavior.) This argument is based on a development in cognitive and computational neuroscience that might provide a genuine alternative to the representations and computations implicit in folk psychological generalizations. Many eliminative materialists assume that folk psychology is committed to propositional representations and computations over their contents that mimic logical inferences (Paul Churchland 1981; Stich 1983; Patricia Churchland  1986).[2]  Even though discovering an alternative to this view has been an eliminativist goal for some time, some eliminativists hold that neuroscience only began delivering this alternative over the past thirty years. Points in and trajectories through vector spaces, as an interpretation of synaptic events and neural activity patterns in biological and artificial neural networks are the key features of this alternative. The differences between these notions of cognitive representation and transformations, and those of the propositional attitudes of folk psychology, provide the basis for one argument for EM (Paul Churchland 1987). However, this argument will be opaque to those with no background in cognitive and computational neuroscience, so we present a few details. With these details in place, we will return to this argument for EM (five paragraphs below). At one level of analysis, the basic computational element of a neural network, biological or artificial, is the nerve cell, or neuron. Mathematically, neurons can be represented as simple computational devices, transforming inputs into output. Both inputs and outputs reflect biological variables. For our discussion, we assume that neuronal inputs are frequencies of action potentials (neuronal “spikes”) in the axons whose terminal branches synapse onto the neuron in question, while neuronal output is the frequency of action potentials generated in its axon after processing the inputs. A neuron thereby computes its total input, usually treated mathematically as the sum of the products of the signal strength along each input line times the synaptic weight on that line. It then computes a new activation state based on its total input and current activation state, and a new output state based on its new activation value. The neuron’s output state is transmitted as a signal strength to whatever neurons its axon synapses on. The output state reflects systematically the neuron’s new activation  state.[3] Analyzed in this fashion, both biological and artificial neural networks are interpreted naturally as vector-to-vector transformers. The input vector consists of values reflecting activity patterns in axons synapsing on the network’s neurons from outside (e.g., from sensory transducers or other neural networks). The output vector consists of values reflecting the activity patterns generated in the network’s neurons that project beyond the net (e.g., to motor effectors or other neural networks). Given that each neuron’s activity depends partly upon their total input, and its total input depends partly on synaptic weights (e.g., presynaptic neurotransmitter release rate, number and efficacy of postsynaptic receptors, availability of enzymes in synaptic cleft), the capacity of biological networks to change their synaptic weights make them plastic vector-to-vector transformers. In principle, a biological network with plastic synapses can come to implement any vector-to-vector transformation that its composition permits (number of input units, output units, processing layers, recurrency, cross-connections, etc.) (discussed in Paul Churchland, 1987, with references to the primary scientific literature). Figure 1. The anatomical organization of the cerebellum provides a clear example of a network amenable to this computational interpretation. Consider  Figure 1.  The cerebellum is the bulbous convoluted structure dorsal to the brainstem. A variety of studies (behavioral, neuropsychological, single-cell electrophysiological) implicate this structure in motor integration and fine motor coordination. Mossy fibers (axons) from neurons outside the cerebellum synapse on cerebellar granule cells, which in turn project to parallel fibers. Activity patterns across the collection of mossy fibers (frequency of action potentials per time unit in each fiber projecting into the cerebellum) provide values for the input vector. Parallel fibers make multiple synapses on the dendritic trees and cell bodies of cerebellular Purkinje neurons. Each Purkinje neuron “sums” its post-synaptic potentials (PSPs) and emits a train of action potentials down its axon based (partly) on its total input and previous activation state. Purkinje axons project outside the cerebellum. The network’s output vector is thus the ordered values representing the pattern of activity generated in each Purkinje axon. Changes to the efficacy of individual synapses on the parallel fibers and the Purkinje neurons alter the resulting PSPs in Purkinje axons, generating different axonal spiking frequencies. Computationally, this amounts to a different output vector to the same input activity  pattern—plasticity.[4] Figure 2. This interpretation puts the useful mathematical resources of dynamical systems into the hands of computational neuroscientists. Vector spaces are an example. Learning can then be characterized fruitfully in terms of changes in synaptic weights in the network and subsequent reduction of error in network output. (This approach to learning goes back to Hebb 1949, although the vector-space interpretation was not part of Hebb’s account.) A useful representation of this account uses a synaptic weight-error space. One dimension represents the global error in the network’s output to a given task, and all other dimensions represent the weight values of individual synapses in the network. Consider  Figure 2.  Points in this multi-dimensional state space represent the global performance error correlated with each possible collection of synaptic weights in the network. As the weights change with each performance, in accordance with a biologically-inspired learning algorithm, the global error of network performance continually decreases. The changing synaptic weights across the network with each training episode reduces the total error of the network’s output vector, compared to the desired output vector for the input vector. Learning is represented as synaptic weight changes correlated with a descent along the error dimension in the space (Churchland and Sejnowski 1992). Representations (concepts) can be portrayed as partitions in multi-dimensional vector spaces. One example is a neuron activation vector space. See  Figure 3.  A graph of such a space contains one dimension for the activation value of each neuron in the network (or some specific subset of the network’s neurons, such as those in a specific layer). A point in this space represents one possible pattern of activity in all neurons in the network. Activity patterns generated by input vectors that the network has learned to group together will cluster around a (hyper-) point or subvolume in the activity vector space. Any input pattern sufficiently similar to this group will produce an activity pattern lying in geometrical proximity to this point or subvolume. Paul Churchland (1989) argued that this interpretation of network activity provided a quantitative, neurally-inspired basis for prototype theories of concepts developed in late-twentieth century cognitive psychology. Figure 3. Using this theoretical development, and in the realm of neurophilosophy, Paul Churchland (1987, 1989) offered a novel, neuroscientifically-inspired argument for EM. According to the interpretation of neural networks just sketched, activity vectors are the central kind of representations, and vector-to-vector transformations are the central kind of computations, in the brain. This contrasts sharply with the propositional representations and logical/semantic computations postulated by folk psychology. Vectorial content, an ordered sequence of real numbers, is unfamiliar and alien to common sense. This cross-theoretic conceptual difference is at least as great as that between oxidative and phlogiston concepts, or kinetic-corpuscular and caloric fluid heat concepts. Phlogiston and caloric fluid are two “parade” examples of kinds eliminated from our scientific ontology due to the nature of the intertheoretic relation obtaining between the theories with which they are affiliated and the theories that replaced them. The structural and dynamic differences between the folk psychological and then-emerging cognitive neuroscientific kinds suggested that the theories affiliated with the latter will likewise replace the theory affiliated with the former. But this claim was the key premise of the eliminativist argument based on predicted intertheoretic relations. And with the rise of neural networks and parallel distributed processing, intertheoretic contrasts with folk-psychological explanatory kinds were no longer just an eliminativist’s future hope. Computational and cognitive neuroscience was delivering an alternative kinematics for cognition, one that provided no structural analogue for folk psychology’s propositional attitudes or logic-like computations over propositional contents. Certainly the vector-space alternatives of this interpretation of neural networks are alien to folk psychology. But do they justify EM? Even if the propositional contents of folk-psychological posits find no analogues in one theoretical development in cognitive and computational neuroscience (that was hot three decades ago), there might be other aspects of cognition that folk psychology gets right. Within the scientific realism that informed early neurophilosophy, concluding that a cross-theoretic identity claim is true (e.g., folk psychological state F is identical to neural state N) or that an eliminativist claim is true (there is no such thing as folk psychological state F) depended on the nature of the intertheoretic reduction obtaining between the theories affiliated with the posits in question (Hooker 1981a,b,c; Churchland 1986; Bickle, 1998). But the underlying account of intertheoretic reduction also recognized a spectrum of possible reductions, ranging from relatively “smooth” through “significantly revisionary” to “extremely  bumpy”.[5]  Might the reduction of folk psychology to a “vectorial” computational neuroscience occupy some middle ground between “smooth” and “bumpy” intertheoretic reduction endpoints, and hence suggest a “revisionary” conclusion? The reduction of classical equilibrium thermodynamics-to-statistical mechanics provided a potential analogy here. John Bickle (1992, 1998, chapter 6) argued on empirical grounds that such an outcome is likely. He specified conditions on “revisionary” reductions from historical examples and suggested that these conditions are obtaining between folk psychology and cognitive neuroscience as the latter develops. In particular, folk psychology appears to have gotten right the grossly-specified functional profile of many cognitive states, especially those closely related to sensory inputs and behavioral outputs. It also appears to get right the “intentionality” of many cognitive states—the object that the state is of or about—even though cognitive neuroscience eschews its implicit linguistic explanation of this feature. Revisionary physicalism predicts significant conceptual change to folk psychological concepts, but denies total elimination of the caloric fluid-phlogiston variety. The philosophy of science is another area where vector space interpretations of neural network activity patterns has impacted philosophy. In the Introduction to his (1989) book, A Neurocomputational Perspective, Paul Churchland asserted, distinctively neurophilosophically, that it will soon be impossible to do serious work in the philosophy of science without drawing on empirical work in the brain and behavioral sciences. To justify this claim, in Part II of the book he suggested neurocomputational reformulations of key concepts from the philosophy of science. At the heart of his reformulations is a neurocomputational account of the structure of scientific theories (1989: chapter 9). Problems with the orthodox “sets-of-sentences” view of scientific theories have been well-known since the 1960s. Churchland advocated replacing the orthodox view with one inspired by the “vectorial” interpretation of neural network activity. Representations implemented in neural networks (as sketched above) compose a system that corresponds to important distinctions in the external environment, are not explicitly represented as such within the input corpus, and allow the trained network to respond to inputs in a fashion that continually reduces error. According to Churchland, these are functions of theories. Churchland was bold in his assertion: an individual’s theory-of-the-world is a specific point in that individual’s error-synaptic weight vector space. It is a configuration of synaptic weights that partitions the individual’s activation vector space into subdivisions that reduce future error messages to both familiar and novel inputs. (Consider again  Figure 2  and  Figure 3.)  This reformulation invites an objection, however. Churchland boasts that his theory of theories is preferable to existing alternatives to the orthodox “sets-of-sentences” account—for example, the semantic view (Suppe 1974; van Fraassen 1980)—because his is closer to the “buzzing brains” that use theories. But as Bickle (1993) noted, neurocomputational models based on the mathematical resources described above are a long way into the realm of mathematical abstraction. They are little more than novel (albeit suggestive) application of the mathematics of quasi-linear dynamical systems to simplified schemata of brain circuitries. Neurophilosophers owe some account of identifications across ontological categories (vector representations and transformation to what?) before the philosophy of science community will treat theories as points in high-dimensional state spaces implemented in biological neural networks. (There is an important methodological assumption lurking in Bickle’s objection, however, which we will discuss toward the end of the next paragraph.) Churchland’s neurocomputational reformulations of other scientific and epistemological concepts build on this account of theories. He sketches “neuralized” accounts of the theory-ladenness of perception, the nature of concept unification, the virtues of theoretical simplicity, the nature of Kuhnian paradigms, the kinematics of conceptual change, the character of abduction, the nature of explanation, and even moral knowledge and epistemological normativity. Conceptual redeployment, for example, is the activation of an already-existing prototype representation—the centerpoint or region of a partition of a high-dimensional vector space in a trained neural network—by a novel type of input pattern. Obviously, we can’t here do justice to Churchland’s many and varied attempts at reformulation. We urge the intrigued reader to examine his suggestions in their original form. But a word about philosophical methodology is in order. Churchland is not attempting “conceptual analysis” in anything resembling its traditional philosophical sense. Neither, typically, are neurophilosophers in any of their reformulation projects. (This is why a discussion of neurophilosophical reformulations fits with a discussion of EM.) There are philosophers who take the discipline’s ideal analyses to be a relatively simple set of necessary and sufficient conditions, expressed in non-technical natural language, governing the application of important concepts (like justice, knowledge, theory, or explanation). These analyses should square, to the extent possible, with pretheoretical usage. Ideally, they should preserve synonymy. Other philosophers view this ideal as sterile, misguided, and perhaps deeply mistaken about the underlying structure of human knowledge (Ramsey 1992). Neurophilosophers tend to reside in the latter group. Those who dislike philosophical speculation about the promise and potential of developing science to reformulate (“reform-ulate”) traditional philosophical concepts have probably already discovered that neurophilosophy is not for them. But the familiar charge that neurocomputational reformulations of the sort Churchland attempts are “philosophically uninteresting” or “irrelevant” because they fail to provide “analyses” of theory, explanation, and the like will fall on deaf ears among many contemporary “naturalistic” philosophers, who have by and large given up on traditional philosophical “analysis”. Before we leave the topic of proposed neurophilosophical applications of this theoretical development from “neural networks”-style cognitive/computational neuroscience, one final point of actual scientific detail bears mention. This approach did not remain state-of-the-art computational neuroscience for long. Many neural modelers quickly gave up this approach to modeling the brain. Compartmental modeling enabled computational neuroscientists to mimic activity in and interactions between patches of neuronal membrane (Bower and Beeman 1995). This approach permitted modelers to control and manipulate a variety of subcellular factors that determine action potentials per time unit, including the topology of membrane structure in individual neurons, variations in ion channels across membrane patches, and field properties of post-synaptic potentials depending on the location of the synapse on the dendrite or soma. By the mid-1990s modelers quickly began to “custom build” the neurons in their target circuitry. Increasingly powerful computer hardware still allowed them to study circuit properties of modeled networks. For these reasons, many serious computational neuroscientists switched to working at a level of analysis that treats neurons as structured rather than simple computational devices. With compartmental modeling, vector-to-vector transformations came to be far less useful in serious neurobiological models, replaced by differential equations representing ion currents across patches of neural membrane. Far more biological detail came to be captured in the resulting models than “connectionist” models permitted. This methodological change across computational neuroscience meant that a neurophilosophy guided by “connectionist” resources no longer drew from the state of the art of the scientific field. Philosophy of science and scientific epistemology were not the only areas where neurophilosophers urged the relevance of neuroscientific discoveries for traditionally philosophical topics. A decade after Neurophilosophy’s publication, Kathleen Akins (1996) argued that a “traditional” view of the senses underlies a variety of sophisticated “naturalistic” programs about intentionality. (She cites the Churchlands, Daniel Dennett, Fred Dretske, Jerry Fodor, David Papineau, Dennis Stampe, and Kim Sterelny as examples.) But then-recent neuroscientific work on the mechanisms and coding strategies implemented by sensory receptors shows that this traditional view is mistaken. The traditional view holds that sensory systems are “veridical” in at least three ways. (1) Each signal in the system correlates with a small range of properties in the external (to the body) environment. (2) The structure in the relevant external relations that the receptors are sensitive to is preserved in the structure of the internal relations among the resulting sensory states. And (3) the sensory system reconstructs faithfully, without fictive additions or embellishments, the external events. Using then-recent neurobiological discoveries about response properties of thermal receptors in the skin (i.e., “thermoreceptors”) as an illustration, Akins showed that sensory systems are “narcissistic” rather than “veridical”. All three traditional assumptions are violated. These neurobiological details and their philosophical implications open novel questions for the philosophy of perception and for the appropriate foundations for naturalistic projects about intentionality. Armed with the known neurophysiology of sensory receptors, our “philosophy of perception” or account of “perceptual intentionality” will no longer focus on the search for correlations between states of sensory systems and “veridically detected” external properties. This traditional philosophical (and scientific!) project rests upon a mistaken “veridicality” view of the senses. Neuroscientific knowledge of sensory receptor activity also shows that sensory experience does not serve the naturalist well as a “simple paradigm case” of an intentional relation between representation and world. Once again, available scientific detail showed the naivety of some traditional philosophical projects. Focusing on the anatomy and physiology of the pain transmission system, Valerie Hardcastle (1997) urged a similar negative implication for a popular methodological assumption. Pain experiences have long been philosophers’ favorite cases for analysis and theorizing about conscious experiences generally. Nevertheless, every position about pain experiences has been defended: eliminativism, a variety of objectivist views, relational views, and subjectivist views. Why so little agreement, despite agreement that pain experiences are the place to start an analysis or theory of consciousness? Hardcastle urged two answers. First, philosophers tend to be uninformed about the neuronal complexity of our pain transmission systems, and build their analyses or theories on the outcome of a single component of a multi-component system. Second, even those who understand some of the underlying neurobiology of pain tend to advocate gate-control  theories.[6]  But the best existing gate-control theories are vague about the neural mechanisms of the gates. Hardcastle instead proposed a dissociable dual system of pain transmission, consisting of a pain sensory system closely analogous in its neurobiological implementation to other sensory systems, and a descending pain inhibitory system. She argued that this dual system is consistent with neuroscientific discoveries and accounts for all the pain phenomena that have tempted philosophers toward particular (but limited) theories of pain experience. The neurobiological uniqueness of the pain inhibitory system, contrasted with the mechanisms of other sensory modalities, renders pain processing atypical. In particular, the pain inhibitory system dissociates pain sensation from stimulation of nociceptors (pain receptors). Hardcastle concluded from the neurobiological uniqueness of pain transmission that pain experiences are atypical conscious events, and hence not a good place to start theorizing about or analyzing the general type.