 This section addresses three distinct issues concerning simplicity and its relation to other methodological issues. These issues concern quantitative parsimony, plenitude, and induction. Theorists tend to be frugal in their postulation of new entities. When a trace is observed in a cloud-chamber, physicists may seek to explain it in terms of the influence of a hitherto unobserved particle. But, if possible, they will postulate one such unobserved particle, not two, or twenty, or 207 of them. This desire to minimize the number of individual new entities postulated is often referred to as quantitative parsimony. David Lewis articulates the attitude of many philosophers when he writes: Is the initial assumption that one particle is acting to cause the observed trace more rational than the assumption that 207 particles are so acting? Or is it merely the product of wishful thinking, aesthetic bias, or some other non-rational influence? Nolan (1997) examines these questions in the context of the discovery of the  neutrino.[21]  Physicists in the 1930's were puzzled by certain anomalies arising from experiments in which radioactive atoms emit electrons during so-called Beta decay. In these experiments the total spin of the particles in the system before decay exceeds by 1/2 the total spin of the (observed) emitted particles. Physicists' response was to posit a ‘new’ fundamental particle, the neutrino, with spin 1/2 and to hypothesize that exactly one neutrino is emitted by each electron during Beta decay. Note that there is a wide range of very similar neutrino theories which can also account for the missing spin. H1: 1 neutrino with a spin of 1/2 is emitted in each case of Beta decay. H2: 2 neutrinos, each with a spin of 1/4 are emitted in each case of Beta decay. and, more generally, for any positive integer n, Each of these hypotheses adequately explains the observation of a missing 1/2-spin following Beta decay. Yet the most quantitatively parsimonious hypothesis, H1, is the obvious default  choice.[22] One promising approach is to focus on the relative explanatory power of the alternative hypotheses, H1, H2, … Hn. When neutrinos were first postulated in the 1930's, numerous experimental set-ups were being devised to explore the products of various kinds of particle decay. In none of these experiments had cases of ‘missing’ 1/3-spin, or 1/4-spin, or 1/100-spin been found. The absence of these smaller fractional spins was a phenomenon which competing neutrino hypotheses might potentially help to explain. Consider the following two competing neutrino hypotheses: H1: 1 neutrino with a spin of 1/2 is emitted in each case of Beta decay. H10: 10 neutrinos, each with a spin of 1/20, are emitted in each case of Beta decay. Why has no experimental set-up yielded a ‘missing’ spin-value of 1/20? H1 allows a better answer to this question than H10 does, for H1 is consistent with a simple and parsimonious explanation, namely that there exist no particles with spin 1/20 (or less). In the case of H10, this potential explanation is ruled out because H10 explicitly postulates particles with spin 1/20. Of course, H10 is consistent with other hypotheses which explain the non-occurrence of missing 1/20-spin. For example, one might conjoin to H10 the law that neutrinos are always emitted in groups of ten. However, this would make the overall explanation less syntactically simple, and hence less virtuous in other respects. In this case, quantitative parsimony brings greater explanatory power. Less quantitatively parsimonious hypotheses can match this power only by adding auxiliary claims which decrease their syntactic simplicity. Thus the preference for quantitatively parsimonious hypotheses emerges as one facet of a more general preference for hypotheses with greater explanatory power. One distinctive feature of the neutrino example is that it is ‘additive.’ It involves postulating the existence of a collection of qualitatively identical objects which collectively explain the observed phenomenon. The explanation is additive in the sense that the overall phenomenon is explained by summing the individual positive contributions of each  object.[23]  Whether the above approach can be extended to non-additive cases involving quantitative parsimony is an interesting question. Jansson and Tallant (forthcoming) argue that it can, and they offer a probabilistic analysis that aims to bring together a variety of different cases where quantitative parsimony plays a role in hypothesis selection. Consider a case in which the aberrations of a planet's orbit can be explained by postulating a single unobserved planet, or it can be explained by postulating two or more unobserved planets. In order for the latter situation to be actual, the multiple planets must orbit in certain restricted ways so as to match the effects of a single planet. Prima facie this is unlikely, and this counts against the less quantitatively parsimonious hypothesis. Ranged against the principles of parsimony discussed in previous sections is an equally firmly rooted (though less well-known) tradition of what might be termed “principles of explanatory  sufficiency.”[24]  These principles have their origins in the same medieval controversies that spawned Occam's Razor. Ockham's contemporary, Walter of Chatton, proposed the following counter-principle to Occam's Razor: A related counter-principle was later defended by Kant: There is no inconsistency in the coexistence of these two families of principles, for they are not in direct conflict with each other. Considerations of parsimony and of explanatory sufficiency function as mutual counter-balances, penalizing theories which stray into explanatory inadequacy or ontological  excess.[25]  What we see here is an historical echo of the contemporary debate among statisticians concerning the proper trade-off between simplicity and goodness of fit. There is, however, a second family of principles which do appear directly to conflict with Occam's Razor. These are so-called ‘principles of plenitude.’ Perhaps the best-known version is associated with Leibniz, according to whom God created the best of all possible worlds with the greatest number of possible entities. More generally, a principle of plenitude claims that if it is possible for an object to exist then that object actually exists. Principles of plenitude conflict with Occam's Razor over the existence of physically possible but explanatorily idle objects. Our best current theories presumably do not rule out the existence of unicorns, but nor do they provide any support for their existence. According to Occam's Razor we ought not to postulate the existence of unicorns. According to a principle of plenitude we ought to postulate their existence. The rise of particle physics and quantum mechanics in the 20th Century led to various principles of plenitude being appealed to by scientists as an integral part of their theoretical framework. A particularly clear-cut example of such an appeal is the case of magnetic  monopoles.[26]  The 19th-century theory of electromagnetism postulated numerous analogies between electric charge and magnetic charge. One theoretical difference is that magnetic charges must always come in oppositely-charged pairs, called “dipoles” (as in the North and South poles of a bar magnet), whereas single electric charges, or “monopoles,” can exist in isolation. However, no actual magnetic monopole had ever been observed. Physicists began to wonder whether there was some theoretical reason why monopoles could not exist. It was initially thought that the newly developed theory of quantum mechanics ruled out the possibility of magnetic monopoles, and this is why none had ever been detected. However, in 1931 the physicist Paul Dirac showed that the existence of monopoles is consistent with quantum mechanics, although it is not required by it. Dirac went on to assert the existence of monopoles, arguing that their existence is not ruled out by theory and that “under these circumstances one would be surprised if Nature had made no use of it” (Dirac 1930, p. 71, note 5). This appeal to plenitude was widely—though not universally—accepted by other physicists. Others have been less impressed by Dirac's line of argument: It is difficult to know how to interpret these principles of plenitude. Quantum mechanics diverges from classical physics by replacing of a deterministic model of the universe with a model based on objective probabilities. According to this probabilistic model, there are numerous ways the universe could have evolved from its initial state, each with a certain probability of occurring that is fixed by the laws of nature. Consider some kind of object, say unicorns, whose existence is not ruled out by the initial conditions plus the laws of nature. Then one can distinguish between a weak and a strong version of the principle of plenitude. According to the weak principle, if there is a small finite probability of unicorns existing then given enough time and space unicorns will exist. According to the strong principle, it follows from the theory of quantum mechanics that if it is possible for unicorns to exist then they do exist. One way in which this latter principle may be cashed out is in the ‘many-worlds’ interpretation of quantum mechanics, according to which reality has a branching structure in which every possible outcome is realized. The problem of induction is closely linked to the issue of simplicity. One obvious link is between the curve-fitting problem and the inductive problem of predicting future outcomes from observed data. Less obviously, Schulte (1999) argues for a connection between induction and ontological parsimony. Schulte frames the problem of induction in information-theoretic terms: given a data-stream of observations of non-unicorns (for example), what general conclusion should be drawn? He argues for two constraints on potential rules. First, the rule should converge on the truth in the long run (so if no unicorns exist then it should yield this conclusion). Second, the rule should minimize the maximum number of changes of hypothesis, given different possible future observations. Schulte argues that the ‘Occam Rule’—conjecture that Ω does not exist until it has been detected in an experiment—is optimal relative to these constraints. An alternative rule—for example, conjecturing that Ω exists until 1 million negative results have been obtained—may result in two changes of hypothesis if, say, Ω's are not detected until the 2 millionth experiment. The Occam Rule leads to at most one change of hypothesis (when an Ω is first detected). (See also Kelly 2004, 2007.) Schulte (2008) applies this approach to the problem of discovering conservation laws in particle physics. The analysis has been criticized by Fitzpatrick (2013), who raises doubts about why long-run convergence to the truth should matter when it comes to predicting the outcome of the very next experiment. With respect to the justification question, arguments have been made in both directions. Scientists are often inclined to justify simplicity principles on broadly inductive grounds. According to this argument, scientists select new hypotheses based partly on criteria that have been generated inductively from previous cases of theory choice. Choosing the most parsimonious of the acceptable alternative hypotheses has tended to work in the past. Hence scientists continue to use this as a rule of thumb, and are justified in so doing on inductive grounds. One might try to bolster this point of view by considering a counterfactual world in which all the fundamental constituents of the universe exist in pairs. In such a ‘pairwise’ world, scientists might well prefer pairwise hypotheses in general to their more parsimonious rivals. This line of argument has a couple of significant weaknesses. Firstly, one might legitimately wonder just how successful the choice of parsimonious hypotheses has been; examples from chemistry spring to mind, such as oxygen molecules containing two atoms rather than one. Secondly, and more importantly, there remains the issue of explaining why the preference for parsimonious hypotheses in science has been as successful as it has been. Making the justificatory argument in the reverse direction, from simplicity to induction, has a strong historical precedent in philosophical approaches to the problem of induction, from Hume onwards. Justifying the ‘straight rule’ of induction by appeal to some general Principle of Uniformity is an initially appealing response to the skeptical challenge. However, in the absence of a defense of the underlying Principle itself (and one which does not, on pain of circularity, depend inductively on past success), it is unclear how much progress this represents. There have also been attempts (see e.g. Steel 2009) to use simplicity considerations to respond to Nelson Goodman's ‘new riddle of induction.’