 ST is only one of several accounts of mindreading on the market. A rough-and-ready list of the alternatives should at least include: the Intentional Stance Theory (Dennett 1987; Gergely & Csibra 2003; Gergely et al. 1995); Interactionism (Gallagher 2001; Gallagher & Hutto 2008; De Jaegher at al. 2010); and the Theory-Theory (Gopnik & Wellman 1992; Gopnik & Meltzoff 1997; Leslie 1994; Scholl & Leslie 1999). In this entry, we will discuss the Theory-Theory (TT) only, given that the TT-ST controversy has constituted the focal point of the debate on mindreading during the last 30 years or so. As suggested by its name, the Theory-Theory proposes that mindreading is grounded by the possession of a Theory of Mind (“a folk psychology”)—i.e., it is based on the tacit knowledge of the following body of information: a number of “folk” laws or principles connecting mental states with sensory stimuli, behavioural responses, and other mental states. Here are a couple of putative examples: Law of sight: If S is in front of object O, S directs her eye-gaze to O, S’s visual system is properly functioning, and the environmental conditions are optimal, then ceteris paribus S will see O. Law of the practical syllogism: If S desires a certain outcome G and S believes that by performing a certain action A she will obtain G, then ceteris paribus S will decide to perform A. The main divide among Theory-Theorists concerns how the Theory of Mind is acquired—i.e., it concerns where this body of knowledge comes from. According to the Child-Scientist Theory-Theory (Gopnik & Wellman 1992; Gopnik & Meltzoff 1997), a child constructs a Theory of Mind exactly as a scientist constructs a scientific theory: she collects evidence, formulates explanatory hypotheses, and revises these hypotheses in the light of further evidence. In other words, “folk” laws and principles are obtained through hypothesis testing and revision—a process that, according to proponents of this view, is guided by a general-purpose, Bayesian learning mechanism (Gopnik & Wellman 2012). On the contrary, the Nativist Theory-Theory (Carruthers 2013; Scholl & Leslie 1999) argues that a significant part of the Theory of Mind is innate, rather than learned. More precisely, Nativists typically consider the core of the Theory of Mind as resulting from the maturation of a cognitive module specifically dedicated to representing mental states These disagreements notwithstanding, the main tenet of TT is clear enough: attributions of mental states to other people are guided by the possession of a Theory of Mind. For example, if I know that you desire to buy a copy of The New York Times and I know that you believe that if you go to News & Booze you can buy a copy, then I can use the Law of the Practical Syllogism to infer that you will decide to go to News & Booze. TT has been so popular among philosophers and cognitive scientists that the explanation it proposes has ended up being the name of the very phenomenon to be explained: on many occasions, scholars use the expression “Theory of Mind” as a synonym of “mindreading”. Simulation Theorists, however, have never been particularly impressed by this. According to them, there is no need to invoke the tacit knowledge of a Theory of Mind to account for mindreading, since a more parsimonious explanation is available: we reuse our own cognitive mechanisms to mentally simulate others’ mental states. For example, why do I need to know the Law of the Practical Syllogism, if I can employ my own decision-making mechanism (which I have anyway) to simulate your decision? It is uneconomical—Simulation Theorists say—to resort to an information-rich strategy, if an information-poor strategy will do equally as well. The difference between TT and ST can be further illustrated through a nice example given by Stich and Nichols (1992). Suppose that you want to predict the behavior of an airplane in certain atmospheric conditions. You can collect the specifications of the airplane and infer, on the basis of aerodynamic theory, how the airplane will behave. Alternatively, you can build a model of the airplane and run a simulation. The former scenario approximates the way in which TT describes our capacity to represent others’ mental states, while the latter approximates ST. Two points need to be stressed, though. First, while knowledge of aerodynamic theory is explicit, TT says that our knowledge of the Theory of Mind is typically implicit (or tacit). That is, someone who knows aerodynamic theory is aware of the theory’s laws and principles and is able to report them correctly, while the laws and principles constituting one’s Theory of Mind typically lie outside awareness and reportability. Second, when we run a simulation of someone else’s mental states, we do not need to build a model: we are the model—that is, we use our own mind as a model of others’ minds. Simulation Theorists maintain that the default state for the “model” is one in which the simulator simply makes no adjustments when simulating another individual. That is, ST has it that we are automatically disposed to attribute to a target mental states no different from our own current states. This would often serve adequately in social interaction between people who are cooperating or competing in what is for practical purposes the same situation. We tend to depart from this default when we perceive relevant differences between others’ situations and our own. In such cases, we might find ourselves adjusting for situational differences by putting ourselves imaginatively in what we consider the other’s situation to be. We might also make adjustments for individual differences. An acquaintance will soon be choosing between candidate a and candidate b in an upcoming election. To us, projecting ourselves imaginatively into that voting situation, the choice is glaringly obvious: candidate a, by any reasonable criteria. But then we may wonder whether this imaginative projection into the voting situation adequately represents our acquaintance in that situation. We might recall things the person has said, or peculiarities of dress style, diet, or entertainment, that might seem relevant. Internalizing such behavior ourselves, trying to “get behind” it as an actor might get behind a scripted role, we might then put, as it were, a different person into the voting situation, one who might choose candidate b. Such a transformation would require quarantining some of our own mental states, preferences, and dispositions, inhibiting them so that they do not contaminate our off-line decision-making in the role of the other. Such inhibition of one's own mental states would be cognitively demanding. For that reason, ST predicts that mindreading will be subject to egocentric errors—that is, it predicts that we will often attribute to a target the mental state that we would have if we were in the target’s situation, rather than the state the target is actually in (Goldman 2006). In  section 6.2,  we shall discuss whether this prediction is borne out by the data. On the face of it, ST and TT could not be more different from one another. Some philosophers, however, have argued that, on closer inspection, ST collapses into TT, thus revealing itself as a form of TT in disguise. The collapse argument was originally formulated by Daniel Dennett (1987): If I make believe I am a suspension bridge and wonder what I will do when the wind blows, what “comes to my mind” in my make-believe state depends on… my knowledge of physics… Why should my making believe I have your beliefs be any different? In both cases, knowledge of the imitated object is needed to drive the… “simulation”, and the knowledge must be… something like a theory. (Dennett 1987: 100–101, emphasis added) Dennett’s point is clear. If I imagine being, say, a bridge, what I imagine will depend on my theory of bridges. Suppose that I have a folk theory of bridges that contains the following principle: “A bridge cannot sustain a weight superior to its own weight”. In this case, if I imagine an elephant weighing three tons walking over a bridge weighing two tons, I will imagine the bridge collapsing. Since my “bridge-simulation” is entirely theory-driven, “simulation” is a misnomer. The same carries over to “simulating other people”s mental states’, Dennett says. If I try to imagine your mental states, what I imagine will depend entirely on my Theory of Mind. Therefore, the label “mental simulation” is misleading. Heal (1986) and Goldman (1989) promptly replied to Dennett. Fair enough, if a system S tries to simulate the state of a radically different system Q (e.g., if a human being tries to simulate the state of a bridge), then S’s simulation must be guided by a theory. However, if a system S tries to simulate the state of a relevantly similar system S*, then S’s simulation can be entirely process-driven: to simulate the state which S* is in, S simply has to run in itself a process similar to the one S* underwent. Given that, for all intents and purposes, human beings are relevantly similar to each other, a human being can mentally simulate what follows from having another human being’s mental states without resorting to a body of theoretical knowledge about the mind’s inner workings. She will just need to reuse her own cognitive mechanisms to implement a simulation process. This reply invited the following response (Jackson 1999). If the possibility of process-driven simulation is grounded in the similarity between the simulator and the simulated, then I have to assume that you are relevantly similar to me, when I mentally simulate your mental states. This particular assumption, in turn, will be derived from a general principle—something like “Human beings are psychologically similar”. Therefore, mental simulation is grounded in the possession of a theory. The threat of collapse is back! One reply to Jackson’s arguments is as follows (for other replies see Goldman 2006): the fact that process-driven simulation is grounded in the similarity among human beings does not entail that, in order to run a simulation, a simulator must know (or believe, or assume) that such similarity obtains; no more, indeed, than the fact that the solubility of salt is grounded in the molecular structure of salt entails that a pinch of salt needs to know chemistry to dissolve in water. Granting that ST and TT are distinct theories, we can now ask a different question: are the theories better off individually or should they join forces somehow? Let us be more explicit. Can ST on its own offer an adequate account of mindreading (or at least of the great majority of its episodes)? And what about TT? A good number of theorists now believe that neither ST nor TT alone will do. Rather, many would agree that these two theories need to cooperate, if they want to reach a satisfactory explanation of mindreading. Some authors have put forward TT-ST hybrid models, i.e., models in which the tacit knowledge of a Theory of Mind is the central aspect of mindreading, but it is in many cases supplemented by simulation processes (Botterill & Carruthers 1999; Nichols & Stich 2003). Other authors have instead defended ST-TT hybrid models, namely, accounts of mindreading where the pride of place is given to mental simulation, but where the possession of a Theory of Mind plays some non-negligible role nonetheless (Currie & Ravenscroft 2002; Goldman 2006; Heal 2003). Since this entry is dedicated to ST, we will briefly touch upon one instance of the latter variety of hybrid account. Heal (2003) suggested that the domain of ST is restricted to those mental processes involving rational transitions among contentful mental states. To wit, Heal maintains that mental simulation is the cognitive routine that we employ to represent other people’s rational processes, i.e., those cognitive processes which are sensitive to the semantic content of the mental states involved. On the other hand, when starting point and/or outcome are [states] without content, and/or the connection is not [rationally] intelligible, there is no reason … to suppose that the process … can be simulated. (Heal 2003: 77) An example will clarify the matter. Suppose that I know that you desire to eat sushi, and that you believe that you can order sushi by calling Yama Sushi. To reach the conclusion that you will decide to call Yama Sushi, I only need to imagine desiring and believing what you desire and believe, and to run a simulated decision-making process in myself. No further knowledge is required to predict your decision: simulation alone will do the job. Consider, on the other hand, the situation in which I know that you took a certain drug and I want to figure out what your mental states will be. In this case—Heal says—my prediction cannot be based on mental simulation. Rather, I need to resort to a body of information about the likely psychological effects of that drug, i.e., I have to resort to a Theory of Mind (fair enough, I can also take the drug myself, but this will not count as mental simulation). This, according to Heal, generalizes to all cases in which a mental state is the input or the output of a mere causal process. In those cases, mental simulation is ineffective and should be replaced by theorizing. Still, those cases do not constitute the central part of mindreading. In fact, many philosophers and cognitive scientists would agree that the crucial component of human mindreading is the ability to reason about others’ propositional attitudes. And this is exactly the ability that, according to Heal, should be explained in term of mental simulation. This is why Heal’s proposal counts as an ST-TT hybrid, rather than the other way around.