{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil_html=('/home/meeka/Desktop/NU/453/assn1/philosophy/philanguage')\n",
    "os.listdir(phil_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_directory = '/home/meeka/Desktop/NU/453/assn2/philosophy/philanguage'\n",
    "corpus_directory = '/home/meeka/Desktop/NU/453/assn2/philosophy/corpus'\n",
    "\n",
    "\n",
    "def save_txt(lines, filename):\n",
    "    file=open(filename, 'w')\n",
    "    file.write(lines)\n",
    "    file.close()\n",
    "    \n",
    "def process_htmldocs(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        path=directory + '/' + filename\n",
    "        load_htmldoc(path)   \n",
    "\n",
    "def load_htmldoc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    content=file.read()\n",
    "    file.close()\n",
    "    #main text extraction\n",
    "    webtext=BeautifulSoup(content, 'html.parser')\n",
    "    pagetext=webtext.find(id=\"main-text\")\n",
    "\n",
    "    #clean subject header text\n",
    "    subject=[headers.text.encode('utf-8').decode('ascii', 'ignore') for headers in webtext.find_all('h1', limit=1)]\n",
    "    char_remove=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    subject=[char_remove.sub(' ', w) for w in subject]\n",
    "\n",
    "    #cycle through each entry to split sub-sections into unique docs\n",
    "    topics=[]\n",
    "    for headers in pagetext.find_all('h2'):\n",
    "        headers=headers.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "        topics.append(subject[0]+'_'+headers)\n",
    "    topics=[char_remove.sub(' ', w) for w in topics]\n",
    "    \n",
    "    #Split HTML sections by header classes\n",
    "    collection=[]\n",
    "    body_split = re.split('(<h2>|</h2>)', str(pagetext))\n",
    "    for section in body_split:\n",
    "        clean=BeautifulSoup(section, 'html.parser')\n",
    "        paragraphs=clean.select('p')\n",
    "        if paragraphs:\n",
    "            collection.append(paragraphs)\n",
    "            \n",
    "    counter=0\n",
    "    for section in collection:\n",
    "        text=''\n",
    "        htmlstrip=['\\n','<p>','</p>','<em>','</em>']\n",
    "        for x in range(len(section)):\n",
    "            sent=str(section[x].text)\n",
    "            for item in htmlstrip:\n",
    "                sent=sent.strip().replace(item, ' ')\n",
    "            text=text +' '+ sent\n",
    "        filename = corpus_directory + '/' + topics[counter]+'.txt'\n",
    "        save_txt(text, filename)\n",
    "        counter=counter+1\n",
    "    \n",
    "            \n",
    "process_htmldocs(phil_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_directory = '/home/meeka/Desktop/NU/453/assn2/philosophy/philanguage'\n",
    "corpus_directory = '/home/meeka/Desktop/NU/453/assn2/philosophy/corpus'\n",
    "\n",
    "\n",
    "page_dirname = 'corpus'\n",
    "\n",
    "\n",
    "def process_htmldocs(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        path=directory + '/' + filename\n",
    "        load_htmldoc(path)\n",
    "\n",
    "def load_htmldoc(filename):\n",
    "    \n",
    "    file=open(filename, 'r')\n",
    "    content=file.read()\n",
    "    file.close()\n",
    "    #main text extraction\n",
    "    webtext=BeautifulSoup(content, 'html.parser')\n",
    "    pagetext=webtext.find(id=\"main-text\")\n",
    "\n",
    "    #Identify sub headers for topic sections\n",
    "    subject=[headers.text.encode('utf-8').decode('ascii', 'ignore') for headers in webtext.find_all('h1')]\n",
    "    char_remove=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    subject=[char_remove.sub(' ', w) for w in subject]\n",
    "    #subject=[header.replace('/',' ') for header in subject]    \n",
    "\n",
    "    topics=[]\n",
    "    for headers in pagetext.find_all('h2'):\n",
    "        headers=headers.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "        removal=string.punctuation\n",
    "        for item in removal:\n",
    "            headers=headers.replace(item, '')\n",
    "        topics.append(subject[0]+'_'+headers)\n",
    "\n",
    "    #Split HTML sections by header classes\n",
    "    fullbody=str(pagetext)\n",
    "    body_split = re.split('(<h2>|</h2>)', fullbody)\n",
    "    collection=[]\n",
    "    for section in body_split:\n",
    "        clean=BeautifulSoup(section, 'html.parser')\n",
    "        paragraphs=clean.select('p')\n",
    "        if paragraphs:\n",
    "            collection.append(paragraphs)\n",
    "\n",
    "    #split and clean each section of text\n",
    "    counter=0\n",
    "    for section in collection:\n",
    "        text=''\n",
    "        htmlstrip=['\\n','<p>','</p>','<em>','</em>']\n",
    "        for x in range(len(section)):\n",
    "            sent=str(section[x].text)\n",
    "            for item in htmlstrip:\n",
    "                sent=sent.strip().replace(item, ' ')\n",
    "            text=text +' '+ sent\n",
    "        try:\n",
    "            filename = corpus_directory + '/' + topics[counter]+'.txt'\n",
    "            save_list(text, filename)\n",
    "        except:\n",
    "            pass\n",
    "        counter=counter+1\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    file=open(filename, 'w')\n",
    "    file.write(lines)\n",
    "    file.close()\n",
    "\n",
    "process_htmldocs(html_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33659\n"
     ]
    }
   ],
   "source": [
    "# # Create Vocabulary across entire corpus:\n",
    "\n",
    "# def load_doc(filename):\n",
    "#     file=open(filename, 'r')\n",
    "#     text=file.read()\n",
    "#     file.close()\n",
    "#     return text    \n",
    "    \n",
    "# def clean_doc(doc):\n",
    "#     tokens=doc.split()\n",
    "#     tokens=[word.lower() for word in tokens]\n",
    "#     re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "#     tokens=[re_punc.sub('',w) for w in tokens]\n",
    "#     tokens=[word for word in tokens if word.isalpha()]\n",
    "#     stop_words=set(stopwords.words('english'))\n",
    "#     tokens=[word for word in tokens if not word in stop_words]\n",
    "#     tokens=[word for word in tokens if len(word)>2]\n",
    "#     return tokens\n",
    "\n",
    "# def add_doc_to_vocab(filename, vocab):\n",
    "#     doc=load_doc(filename)\n",
    "#     tokens=clean_doc(doc)\n",
    "#     vocab.update(tokens)\n",
    "    \n",
    "# def process_docs(directory, vocab):\n",
    "#     for filename in listdir(directory):\n",
    "#         path=directory+'/'+filename\n",
    "#         add_doc_to_vocab(path, vocab)\n",
    "    \n",
    "# def save_list(lines, filename):\n",
    "#     data='\\n'.join(lines)\n",
    "#     file=open(filename, 'w')\n",
    "#     file.write(data)\n",
    "#     file.close()\n",
    "    \n",
    "# vocab=Counter()\n",
    "# process_docs(corpus_directory, vocab)\n",
    "# print(len(vocab))\n",
    "\n",
    "# min_occurrence=3\n",
    "# tokens=[k for k,c in vocab.items() if c >= min_occurrence]\n",
    "# save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_doc(filename):\n",
    "#     file=open(filename, 'r')\n",
    "#     text=file.read()\n",
    "#     file.close()\n",
    "#     return text    \n",
    "\n",
    "# def clean_doc(doc):\n",
    "#     tokens=doc.split()\n",
    "#     tokens=[word.lower() for word in tokens]\n",
    "#     re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "#     tokens=[re_punc.sub('',w) for w in tokens]\n",
    "#     tokens=[word for word in tokens if word.isalpha()]\n",
    "#     stop_words=set(stopwords.words('english'))\n",
    "#     tokens=[word for word in tokens if not word in stop_words]\n",
    "#     tokens=[word for word in tokens if len(word)>2]\n",
    "#     return tokens\n",
    "\n",
    "# def process_docs(directory, top_vocab):\n",
    "#     for filename in listdir(directory):\n",
    "#         path=directory+'/'+filename\n",
    "#         doc=load_doc(path)\n",
    "#         tokens=clean_doc(doc)\n",
    "#         vocab_tokens=[word for word in tokens if word in top_vocab]\n",
    "#         wordcount=Counter(tokens)\n",
    "#         vocabcount=Counter(vocab_tokens)\n",
    "#         corpus_vdict[filename]=vocabcount\n",
    "            \n",
    "\n",
    "# corpus_vdict={}\n",
    "# corpus=[]\n",
    "# process_docs(corpus_directory, top_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_directory = '/home/meeka/Desktop/NU/453/assn2/philosophy/corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classes to label corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create vocabulary sets for class divisions\n",
    "\n",
    "logic=['logic', 'logical', 'logics', 'syllogism', 'syllogisms', 'model']\n",
    "math=['mathematics','mathematical', 'number', 'set', 'sets', 'probability','probabilities', 'proof']\n",
    "language=['language', 'linguistic', 'sentences', 'sentence', 'proposition', 'propositions', 'verb', 'verbs',\n",
    "         'discourse', 'word', 'words']\n",
    "mental=['cognition', 'cognitive', 'consciousness', 'thought', 'thoughts','knowledge', 'know', 'mental', \n",
    "        'perception', 'neural', 'brain', 'mind', 'selfknowledge']\n",
    "ontology=['objects', 'object', 'truth', 'abstract', 'abstraction', 'phenomenal', 'phenomenology',\n",
    "             'representation', 'representational', 'representations', 'experience', 'experiences']\n",
    "ethics=['ethics', 'ethical', 'moral', 'morality', 'religion']\n",
    "\n",
    "top_vocab=[]\n",
    "\n",
    "def merge_list(group):\n",
    "    for word in group:\n",
    "        top_vocab.append(word)\n",
    "\n",
    "merge_list(logic)\n",
    "merge_list(math)\n",
    "merge_list(language)\n",
    "merge_list(mental)\n",
    "merge_list(ontology)\n",
    "merge_list(ethics)\n",
    "\n",
    "len(top_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dict of entire corpus\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text    \n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)>2]\n",
    "    return tokens\n",
    "\n",
    "def process_docs(directory):\n",
    "    for filename in listdir(directory):\n",
    "        path=directory+'/'+filename\n",
    "        doc=load_doc(path)\n",
    "        tokens=clean_doc(doc)\n",
    "        #process lists, counters, dicts:\n",
    "        vocab.update(tokens)\n",
    "        wordcount=Counter(tokens)\n",
    "        corpus_dict_top5[filename]=wordcount.most_common(5)\n",
    "        line= ' '.join(tokens)\n",
    "        corpus_dict_sent[filename]=line\n",
    "        vocab_tokens=[word for word in tokens if word in top_vocab]\n",
    "        vocabcount=Counter(vocab_tokens)\n",
    "        corpus_vdict[filename]=vocabcount\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "vocab=Counter()\n",
    "\n",
    "#Top 5 words in each document for categorization\n",
    "corpus_dict_top5={}\n",
    "#Dict with top wordcounts for top vocab 6 group categorization list\n",
    "corpus_vdict={}\n",
    "#Dict with full length sentences\n",
    "corpus_dict_sent={}\n",
    "\n",
    "process_docs(corpus_directory)\n",
    "\n",
    "min_occurrence=50\n",
    "vocab=[k for k,c in vocab.items() if c >= min_occurrence]\n",
    "save_list(vocab, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mental      189\n",
       "logic       139\n",
       "ontology    135\n",
       "language    128\n",
       "math         95\n",
       "ethics       55\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create subset for class labels, merge with main corpus to align docs with classes\n",
    "\n",
    "def group_dfs(group):\n",
    "    group_set={}\n",
    "    for k,v in corpus_dict_top5.items():\n",
    "        for name, count in v:\n",
    "            if name in group:\n",
    "                group_set[k]=v\n",
    "    dkeys=[]\n",
    "    dvals=[]\n",
    "    for x,y in group_set.items():\n",
    "        dkeys.append(x)\n",
    "        dv=[]\n",
    "        for item in y:\n",
    "            dv.append(str(item[0]+'-'+str(item[1])))\n",
    "        dvals.append(dv)\n",
    "    headers=[]\n",
    "    for x in range(1,6):\n",
    "        label=str('word'+str(x))\n",
    "        headers.append(label)\n",
    "    newdf=pd.DataFrame(dvals, columns=headers, index=dkeys)\n",
    "    return newdf\n",
    "\n",
    "logic_df=group_dfs(logic)\n",
    "math_df=group_dfs(math)\n",
    "language_df=group_dfs(language)\n",
    "mental_df=group_dfs(mental)\n",
    "ontology_df=group_dfs(ontology)\n",
    "ethics_df=group_dfs(ethics)\n",
    "\n",
    "logic_df['class']='logic'\n",
    "math_df['class']='math'\n",
    "language_df['class']='language'\n",
    "mental_df['class']='mental'\n",
    "ontology_df['class']='ontology'\n",
    "ethics_df['class']='ethics'\n",
    "\n",
    "frames=[logic_df, math_df, language_df, mental_df, ontology_df, ethics_df]\n",
    "full_df=pd.concat(frames)\n",
    "full_df=full_df.reset_index()\n",
    "full_df=full_df.rename(columns={\"index\":\"document\"})\n",
    "full_df.to_csv('full_concat.csv')\n",
    "    \n",
    "full_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full df shape:  (741, 7)\n",
      "unique docs shape:  (482, 7)\n",
      "dups docs shape:  (259, 7)\n",
      "\n",
      "Value counts of unique docs:\n",
      "mental      141\n",
      "logic        84\n",
      "ontology     75\n",
      "language     72\n",
      "math         66\n",
      "ethics       44\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Identify Dups, separate datasets:\n",
    "\n",
    "dups=full_df.duplicated(subset=['document'])\n",
    "df_dup=pd.concat([full_df['document'], dups],axis=1, join='inner')\n",
    "df_dup=df_dup.rename(columns={0:'duplicate'})\n",
    "df_dup=df_dup[df_dup.duplicate]\n",
    "\n",
    "duplist=df_dup.document.to_list()\n",
    "uniquedocs=full_df[~full_df.document.isin(duplist)]\n",
    "dupdocs=full_df[full_df.document.isin(duplist)]\n",
    "\n",
    "print('full df shape: ', full_df.shape)\n",
    "print('unique docs shape: ', uniquedocs.shape)\n",
    "print('dups docs shape: ', dupdocs.shape)\n",
    "print('\\nValue counts of unique docs:')\n",
    "print(uniquedocs['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final corpus shape:  (546, 7)\n",
      "final corpus counts: \n",
      "\n",
      "mental      141\n",
      "logic        84\n",
      "ontology     75\n",
      "language     72\n",
      "math         66\n",
      "ethics       44\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Clean duplicate docs based on first word assignment to category\n",
    "\n",
    "docfilter=dupdocs.drop_duplicates(subset='document').copy(deep=True)\n",
    "docfilter2=docfilter.copy(deep=True)\n",
    "docfilter2[['word1w','word1c']]=docfilter2.word1.str.split(\"-\",expand=True)\n",
    "\n",
    "keepcols=['document','word1w']\n",
    "primaryword=docfilter2.filter(items=keepcols, axis=1).copy(deep=True)\n",
    "\n",
    "NaN=np.nan\n",
    "primaryword['class']=NaN\n",
    "\n",
    "def firstword(group, name):\n",
    "    for i in range(len(primaryword)):\n",
    "        for word in group:\n",
    "            if primaryword.iloc[i,1]==word:\n",
    "                primaryword.iloc[i,2]=name\n",
    "\n",
    "firstword(logic, 'logic')\n",
    "firstword(math, 'math')\n",
    "firstword(language, 'language')\n",
    "firstword(mental, 'mental')\n",
    "firstword(ontology, 'ontology')\n",
    "firstword(ethics, 'ethics')\n",
    "\n",
    "primaryword.dropna(subset=['class'], inplace=True)\n",
    "docfilter=docfilter.drop(columns=['class'])\n",
    "uniquefiltered=pd.concat([docfilter, primaryword['class']],axis=1, join='inner')\n",
    "uniquefiltered.shape\n",
    "\n",
    "frames=[uniquedocs, uniquefiltered]\n",
    "final_corpus_df=pd.concat(frames)\n",
    "print('final corpus shape: ', final_corpus_df.shape)\n",
    "print('final corpus counts: \\n')\n",
    "print(uniquedocs['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selfknowledge</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>thought</th>\n",
       "      <th>cognitive</th>\n",
       "      <th>mental</th>\n",
       "      <th>know</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>phenomenal</th>\n",
       "      <th>experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Self Knowledge_1 The Distinctiveness of SelfKnowledge.txt</th>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folk Psychology as Mental Simulation_7 Conclusion.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gertrude Elizabeth Margaret Anscombe_3 Metaphysics.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consciousness and Intentionality_8 Consciousness in Mind.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thomas Reid_6 Moral Philosophy.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folk Psychology as Mental Simulation_6 Simulation Theory Pros and Cons.txt</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narrow Mental Content_3 Arguments for Narrow Content.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Externalism About Mental Content_6 Externalism and Selfknowledge.txt</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self Knowledge_2 Doubts about the distinctiveness of selfknowledge.txt</th>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self Knowledge_3 Accounts of SelfKnowledge.txt</th>\n",
       "      <td>73.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    selfknowledge  knowledge  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...           18.0        5.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...            1.0        1.0   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...            1.0        NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...            1.0        NaN   \n",
       "Thomas Reid_6 Moral Philosophy.txt                            1.0        4.0   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...            2.0        6.0   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...            1.0        1.0   \n",
       "Externalism About Mental Content_6 Externalism ...           15.0       11.0   \n",
       "Self Knowledge_2 Doubts about the distinctivene...           10.0        5.0   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt               73.0       31.0   \n",
       "\n",
       "                                                    thought  cognitive  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...      2.0        1.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...      NaN        3.0   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...      1.0        NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...     10.0        5.0   \n",
       "Thomas Reid_6 Moral Philosophy.txt                      2.0        NaN   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...      NaN        4.0   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...      5.0        NaN   \n",
       "Externalism About Mental Content_6 Externalism ...      7.0        NaN   \n",
       "Self Knowledge_2 Doubts about the distinctivene...      4.0        1.0   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt          4.0        6.0   \n",
       "\n",
       "                                                    mental  know  thoughts  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...    19.0   3.0       6.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...     2.0   NaN       NaN   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...     NaN   NaN       NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...     6.0   1.0       1.0   \n",
       "Thomas Reid_6 Moral Philosophy.txt                     2.0   1.0       NaN   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...    25.0   2.0       NaN   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...    10.0   NaN      10.0   \n",
       "Externalism About Mental Content_6 Externalism ...     1.0   9.0      14.0   \n",
       "Self Knowledge_2 Doubts about the distinctivene...     8.0   7.0       NaN   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt        49.0  15.0       8.0   \n",
       "\n",
       "                                                    consciousness  phenomenal  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...            1.0         2.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...            NaN         NaN   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...            NaN         NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...           27.0         5.0   \n",
       "Thomas Reid_6 Moral Philosophy.txt                            NaN         NaN   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...            NaN         NaN   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...            NaN        23.0   \n",
       "Externalism About Mental Content_6 Externalism ...            NaN         NaN   \n",
       "Self Knowledge_2 Doubts about the distinctivene...            NaN         NaN   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt                4.0        12.0   \n",
       "\n",
       "                                                    experience  \n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...         2.0  \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...         NaN  \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...         NaN  \n",
       "Consciousness and Intentionality_8 Consciousnes...         2.0  \n",
       "Thomas Reid_6 Moral Philosophy.txt                         1.0  \n",
       "Folk Psychology as Mental Simulation_6 Simulati...         1.0  \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...         5.0  \n",
       "Externalism About Mental Content_6 Externalism ...         NaN  \n",
       "Self Knowledge_2 Doubts about the distinctivene...         1.0  \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt            11.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_texts=final_corpus_df['document']\n",
    "final_texts=final_texts.to_list()\n",
    "\n",
    "final_vcorpus={}\n",
    "\n",
    "for k,v in corpus_vdict.items():\n",
    "    for word in final_texts:\n",
    "        if k == word:\n",
    "            final_vcorpus[k]=v\n",
    "\n",
    "vocab_matrix=pd.DataFrame.from_dict(final_vcorpus, orient='index')\n",
    "vocab_matrix.to_csv('vocab_matrix_analyst.csv')\n",
    "vocab_matrix.iloc[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(corpus_vdict.items())[:4]\n",
    "#final_texts[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 2901)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "final_corpus=[]\n",
    "final_labels=[]\n",
    "\n",
    "for k,v in corpus_dict_sent.items():\n",
    "    for word in final_texts:\n",
    "        if k == word:\n",
    "            final_corpus.append(v)\n",
    "            final_labels.append(k)\n",
    "\n",
    "\n",
    "            vectorizer=TfidfVectorizer(vocabulary=vocab)\n",
    "X=vectorizer.fit_transform(final_corpus)\n",
    "print(X.shape)\n",
    "\n",
    "feature_names=vectorizer.get_feature_names()\n",
    "corpus_index=[n for n in final_corpus]\n",
    "Tfidf_df_matrix=pd.DataFrame(X.todense(), index=final_labels, columns=feature_names)\n",
    "Tfidf_df_matrix.T.to_csv('vocab_matrix_tfidf.csv')\n",
    "\n",
    "Tfidf_df_matrix_topVocab=Tfidf_df_matrix[top_vocab]\n",
    "Tfidf_df_matrix_topVocab.iloc[:10,:10]\n",
    "Tfidf_df_matrix_topVocab.to_csv('Tfidf_df_matrix_topVocab.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3 NN Embeddings (Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 50)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(final_corpus_tokens)]\n",
    "model_50dim = Doc2Vec(documents, vector_size=50, min_count=2, epochs=50)\n",
    "\n",
    "doc2vec_50_vectors = np.zeros((len(final_corpus_tokens), 50)) \n",
    "for i in range(0, len(final_corpus_tokens)):\n",
    "    doc2vec_50_vectors[i,] = model_50dim.infer_vector(final_corpus_tokens[i]).transpose()\n",
    "print(doc2vec_50_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split corpus into train, test groupings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "doc_class_df=final_corpus_df[['document','class']]\n",
    "final_texts=doc_class_df.values.tolist()\n",
    "\n",
    "final_corpus_dict={}\n",
    "final_labels_dict={}\n",
    "\n",
    "\n",
    "for k,v in corpus_dict_sent.items():\n",
    "    for item in final_texts:\n",
    "        if k == item[0]:\n",
    "            final_corpus_dict[k]=v\n",
    "            final_labels_dict[k]=item[1]\n",
    "#             final_corpus.append(v)\n",
    "#             final_titles.append(k)\n",
    "\n",
    "X=list(final_corpus_dict.values())\n",
    "y=list(final_labels_dict.values())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=10, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# label_dict={}\n",
    "\n",
    "# for k,v in labelclass_dict.items():\n",
    "#     for word in final_texts:\n",
    "#         if k == word:\n",
    "#             label_dict[word]=v\n",
    "\n",
    "# final_corpus_tokens=[]\n",
    "# for item in final_corpus:\n",
    "#     doc=item.split()\n",
    "#     final_corpus_tokens.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "logic\n",
      "language\n",
      "mental\n",
      "mental\n",
      "logic\n",
      "math\n",
      "math\n",
      "ethics\n",
      "ontology\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_(directory):\n",
    "    for filename in listdir(directory):\n",
    "        tokens=clean_doc(doc)\n",
    "        #process lists, counters, dicts:\n",
    "        vocab.update(tokens)\n",
    "        wordcount=Counter(tokens)\n",
    "        corpus_dict_top5[filename]=wordcount.most_common(5)\n",
    "        line= ' '.join(tokens)\n",
    "        corpus_dict_sent[filename]=line\n",
    "        vocab_tokens=[word for word in tokens if word in top_vocab]\n",
    "        vocabcount=Counter(vocab_tokens)\n",
    "        corpus_vdict[filename]=vocabcount\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def process_sets(y,X):\n",
    "    for text in zip(y,X):\n",
    "        tokens=text[1].split\n",
    "        wordcount=Counter(tokens)\n",
    "        vocabcount=Counter(vocab_tokens)\n",
    "        corpus_vdict[filename]=vocabcount    \n",
    "    \n",
    "corpus_vdict={}\n",
    "\n",
    "process_sets(y_test, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#Maybe fix later, started to\n",
    "#\n",
    "\n",
    "\n",
    "import numpy\n",
    "\n",
    "docfilter=dupdocs.drop_duplicates(subset='document').copy(deep=True)\n",
    "docfilter2=docfilter.copy(deep=True)\n",
    "docfilter2[['word1w','word1c']]=docfilter2.word1.str.split(\"-\",expand=True)\n",
    "\n",
    "keepcols=['document','word1w']\n",
    "primaryword=docfilter2.filter(items=keepcols, axis=1).copy(deep=True)\n",
    "\n",
    "NaN=np.nan\n",
    "primaryword['class']=NaN\n",
    "\n",
    "def firstword(group, name):\n",
    "    for i in range(len(primaryword)):\n",
    "        for word in group:\n",
    "            if primaryword.iloc[i,1]==word:\n",
    "                primaryword.iloc[i,2]=name\n",
    "\n",
    "firstword(logic, 'logic')\n",
    "firstword(math, 'math')\n",
    "firstword(language, 'language')\n",
    "firstword(mental, 'mental')\n",
    "firstword(ontology, 'ontology')\n",
    "firstword(ethics, 'ethics')\n",
    "\n",
    "\n",
    "firstword_nandrop=primaryword.dropna(subset=['class'], inplace=True)\n",
    "firstnandrops=firstword_nandrop.document.to_list()\n",
    "\n",
    "f_nonmatched=firstword[~firstword.document.isin(firstnandrops)]\n",
    "f_matched=firstword[firstword.document.isin(firstnandrops)]\n",
    "\n",
    "docfilter=dupdocs.drop_duplicates(subset='document').copy(deep=True)\n",
    "docfilter2=docfilter.copy(deep=True)\n",
    "docfilter2[['word1w','word1c']]=docfilter2.word1.str.split(\"-\",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primaryword.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# full_df[['word1w','word1c']]=full_df.word1.str.split(\"-\",expand=True)\n",
    "# full_df[['word2w','word2c']]=full_df.word2.str.split(\"-\",expand=True)\n",
    "# full_df[['word3w','word3c']]=full_df.word3.str.split(\"-\",expand=True)\n",
    "# full_df[['word4w','word4c']]=full_df.word4.str.split(\"-\",expand=True)\n",
    "# full_df[['word5w','word5c']]=full_df.word5.str.split(\"-\",expand=True)\n",
    "\n",
    "# full_df.head()\n",
    "\n",
    "\n",
    "#setup misc\n",
    "\n",
    "# group_set={}\n",
    "# for k,v in corpus_dict.items():\n",
    "#     for name, count in v:\n",
    "#         if name in ethics:\n",
    "#             group_set[k]=v\n",
    "# dkeys=[]\n",
    "# dvals=[]\n",
    "# for x,y in group_set.items():\n",
    "#     dkeys.append(x)\n",
    "#     dv=[]\n",
    "#     for item in y:\n",
    "#         dv.append(str(item[0]+'-'+str(item[1])))\n",
    "#     dvals.append(dv)\n",
    "# print(dvals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #old consolidated already\n",
    "# #####################\n",
    "# logic_set={}\n",
    "# math_set={}\n",
    "# language_set={}\n",
    "# mental_set={}\n",
    "# ontology_set={}\n",
    "# ethics_set={}\n",
    "\n",
    "# for k,v in corpus_dict.items():\n",
    "#     for name, count in v:\n",
    "#         if name in logic:\n",
    "#             logic_set[k]=v\n",
    "#         if name in math:\n",
    "#             math_set[k]=v\n",
    "#         if name in language:\n",
    "#             language_set[k]=v\n",
    "#         if name in mental:\n",
    "#             mental_set[k]=v\n",
    "#         if name in ontology:\n",
    "#             ontology_set[k]=v\n",
    "#         if name in ethics:\n",
    "#             ethics_set[k]=v\n",
    "            \n",
    "# print('Logic set docs: ', len(logic_set))\n",
    "# print('Math set docs: ', len(math_set))\n",
    "# print('Language set docs: ', len(language_set))\n",
    "# print('Mental set docs: ', len(mental_set))\n",
    "# print('Ontology set docs: ', len(ontology_set))\n",
    "# print('Ethics set docs: ', len(ethics_set))\n",
    "\n",
    "# logic_df=pd.DataFrame\n",
    "# math_df=pd.DataFrame\n",
    "# language_df=pd.DataFrame\n",
    "# mental_df=pd.DataFrame\n",
    "# ontology_df=pd.DataFrame\n",
    "# ethics_df=pd.DataFrame\n",
    "\n",
    "# def group_dfs(corpus, newdf):\n",
    "#     dkeys=[]\n",
    "#     dvals=[]\n",
    "#     for x,y in corpus.items():\n",
    "#         dkeys.append(x)\n",
    "#         dvals.append(y)\n",
    "#     headers=[]\n",
    "#     for x in range(1,6):\n",
    "#         label=str('word'+str(x))\n",
    "#         headers.append(label)\n",
    "#     newdf=pd.DataFrame(dvals, columns=headers, index=dkeys)\n",
    "#     return newdf\n",
    "\n",
    "# logic_df=group_dfs(logic_set, logic_df)\n",
    "# math_df=group_dfs(math_set, math_df)\n",
    "# language_df=group_dfs(language_set, language_df)\n",
    "# mental_df=group_dfs(mental_set, mental_df)\n",
    "# ontology_df=group_dfs(ontology_set, ontology_df)\n",
    "# ethics_df=group_dfs(ethics_set, ethics_df)\n",
    "\n",
    "# ethics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### old used to review entire corpus as csv\n",
    "# ########################\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# dkeys=[]\n",
    "# dvals=[]\n",
    "# for x,y in corpus_dict.items():\n",
    "#     dkeys.append(x)\n",
    "#     dvals.append(y)\n",
    "\n",
    "# headers=[]\n",
    "# for x in range(1,6):\n",
    "#     label=str('word'+str(x))\n",
    "#     headers.append(label)\n",
    "    \n",
    "# corpus_df=pd.DataFrame(dvals, columns=headers, index=dkeys)\n",
    "# corpus_df.head()\n",
    "\n",
    "# #corpus_df.to_csv('corpus_top20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res={key:corpus_dict[key] for key in corpus_dict.keys() & {'Aristotle_2 The Aristotelian Corpus Character and Primary Divisions.txt'}}\n",
    "print(str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#Create a slim Down Corpus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text    \n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)>2]\n",
    "    return tokens\n",
    "\n",
    "def grow_corpus(tokens):\n",
    "    for x in tokens:\n",
    "        corpus.append(x)\n",
    "\n",
    "def process_docs(directory):\n",
    "    corpus_full=[]\n",
    "    for filename in listdir(directory):\n",
    "        path=directory+'/'+filename\n",
    "        doc=load_doc(path)\n",
    "        tokens=clean_doc(doc)\n",
    "        for x in tokens:\n",
    "            corpus_full.append(x)\n",
    "    return corpus_full\n",
    "\n",
    "\n",
    "corpus_full=process_docs(corpus_directory)\n",
    "print('original corpus full size: ', len(corpus_full))\n",
    "corp_vocab=Counter(corpus_full)   \n",
    "print('corpus full counter size: ', len(corp_vocab))\n",
    "\n",
    "corp_vocab.most_common(100)\n",
    "\n",
    "\n",
    "#Slim down corpus if needed based on word counts:\n",
    "# min_occurrence=100\n",
    "# corp_vocab_wmin=[k for k,c in corp_vocab.items() if c >= min_occurrence]\n",
    "# corpus_slim=[word for word in corpus_full if word in corp_vocab_wmin]\n",
    "# print('Slim down corpus size: ', len(corpus_slim))\n",
    "\n",
    "#Push to .txt file\n",
    "\n",
    "# length=1000\n",
    "# sequences=list()\n",
    "# for i in range(length, len(corpus_slim)):\n",
    "#     seq=corpus_slim[i-length:i]\n",
    "#     line=' '.join(seq)\n",
    "#     sequences.append(line)\n",
    "# print('total Sequences: %d' %len(sequences))\n",
    "\n",
    "# data='\\n'.join(sequences)\n",
    "# file=open('corpus_full.txt', 'w')\n",
    "# file.write(data)\n",
    "# file.close()\n",
    "\n",
    "# top1k=corp_vocab.most_common(200)\n",
    "# vec1=[c for k,c in corp_vocab.items() if c > 500]\n",
    "# vec2=[k for k,c in corp_vocab.items() if c > 500]\n",
    "# plt.plot(vec1,vec2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text    \n",
    "\n",
    "def process_docs(directory):\n",
    "    for filename in listdir(directory):\n",
    "        path=directory+'/'+filename\n",
    "        doc=load_doc(path)\n",
    "        corpus.append(doc)\n",
    "\n",
    "corpus=[]\n",
    "process_docs(corpus_directory)\n",
    "\n",
    "sw=set(stopwords.words('english'))\n",
    "wordlimit='[a-zA-Z]{3,30}'\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=sw, token_pattern=wordlimit, max_features=200)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(stop_words=sw, analyzer='word', ngram_range=(2, 6), max_features=200)\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of related content links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_dir = '/home/meeka/Desktop/NU/453/assn2/philosophy/philanguage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in listdir(html_dir):\n",
    "    webpage=html_dir+'/'+doc\n",
    "    file=open(webpage, 'r')\n",
    "    content=file.read()\n",
    "    file.close()\n",
    "    \n",
    "    webtext=BeautifulSoup(content, 'html.parser')\n",
    "    pagetext=webtext.find(id=\"related-entries\")\n",
    "    links=pagetext.find('p')\n",
    "    print(doc)\n",
    "    for refs in links.find_all('a'):\n",
    "        nodes=refs.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "        print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Language Py3.7",
   "language": "python",
   "name": "language"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
